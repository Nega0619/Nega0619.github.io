<p>현재 저는 강화학습 스터디를 참여해 강화학습에 대한 강의를 보고있습니다.</p>

<p>오늘의 포스팅은 그 강의에 대한 회고록 및 요약 정리에 관한 것입니다.</p>

<hr />

<h6 id="강의정보">강의정보</h6>

<ul>
  <li>
    <p>링크</p>

    <p>https://www.youtube.com/watch?v=TCCjZe0y4Qc</p>
  </li>
  <li>
    <p>강의자</p>

    <p>Hado van Hasselt, Senior Staff Reasearch Scientist, Deepmind</p>
  </li>
  <li>
    <p>강화학습 스터디에서 진행하는 강의 같이보기 도전중!</p>
  </li>
</ul>

<hr />

<h1 id="1-what-is-reinforcement-learning">1. what is reinforcement learning?</h1>

<p>Industrial revolution과 Digital Revolution으로 이야기를 시작고, 본격적인 AI에 관한 이야기가 진행되는데 이 말이 너무 멋있어서 메모했다.</p>

<blockquote>
  <p>Next thing that you can think of which is already ongoing.</p>
</blockquote>

<p>지금 현재, AI 기술이 얼마나 빠르게 발전하고 있는지에 대해 알 수 있는 문장이다.</p>

<h2 id="artificial-intelligence란">Artificial Intelligence란?</h2>

<p>현재는 데이터를 학습하여 그저 계산을 하는 단계는 넘어섰고, 스스로 학습하여 솔루션을 찾을 수 있는 기계의 단계로 넘어가고 있다. 즉 자율적으로 결정을 하고, 학습을 하는 것이다.</p>

<p>우리가 정의 내리는 인공지능은 다음에 따라 결정될 것이다 : To be able to learn to make dicisions to achieve goals.</p>

<h2 id="reinforcement-learning이란">Reinforcement learning이란?</h2>

<p>사람이 환경과 상호작용을 하며 학습을 한다는 아이디어를 시작으로 개발된 이론</p>

<p>강화학습의 특징</p>

<ul>
  <li>
    <p>수동적이기보단 적극적이며, (2강에서 자세히)</p>
  </li>
  <li>
    <p>독립적이지 않고 연속적이다. 향후 상호작용은 이전 상호작용에 의해 달라질 수 있다.</p>
  </li>
  <li>
    <p>목표 지향적이다.</p>
  </li>
  <li>
    <p>we can learn <strong>without examples</strong> of optimal behavior</p>

    <p>모든 하위 행동을 하나하나 지정해주지 않아도, 명확한 목적을 통해 하위 행동이 자동적으로 행동이 정해진다.(이 부분이 잘 이해가 안갑니다. 약 10:00분쯤)</p>
  </li>
  <li>
    <p>보상을 최적화 한다.(학습한다.)</p>

    <p>즉, 목표 : Agent is going to try to optimize sum of rewards, through repeated interaction</p>

    <p>그렇기 때문에 강화학습은, environment와 agent의 상호작용(observation / action)에서 상호작용 하나하나에 관심을 갖기보다, 그 상호작용의 결과물, 즉 보상이 최대가 되도록 하는데에 관심을 갖는다.</p>
  </li>
</ul>

<h3 id="the-reward-hypothesis">The reward hypothesis</h3>

<p>16:14</p>

<h2 id="examples-rl-problems">Examples RL problems</h2>

<p>실제로 강화학습을 이용해 해결한 문제들</p>

<h3 id="fly-a-helicopter">Fly a helicopter</h3>

<p>보상 : air time(비행시간), inverse distance, ..</p>

<h3 id="manage-an-investment-porfolio">Manage an investment porfolio</h3>

<p>보상 : gains, gains minus risk, ..</p>

<h3 id="control-a-power-station">Control a power station</h3>

<p>보상 : efficienct, …</p>

<h3 id="make-a-robot-walk">Make a robot walk</h3>

<p>보상 : distance, speed, …</p>

<h3 id="play-video-or-board-games">play video or board games</h3>

<p>보상 : win, maximise score, …</p>

<h3 id="강화학습을-꼭-이용했어야-하는-distinct한-이유들">강화학습을 꼭 이용했어야 하는 distinct한 이유들</h3>

<ol>
  <li>find solutions</li>
  <li>it can adapt online, deal with unforeseen circumstances</li>
</ol>

<p>강화학습은, 해결책을 찾아야 하거나, 전혀 모르는 상황에 대해서도 적응할 수 있어, 위 두개의 문제에 대한 알고리즘을 제공해 줄 수 있다.</p>

<p>하지만, 강화학습 can adapt online에서, 일반화를 하는것이 아닌, online 환경에서 효과적으로 계속 배울수 있다는 의미.</p>

<p>-&gt; online의 데이터를 이용하여 학습을 하고 추상화 한다는게 아니라, 온라인 환경에서 학습을 할 수 있다는 의미인가요? (22”00)</p>

<h1 id="2-about-reinforcement-learning">2. About Reinforcement Learning</h1>

<h2 id="agent-and-environment---interaction-loop">Agent and Environment - Interaction Loop</h2>

<p><img src="../../../../Typora Images/image-20220111190532616.png" alt="image-20220111190532616" /></p>

<ul>
  <li>매 t단계마다 관찰과 보상을 받음.
    <ul>
      <li>Agent
        <ul>
          <li>관찰받음 O_t 혹은 보상 R_t</li>
          <li>행동 A 실행 A_t</li>
        </ul>
      </li>
      <li>Environment
        <ul>
          <li>행동 A 수령</li>
          <li>O_t+1 방출 혹은 보상 R_t+1</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="reward">Reward</h3>

<ul>
  <li>reward R_t 는 스칼라값</li>
  <li>Agent의 할일은 보상을 최대화 하는 것.</li>
</ul>

<p><img src="../../../../Typora Images/image-20220111191110182.png" alt="image-20220111191110182" /></p>

<p>G는 Goal이라고 생각하는게 보통이지만, 이건 return 반환이라고 생각. 즉, return은 accumulative reward or the sum of rewards into the future의 약어</p>

<h3 id="values">Values</h3>

<p>state s에 대한 cumulative reward를 의미</p>

<p><img src="../../../../Typora Images/image-20220111191513362.png" alt="image-20220111191513362" /></p>

<ul>
  <li>
    <p>agent가 취하는 행동에 따라 다름</p>
  </li>
  <li>
    <p>우리가 원하는 것은, agent가 value가 커지는 액션을 취하도록 만드는 것.</p>

    <p>so, we want to pick actions such that this value becomes large.</p>
  </li>
  <li>
    <p>reward와 value는 해당 state에 대한 action의 유용함(타당함?)을 의미한다.</p>
  </li>
  <li>
    <p>reward와 value는 재귀적으로 정의될 수 있다.</p>

    <p>32:30 return값이 R_t-1+G_t-1이 아니라, t+1인 이유가 이해가 잘안됩니다.</p>

    <p><img src="../../../../Typora Images/image-20220111192113239.png" alt="image-20220111192113239" /></p>
  </li>
</ul>

<h2 id="maximising-value-by-taking-actions">Maximising value by taking actions</h2>

<ul>
  <li>강화학습의 목표는, value를 최대화하는 액션들을 고르는것.
    <ul>
      <li>액션들은 장기간의 결과를 가질수있음.</li>
      <li>보상이 지연되어 발생할 수 있음</li>
      <li>즉시보상을 안받는 것이, 장기간의 보상을 받는게 더 좋을 수있음.</li>
    </ul>
  </li>
</ul>

<h3 id="action-values">Action values</h3>

<p><img src="../../../../Typora Images/image-20220111193007555.png" alt="image-20220111193007555" /></p>

<p>q = 어떤 state일때의 action값. 즉 historical 한 의미 가짐.</p>

<p>q는 상태 및 행동의 가치 기능을 나타냄</p>

<h1 id="3-core-concepts">3. Core concepts</h1>

<h2 id="environment">Environment</h2>

<h3 id="reward-signal">reward signal</h3>

<h2 id="agent">Agent</h2>

<ul>
  <li>Agent component
    <ul>
      <li>agent state</li>
      <li>policy</li>
      <li>value function</li>
      <li>model</li>
    </ul>
  </li>
  <li>
    <p>agent의 내부</p>

    <p><img src="../../../../Typora Images/image-20220111193643781.png" alt="image-20220111193643781" /></p>
  </li>
</ul>

<h3 id="agent-state">Agent state</h3>

<p>36분쯤 여길 좀 중점적으로 같이봣으면 좋겟어여ㅠ</p>

<ul>
  <li>예측
    <ul>
      <li>정책을 정의하고 선택해야함.</li>
      <li>agent 내부 사진에 정책에서 작업으로 action으로 향하는 화살표 추가 가능</li>
    </ul>
  </li>
</ul>

<p><img src="../../../../Typora Images/image-20220111200442278.png" alt="image-20220111200442278" /></p>

<h1 id="어떻게-어디로-나눠야할지-모르겟음-">어떻게 어디로 나눠야할지 모르겟음 ;</h1>

<h3 id="environment-state">Environment state</h3>

<p>Environment’s internal state</p>

<p>It is usually invisible to the agent</p>

<p>​	even if it visible it may contains lots of irrelevant information</p>

<ul>
  <li>additional memory가 필요함</li>
</ul>

<h3 id="fully-observable-environments-state">Fully Observable Environments state</h3>

<p>흔한 경우는 아니지만, agent가 모든 environments state를 다 아는경우가 있다. 이때는 observation = environment state</p>

<h4 id="4-markov-decision-process">4. Markov Decision process</h4>

<p>이 이론은 강화학습의 기반이 된 수학적 프레임워크 이론이다.</p>

<p><img src="../../../../Typora Images/image-20220111195217002.png" alt="image-20220111195217002" /></p>

<p>보상확률과 이후의 연속적인 state 에 대한것.</p>

<p>마르코브 의사결정은 state가 우리가 알아야하는 history를 전부 알고있다. 즉 그 state값만 알아도 의사결정을 내리는데 아무 무리가없다.</p>

<p><img src="../../../../Typora Images/image-20220111195607379.png" alt="image-20220111195607379" /></p>

<h3 id="partially-observable-environments">Partially Observable Environments</h3>

<p>These observations are not Markovian</p>

<ul>
  <li>
    <p>ex</p>

    <p><img src="../../../../Typora Images/image-20220111200107127.png" alt="image-20220111200107127" /></p>
  </li>
</ul>

<p>아래 그림 내용 설명할때 잘 이해가 안가요. 45:40</p>

<p><img src="../../../../Typora Images/image-20220111200255965.png" alt="image-20220111200255965" /></p>

<h3 id="policy">Policy</h3>

<p>에이전트의 행동을 정의</p>

<p>단지, 에이전트에서 작업으로의 맵핑입니다.</p>

<p><img src="../../../../Typora Images/image-20220111202046194.png" alt="image-20220111202046194" /></p>

<p>pi : 상태가 주어진 행동의 확률</p>

<h3 id="value-function--value-estimates">value function &amp; value Estimates</h3>

<h4 id="value-function">value function</h4>

<p><img src="../../../../Typora Images/image-20220111202206439.png" alt="image-20220111202206439" /></p>

<p>value 함수는 return에 대한것을 예측할 수있습니다.</p>

<p>value func는 정책에 의존하기 때문에 앞전에 나온 정의를 명시적으로 수정하였습니다.</p>

<p><img src="../../../../Typora Images/image-20220111203848332.png" alt="image-20220111203848332" />위의 식에 새로운 요소</p>

<p>discount factor는 즉시보상과 장기보상의 중요성을 상쇄시킨다?</p>

<ul>
  <li>value func는 상태의 만족도를 평가하는 데 사용할 수 있습니다.</li>
  <li>
    <p>value func는 action들을 선택하는데 사용할 수 있습니다.</p>
  </li>
  <li>
    <p>return</p>

    <p><img src="../../../../Typora Images/image-20220111204821578.png" alt="image-20220111204821578" /></p>
  </li>
  <li>
    <p>value :</p>

    <p><img src="../../../../Typora Images/image-20220111204846660.png" alt="image-20220111204846660" /></p>

    <p>~pi는 (pi가 결정론적인 상태일지라도) 정책 pi에 의해 선택된다는 것을 의미한다. (벨만 방정식, bellman equation)</p>
  </li>
</ul>

<h3 id="model">model</h3>

<p>모델은 다음 상태를 예측하는 모델 p를 가질 수있습니다.</p>

<p><img src="../../../../Typora Images/image-20220111205854381.png" alt="image-20220111205854381" /></p>

<p>a state, an action, 그리고 다음상태의 state를 input으로 관찰 한후, 다음 상태를 예상하는 확률에 대한 근사치</p>

