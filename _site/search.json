[
  
    {
      "title"       : "[ex_15]문자를 읽을 수 있는 딥러닝",
      "category"    : "",
      "tags"        : "",
      "url"         : "./ex_15-%EB%AC%B8%EC%9E%90%EB%A5%BC-%EC%9D%BD%EC%9D%84-%EC%88%98-%EC%9E%88%EB%8A%94-%EB%94%A5%EB%9F%AC%EB%8B%9D.html",
      "date"        : "2022-02-24 00:00:00 +0900",
      "description" : "",
      "content"     : "오늘은 아이펠 Exploration 노드 15번을 공부하고 공부한 내용을 포스팅해보았습니다.출처 AIFFEL LMS 문제시 연락 부탁드립니다. :) 목차 기계는 어떤 방식으로 글을 읽을까? 딥러닝 문자 인식의 시작 사진 속 문자 찾아내기 - detection 사진 속 문자 읽어내기 - recognition keras-ocr 써보기 테서랙트 써보기 프로젝트 : 다양한 OCR 모델 비교하기기계는 어떤 방식으로 글을 읽을까?딥러닝 문자 인식의 시작사진 속 문자 찾아내기 - detection사진 속 문자 읽어내기 - recognitionkeras-ocr 써보기테서랙트 써보기프로젝트 : 다양한 OCR 모델 비교하기"
    } ,
  
    {
      "title"       : "Swea ) 프로그래밍과 논리, 수학 1   복사본",
      "category"    : "",
      "tags"        : "",
      "url"         : "./SWEA-)-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D%EA%B3%BC-%EB%85%BC%EB%A6%AC,-%EC%88%98%ED%95%99-1-%EB%B3%B5%EC%82%AC%EB%B3%B8.html",
      "date"        : "2022-02-24 00:00:00 +0900",
      "description" : "",
      "content"     : "오늘은 삼성 SW Expert Academy의 프로그래밍과 논리, 수학 강의 1강을 보고 학습한 내용을 정리해보았습니다강의 링크https://swexpertacademy.com/main/learn/course/lectureVideoPlayer.do강의 전체 목차 프로그래밍과 논리/수학 ( 오늘의 분량 ) 논리와 증명 ( 오늘의 분량 ) 수와 표현 집합과 조합론 기초 수식 재귀 동적 프로그래밍 조합론 프로그램 과제 기초 알고리즘 프로그램 과제"
    } ,
  
    {
      "title"       : "[fd_30] computer power up",
      "category"    : "",
      "tags"        : "",
      "url"         : "./fd_30-Computer-Power-UP.html",
      "date"        : "2022-02-23 00:00:00 +0900",
      "description" : "",
      "content"     : "오늘은 아이펠 Fundamental 노드 30번을 공부하고 공부한 내용을 포스팅해보았습니다.깃허브 주소 : https://github.com/Nega0619/Aiffel_Fundamental_nodes/blob/main/%5Bfd_30%5D%20Computer%20Power%20UP.ipynb출처 AIFFEL LMS 문제시 연락 부탁드립니다. :) 목차 멀티태스킹 멀티태스킹이란? 프로세스, 스레드, 프로파일링 Scale-up VS Scale-out 멀티프로세스, 멀티 스레드 멀티스레드 멀티프로세스 스레드 / 프로세스 풀 실전예제1. 멀티태스킹1-1. 멀티태스킹이란?자원을 최적화하는 법, 병렬 컴퓨팅, 분산컴퓨팅의 개념은 빅데이터를 다루기 위해선 꼭 알고있어야 하는 중요한 개념들입니다. 한정된 컴퓨팅 자원을 활용하여 여러가지 일을 효율적으로 진행할 수 있도록 해주는 것을 멀티태스킹이라고 합니다. 멀티태스킹은 동시성 병렬성을 가지고 있으며 다음 글을 참고하니 이해하기 쉬웠습니다. Difference Between Concurrency and Parallelism 동시성 : 동시에 많은 것을 처리함(동시성의 환상을 제공) 병렬성 : 속도를 높이기 위해 동시에 많은 일을 하는 것. 동시성하나의 Processor가 여러가지 task를 동시에 수행하는 개념즉, processor가 특정 순간에 1가지 task만을 수행하지만, 라면을 끓일때 물 끓기까지 기다리는 것 처럼 다른 task를 수행할 수 있는 시간에 task를 전환하여 task를 여러개 수행하는 것 처럼 보이게 하는 것입니다.병렬성유사한 task를 여러 processor가 동시에 수행하는 개념병렬성의 효율을 극대화 시키기 위해서는 동시성이 같이 요구되어야 한다.동기 vs 비동기 (synchronious vs asynchronous)동기와 비동기는 동시성에서 주로 다루게 되는 개념입니다.컴퓨터에서 어떤 일을 바로 하지 못하고 대기해야 하는 경우를 일컬어 바운드(bound)되었다.라는 표현을 쓴다고 합니다.바운드 되었을때, 앞 작업이 기다리기를 하염없이 기다리다 다음 작업을 시작하는 것을 동기(synchronize)라고 하고, 바운드 되고 있는 작업을 기다리는 동안 다른 일을 처리하는 것을 비동기(asynchronize)라고 합니다. 동기 : 일이 순차적으로 진행되며, 요청과 요청에 대한 응답이 연속적으로 실행 비동기 : 일이 비순차적으로 진행되며, 요청과 요청에 대한 응답이 비연속적으로 실행. 특정 코드의 연산이 끝날 때 까지 코드 실행을 멈추지 않으며 다음코드를 실행하고 중간에 실행되는 코드는 콜백함수로 연결하여 처리하기도 함I/O Bound VS CPU Bound컴퓨터가 일을 수행하면서 뭔가를 기다릴 때, 즉 속도 제한이 걸리는 경우는 2가지입니다. I/O Bound : 입 출력에서 데이터 처리에 시간이 소요되는 경우 CPU Bound : 복잡한 수식이나 그래픽 작업과 같은 많은 계산이 요구되는 경우1-2. 프로세스, 스레드, 프로파일링프로세스 Process프로세스란 프로그램을 구동해 프로그램 자체와 프로그램의 상태가 메모리상에서 실행되는 작업의 단위입니다. 하나의 프로그램을 실행할 때, 운영체제는 프로세스를 한개를 생성합니다.프로세스는 운영체제의 커널(Kernel)에서 시스템 자원(CPU, 메모리, 디스크) 과 자료구조를 이용합니다.스레드 Thread스레드(thread)란 어떠한 프로그램 내에서 즉, 프로세스 내에서 실행되는 흐름의 단위예를 들어 요리를 만드는 프로그램이 존재한다고 한다면, 요리사는 프로세스, 재료썰기, 밥짓기, 볶기 등의 작업을 스레드에 비유할 수 있습니다. 같은 작업을 빠르게 처리하기 위해 여러 개의 스레드를 생성하기도 합니다.각각의 작업을 할 때 도마나 불판 등의 주방 공간은 공유되지만 요리사 각자의 주방 공간은 공유하지 않습니다. 마찬가지로 프로세스도 자신만의 전용 메모리 공간 (Heap)을 가집니다.그렇기때문에 스레드들도 메모리 공간자체는 공유 하지만 다른 프로세스와 공유하지는 않습니다.프로파일링 Profiling프로파일링이란 것은, 코드에서 시스템의 어느 부분이 느린지 혹은 어디에서 RAM을 많이 사용하고 있는지 확인하고 싶을때 사용하는 기법입니다.프로파일링은 애플리케이션에서 자원이 가장 집중되는 지점을 정밀하게 찾아내는 기법입니다.프로파일링은 애플리케이션을 실행시키고 각각의 함수 실행 시간을 재는 프로그램입니다.프로파일링은 코드의 병목(bottleneck)을 찾아내고 성능을 측정해주는 도구입니다. 참고링크 파이썬 프로파일러 - cProfile, profile line profiler를 사용하여 파이썬의 각 라인이 어떻게 돌아가는지를 알아보자. scale-up VS scale-out scale-up : 컴퓨터 자원을 활용하기 위해서 자원을 업그레이드/최적화 시키는 것 한대의 컴퓨터 성능을 최적화 시키는 것 scale-out :컴퓨터 자원을 활용하기 위해서 자원을 확장 시키는 것 여러대의 컴퓨터를 한 대처럼 사용하는 것 1-3. 멀티 프로세스, 멀티스레드멀티스레드파이썬에서 멀티스레드 구현은 threading 모듈을 이용합니다. 생성방법 1. 클래스에 Thread 상속받기 Class Delivery(Thread): def order(self): print(\"배달이요\") 생성방법 2. Thread 인스턴스화 t = Thread(target=함수이름, args=()) from threading import *from time import sleep Stopped = False def worker(work, sleep_sec): # 일꾼 스레드입니다. while not Stopped: # 그만 하라고 할때까지 print('do ', work) # 시키는 일을 하고 sleep(sleep_sec) # 잠깐 쉽니다. print('retired..') # 언젠가 이 굴레를 벗어나면, 은퇴할 때가 오겠지요? t = Thread(target=worker, args=('Overwork', 3)) # 일꾼 스레드를 하나 생성합니다. 열심히 일하고 3초간 쉽니다.t.start() # 일꾼, 이제 일을 해야지? 😈 스레드 멈추기 t.terminate() 메소드는 없으므로 멈추려면 flag를 체크해서 멈추도록 합니다. # 이 코드 블럭을 실행하기 전까지는 일꾼 스레드는 종료하지 않습니다. Stopped = True # 일꾼 일 그만하라고 세팅해 줍시다. t.join() # 일꾼 스레드가 종료할때까지 기다립니다. print('worker is gone.') 스레드 생성확인 class Delivery(Thread): def order(self): print(\"배달이요\") work2 = Delivery()print(work2.run) 실행결과 &lt;bound method Delivery.run of &lt;Delivery(Thread-21, initial)» 멀티프로세스 생성 방법 1. Process 인스턴스 생성 import multiprocessing as mp def delivery(): print('delivering...') p = mp.Process(target=delivery, args=())p.start() 프로세스 사용 p = mp.Process(target=delivery, args=())p.start() # 프로세스 시작p.join() # 실제 종료까지 기다림 (필요시에만 사용)p.terminate() # 프로세스 종료 스레드 / 프로세스 풀스레드나 프로세느는 사용할때 단독으로 사용하고 종료해주지 않고 스레드 / 프로세스 풀을 사용해서 생성합니다.“풀(Pool)”은 스레드나 프로세스들로 가득 찬 풀장입니다. 스레드 풀을 만들면 각각의 태스크들에 대해서 자동으로 스레드들을 할당하고 종료합니다. 풀 만드는 방법 2가지 Queue를 사용해서 직접 만드는 방법 concurrent.futures 라이브러리의 ThreadPoolExecutor , ProcessPoolExecutor클래스를 이용하는 방법 concurrent.futures 모듈을 이용해서 스레드 풀 만드는 방법 concurrent.futures 기능 4가지 Executor 객체 ThreadPoolExecutor 객체 ProcessPoolExecutor 객체 Future 객체 ThreadPoolExecuterExecutor 객체를 이용하면 스레드 생성, 시작, 조인 같은 작업을 할 때, with 컨텍스트 관리자와 같은 방법으로 가독성 높은 코드를 구현할 수 있습니다.(프로세스도 동일)with ThreadPoolExecutor() as executor: future = executor.submit(함수이름, 인자)multiprocessing.Poolmultiprocessing.Pool.map을 이용하여 여러 개의 프로세스에 특정한 함수를 매핑해 병렬처리하도록 구현하는 방법이 널리 사용with Pool() as pool: result = pool.map(double, [1, 2, 3, 4, 5]) print(result)위의 메소드를 실행하면 double(i)이라는 메소드가 pool을 통해서 각각 다른 pid를 가진 프로세스들 위에 multiprocess로 실행되었다는 것을 확인할 수 있습니다.실전예제깃허브 참고"
    } ,
  
    {
      "title"       : "Swea ) 프로그래밍과 논리, 수학 1",
      "category"    : "",
      "tags"        : "",
      "url"         : "./SWEA-)-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D%EA%B3%BC-%EB%85%BC%EB%A6%AC,-%EC%88%98%ED%95%99-1.html",
      "date"        : "2022-02-23 00:00:00 +0900",
      "description" : "",
      "content"     : "오늘은 삼성 SW Expert Academy의 프로그래밍과 논리, 수학 강의 1강을 보고 학습한 내용을 정리해보았습니다강의 링크https://swexpertacademy.com/main/learn/course/lectureVideoPlayer.do강의 전체 목차 프로그래밍과 논리/수학 ( 오늘의 분량 ) 논리와 증명 ( 오늘의 분량 ) 수와 표현 집합과 조합론 기초 수식 재귀 동적 프로그래밍 조합론 프로그램 과제 기초 알고리즘 프로그램 과제프로그래밍과 논리 / 수학 프로그래밍이 어려운 이유 능숙해지기 위해 많은 훈련이 필요하지만, 이 과정의 중요 목표는 아님! 논리적인 어려움 존재 ex ( 카드문제 ) 사실과, 주장, 문제에서 사실과 주장을 잘 구분해야한다! 사실 : 모든 카드의 한쪽에는 알파벳, 다른쪽에는 숫자 주장 : 만약 한쪽이 D이면 반대쪽은 3 문제 : 주장이 사실인지 확인하기 위해 아래 카드 중 반드시 뒤집어봐야 하는 카드는 몇개이고 어느것인가? 정답 : D와 7D를 뒤집어 봐야한다는 것은, 3인지를 확인해야하기 위함이고,7을 뒤집어 봐야하는 것은 다른 숫자가 적힌 카드의 뒷면에는 D가 없는지를 확인해봐야하기 때문.여기서 F는 확인 안해도 되는 것이 이미 이미 D와 3에 위반된 조건이기 때문이다.F의 뒤에 3이 있을지 걱정하는 경우인데, 주장은 D 뒤에 3이있다는 것이지 F뒤에 3이 잇으면 안된다가 아님! 3이 있던 없던 상관없음!그렇담 3은 확인 안해도 되는것인가? 이건 F의 문제와 귀결되네. D뒤에 3이 존재하는것을 주장하는 거지 3 뒤에 그 어떤것이 와도 상관없다는 것!7을 뒤집어야 하는 이유는 7뒤에 D가 있으면 주장이 깨어지기 때문!나는 여기서 궁금한 것이, 카드가 저기 4장뿐인가 라는 생각은 할 필요가 없는것인가. 카드 종류가 다양할 수 있잖아. 이 경우에는 전부 다 뒤집어야 하나? 주장을 제대로 파악하지 못해서 생긴 문제다. 논리 != 직관 직관이란, 논리적인 느낌을 주는 것. 장점은 익숙한 상황에서는 매우 빠름 단점은 정확하지 않다. 가끔은 익숙한 상황에서도 틀릴 수 있다. 또한, 강한 착각을 일으킬 수 있다. 논리와 증명 soft logic 논리적으로 부정확한 표현을 사용하지만, 어떤 의미인지 모든 사람이 알고있다는 가정 hard logic 프로그래밍 언어 표현의 모두 논리학에서 나온 것입니다. 사용되고 있는 수 많은 알고리즘을 이해하기 위해서는 hard logic이 필요하다. 프로그래밍의 어려움에는 soft logic으로 알고리즘을 이해하려고 하는 것입니다. 알고리즘 설명을 보고 또 봐도 이해가 안되는 것은 증명을 안봤기 때문 증명을 봐도 이해가 안되는 것은 직관으로 이해하려고 하기 때문. 직관으로 완전한 이해를 얻는 것은 불 가 능 논리연습 1 다음을 명제식 형태로 쓰고 참인지 거짓인지 판단하시오 만약 0이 홀수라면, 미국에서 2080년 월드컵이 열린다. 만약이 prime number라면 2는 짝수이다. 1. p 이면 q이다 라는 명제에서 p는 0이 홀수이다. q는 미국에서2080년 월드컵이 열린다 입니다. p이면 q이다 의 명제의 전제는 p가 참일 경우입니다. p가 거짓이면 명제식 자체가 거짓이 됩니다. p이면 q이다 (참) p이면 ~q이다 (거짓) ~p이면 q이다 (참) ~p이면 ~q이다 (참) p이면 q라고 주장했지, ~p이면 ~q라고 말한게 아닙니다! 즉, ~p이면 q, ~q 둘 다 참입니다. 가정이 거짓이되면 전체는 참이됩니다. 1번 : 참 2. p 이면 q이다 라는 명제에서 p는 저 숫자가 prime numer이다. q는 2는 짝수이다. 입니다. p의 사실 여부와 상관없이, q가 항상 참이므로, 2번 명제는 항상 참입니다. p가 참이어도 결론 q는 참, p가 거짓이어도 결론 q는 참이기 때문입니다.논리 연습2p와 q가 명제이고, p -&gt; q가 거짓이라고 하자. 다음 명제식의 참 거짓은 어떻게 되는가?ⓐ ~p -&gt; qⓑ p V qⓒq -&gt; p생각정리.p가 참이면 q의 참 거짓에 따라 거짓이 판별됨.p가 거짓이면 q는 항상 참이다.근데, q는 거짓이라그랬으니까, p는 참인데, q가 거짓인 경우뿐이네!즉, 참인 경우 : ~q or q -&gt; p인 경우 (c) ,거짓인 경우 : ~p -&gt; q인 경우 (a)(a) : 참 (가정이 거짓이 되면 전체는 참이 되기 때문입니다.)(b) : 참 (참 과 거짓 이면 참입니다.)(c) : 참 b의 문법은 뭘까요? p와 q 합집합으로 이해하면 됩니다.논리 연습 3.다음 명제들의 역 이 대우를 쓰시오 만약 0이 홀수라면, 미국에서 2080년 월드컵이 열린다. 만약 가 prime number라면 2는 짝수이다. 역 : 만약 미국에서 2080년 월드컵이 열린다면 0이 홀수이다. 이 : 만약 0이 홀수가 아니라면, 미국에서 2080년 월드컵이 열리지 않는다. 대우 : 미국에서 2080년 월드컵이 열리지 않는다면, 0이 홀수가 아니다. 역 : 만약 2가 짝수라면저 숫자는 prime number이다. 이 : 만약 저 숫자가 prime number가 아니라면, 2는 짝수가 아니다. 대우 : 2가 짝수가 아니라면, 저 숫자가 prime number가 아니다.논리 연습 4. 증명이란? 정확한 명제식으로 표현할 수 있는 것이라야 함 보통은 정확한 명제식까지 쓰지는 않으나 근본적으로는 명제식으로 바꿀 수 있음 당구공 paradox 수학적 귀납법이란? 자연수 n에 관한 명제 P(n)이 모든 자연수 n에 대하여 성립함을 다음과 같이 증명하는 방법 자연수 n에 관한 어떤 명제 P(n)에 대하여 (1) n = 1일 때, 즉 P(1)이 성립함을 증명하고, (2) n = k일 때, 즉 P(k)가 성립한다고 가정하면 n = k+1일 때, 즉 P(k+1)도 성립함을 증명한다. 대부분 사람들이 P(n)이 참이라고 가정할 수없다고 반론함. 수학적 귀납법에서 필요한 것은 P(n)-&gt;P(n+1)이 참임을 보이는 것 뿐이므로, P(n)이 정말로 참일 필요는 없음. P(n)이 거짓이면 전부 통째로 참이되어버리니까 오히려 좋아 위 증명에서 실제로 잘못된 부분은 위의 두 상황에서 처음 뺀 당구공과 두번째로 뺀 당구공의 색이 같음을 알수있으므로,,, 처음뺀 당구공과 두번째로 뺀 당구공의 색이 같다는 것은 공통부분이 있다는 것인데, 실제로 n = 1인 경우, 즉, n+1=2인 경우 공통부분이 없다. n=1인 경우에는 주머니안에 있는 공들의 색이 전부 같다는게 아니라, n=1인경우에는 주머니에 그 공 하나라서 색이 같을 수 밖에 없다는 것이므로 성립하지 않는대. 조금 이해가 안되는 부분이 공을 뺐을 때 다른색깔일 경우는 생각조차 안하는건가? Infinitely Many Prime Numbers"
    } ,
  
    {
      "title"       : "강화학습 치팅시트",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-%EC%B9%98%ED%8C%85%EC%8B%9C%ED%8A%B8.html",
      "date"        : "2022-02-22 00:00:00 +0900",
      "description" : "",
      "content"     : "파이 스타 파이_뉴= 파이 ​ != 파이"
    } ,
  
    {
      "title"       : "[ex_14] find another artist that iu fans will like",
      "category"    : "",
      "tags"        : "",
      "url"         : "./ex_14-Find-another-artist-that-IU-fans-will-like.html",
      "date"        : "2022-02-22 00:00:00 +0900",
      "description" : "",
      "content"     : "오늘은 아이펠 Exploration 노드 14번을 공부하고 공부한 내용을 포스팅해보았습니다.전체 코드는 깃허브를 참고하시고, 여기서는 중요한 메소드만을 작성하였습니다.출처 AIFFEL LMS 문제시 연락 부탁드립니다. :) 목차 추천 시스템이란? 데이터 탐색과 전처리 사용자의 명시적 / 암묵적 평가 Matrix Factorization ( MF ) CSR (Compressed Sparse Row) Matrix MF 모델 학습하기 비슷한 아티스트 찾기 + 유저에게 추천하기1. 추천시스템이란?추천 시스템은 사용자가 선호할 만한 아이템을 추측함으로써 여러 가지 항목 중 사용자에게 적합 한 특정 항목을 선택(information filtering)하여 제공하는 시스템입니다. 고전적 추천 시스템 협업 필터링 대규모의 기존 사용자 행동 정보를 분석하여 해당 사용자와 비슷한 성향의 사용자들이 기존에 좋아했던 항목을 추천하는 기술 행렬분해(Matrix Factorization), k-최근접 이웃 알고리즘 (k-Nearest Neighbor algorithm; kNN) 등의 방법이 많이 사용 협업 필터링을 위해서는 반드시 기존 자료를 활용 BUT 이러한 자료들을 사용자에게 직접 요구해야만 하는것은 아님 단점 Cold Start 협업 필터링은 기존의 자료가 필요한바, 기존에 없던 새로운 항목이 추가되는 경우는 추천이 곤란해지는 현상 사용자 수가 많은 경우 효율 적으로 추천할 수 없음 계산이 몇 시간에서 며칠까지 걸리는 경우가 종종 생김 Long tail 시스템 항목이 많다 하더라도 사용자들은 소수의 인기 있는 항목에만 관심을 보이기 마련이다. 따라서 사용자들의 관심이 적은 다수의 항목은 추천 을 위한 충분한 정보를 제공하지 못하는 경우 콘텐츠 기반 필터링 항목 자체를 분석하여 추천을 구현 항목을 분석한 프로파일(item profile)과 사용자의 선호도를 추출한 프로파일(user profile)을 추출하여 이의 유사성을 계산 아이템 분석 알고리즘이 핵심적 군집분석(Clustering analysis), 인공신경망(Artificial neural network), tf-idf(term frequencyinverse document frequency) 등의 기술이 사용 로 협업 필터링에서 발생하는 콜드 스타트 문제 를 자연스럽게 해결 단점 만 다양한 형식의 항목을 추천하기 어려움 모델 기반 협력 필터링 기존 항목 간 유사성을 단순하게 비교하는 것에서 벗어나 자료 안에 내재한 패턴 을 이용하는 기법 자료의 크기를 동적으로 변화시키는 방법 영 화를 추천하는 경우, ‘해리 포터’ 시리즈 2편을 추천하기 위해서는 ‘해리 포터’ 시리즈 1편, 단 한 편을 좋아했는가가 다른 무엇보다 중요한 요소 추천을 위한 자료의 크기를 변화시키는 방법 잠재(latent) 모델에 기반을 둔 방법 잠재 모델이란 사용자가 특정 항목을 선호 하는 이유를 알고리즘적으로 알아내는 기법 LDA(Latent Dirichlet Allocation), 베이지안 네트워크 (Bayesian Network) 등의 알고리즘이 사용 출처 : 콘텐츠 추천 알고리즘의 진화 2. 데이터 탐색과 전처리 TSV 파일이란? tsv는 Tab-Separated Values의 약자로서, 구분자가 tab(‘\\t’) 문자 CSV 파일이란? csv는 Comma-Separated Values의 약자로서, 구분자가 comma(‘,’) 문자 tsv 파일 읽어오기 data = pd.read_csv(fname, sep='\\t', names= col_names) # sep='\\t'로 주어야 tsv를 열 수 있습니다. pandas에서 사용할 컬럼만 남기기 using_cols = ['user_id', 'artist', 'play']data = data[using_cols] pandas Dataframe에서 유니크한 데이터 개수알아보기 data['user_id'].nunique() pandas Dataframe에서 group by 사용하기 user_median = data.groupby('user_id')['play'].median() 꼭 암기 # 아티스트 이름은 꼭 데이터셋에 있는 것과 동일하게 맞춰주세요. my_favorite = ['black eyed peas' , 'maroon5' ,'jason mraz' ,'coldplay' ,'beyoncé'] # 'hwi'이라는 user_id가 위 아티스트의 노래를 30회씩 들었다고 가정하겠습니다.my_playlist = pd.DataFrame({'user_id': ['hwi']*5, 'artist': my_favorite, 'play':[30]*5}) if not data.isin({'user_id':['zimin']})['user_id'].any(): # user_id에 'zimin'이라는 데이터가 없다면 data = data.append(my_playlist) # 위에 임의로 만든 my_favorite 데이터를 추가해 줍니다. data = data.append(my_palylist)로 하면 index 순서대로가 아닌 0 ,1, 2, 3, 4, …. , 56643,56644, 0,1,2,3,4 순으로 들어오게 됩니다. 인덱스 순서대로 넣게해주려면 data = data.append(my_playlist, ignore_index=True) 처럼 ignore_index=True옵션을 넣어주면 됩니다. https://yganalyst.github.io/data_handling/Pd_2/에서의 inplace와 drop은 안됨. 3. 사용자의 명시적 / 암묵적 평가 명시적 평가 : 좋아요나 별점과 같은 사용자가 직접적으로 드러낸 데이터 암묵적 평가 : 어떤 곡을 몇번 플레이했다 / 어떤 영화를 몇번 봤다와 같은 서비스를 이용하면서 자연스럽게 발생하는 암묵적(Implicit) 피드백 암묵적 데이터를 사용할때 참고하면 좋을 논문 : Collaborative Filtering for Implicit Feedback Datasets 특징 부정적인 피드백이 없다.(No Negative Feedback) 애초에 잡음이 많다.(Inherently Noisy) 수치는 신뢰도를 의미한다.(The numerical value of implicit feedback indicates confidence) Implicit-feedback Recommender System의 평가는 적절한 방법을 고민해봐야 한다.(Evaluation of implicit-feedback recommender requires appropriate measures) 해당 프로젝트에서는 암묵적 데이터의 해석에 대한 규칙을 다음과 같이 정했습니다. 한번이라도 들었으면 선호라고 판단 많이 재생한 아티스트에 대해 가중치를 주어서 확실히 좋아한다고 판단한다.4. Matrix Factorization ( MF )해당 프로젝트에서는 m명의 사용자들이 n명의 아티스트에 대해 평가한 데이터를 포함한 (m*n) 사이즈의 평가 행렬(Rating Matrix)을 만들었습니다.행렬에는 그림과 같이 결측치가 존재하며, 추천시스템의 협업 필터링이란 이 평가행렬을 전제로합니다.추천시스템의 모델은 Matrix Factorization(MF, 행렬 분해) 모델을 사용하였습니다. 기본아이디어는 (m*n) 사이즈의 행렬 R을 (m*k) 사이즈의 행렬 P와 (k*n)사이즈의 행렬 Q로 분해한다면 R이란 P와 Q의 행렬곱으로 표현할수 있다는 것입니다. 아이디어는 단순하지만, k는 m이나 n보다 훨씬 작은 값이므로, 계산량 측면에서 매우 유리하고 MF모델의 성능이 준수하고 Scalability가 좋아서 많이 사용됩니다.실제 영화 추천 시스템에 예를들어 보자면 다음과 같습니다.MF 모델의 목표는 모든 유저와 아이템들에 대해서 K-Dimension 벡터를 벡터를 잘 만드는 것입니다. 벡터를 잘 만드는 기준은 유저 i의 벡터 U_i와 아이템 백터 j의 벡터 I_j를 내적했을 때 유저 i가 아이템 j에 대해 평가한 수치 M_ij와 비슷한지 입니다.MF에도 다양한 변형이 있으며, 해당 프로젝트에서 사용할 모델은 여기 논문에서 제안한 모델입니다.5. CSR (Compressed Sparse Row) Matrix실제로 평가행렬을 만든다고 생각해 보면, 유저수도 수백명에 아티스트는 29만명이 넘습니다. 평가행렬의 값이 1Byte를 가진다고해도 실제 유저수와 아티스트의 곱인 평가행렬의 크기는 엄청날 것이며, 대부분 0으로 채워질 것입니다.이 평가행렬의 전부를 메로이에 올려놓고 작업한다는 것은 불가능하며, 그 대안으로 사용되는 것이 CSR Matrix입니다.CSR Matrix는 Sparse한 matrix에서 0이 아닌 유효한 데이터로만 채우는 Matrix입니다.6. MF 모델 학습하기7. 비슷한 아티스트 찾기 + 유저에게 추천하기"
    } ,
  
    {
      "title"       : "깃허브 토큰 만들기 + 깃허브 커널에서 깃 올리기",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%EA%B9%83%ED%97%88%EB%B8%8C-%ED%86%A0%ED%81%B0-%EB%A7%8C%EB%93%A4%EA%B8%B0-+-%EA%B9%83%ED%97%88%EB%B8%8C-%EC%BB%A4%EB%84%90%EC%97%90%EC%84%9C-%EA%B9%83-%EC%98%AC%EB%A6%AC%EA%B8%B0.html",
      "date"        : "2022-02-21 00:00:00 +0900",
      "description" : "",
      "content"     : "노드복사러에게 남기는 글.공용 git이기 때문에, 여러분은 (토큰을 생성하지 않았을 시) 토큰생성 먼저 하시고, 3-3의 git clone을 이용하여 작성된 노드들을 올려주시면 됩니다. 그 이후에 추가적으로 글을 올리실 때에는 3-1과 3-2를 참고하여 config 파일작성하고, fetch , pull 진행해주신 다음 add commit push 명령어를 실행하시면 됩니다.해당 글은 혹시, 개인적으로 repository를 생성하시고 싶으신 분들을 위해 레포지토리를 아예 생성하는것 부터시작합니다. 참고되셨으면 좋겠네요.이 이상 문제 생길시, 카톡으로 문의바랍니다.리눅스 커널에서 깃허브를 처음 사용해보는 분들을 위해 작성해 보았습니다.해당 포스팅은 깃허브에 가입을 하신 상태를 전제로 진행합니다.리눅스 커널에서 깃허브를 사용하는 방법을 알려드리지만, 깃허브의 add / commit / push등의 용어에 대해서는 설명하지 않습니다.목차 깃허브 토큰 만들기 깃허브 토큰 생성 페이지로 이동하기 토큰 생성하기 로컬 Git과 Github Repository 연결하기 Github repository 생성하기 로컬 Git 생성하기 Github repository와 로컬 Git 연결하기 git config 작성하기 Github repository에 파일 push하기 add와 commit하기 git branch 생성 push하기 이미 생성된 repository에 추가로 commit하고 싶다면? git fetch와 pull git config 작성 git clone하기 0. 깃허브 토큰 만들기깃허브 토큰은, 로컬 git에서 깃허브로 commit 혹은 pull등의 작업을 할 때 권한을 허가받기 위하여 필요합니다.생성 방법은 아래에 설명되어 있으며, 토큰 정보는 단 한번만 보이므로 잘 저장해놓아야 합니다.1. 깃허브 토큰 생성 페이지로 이동하기요약 : 깃허브 profile page &gt; Settings &gt; Developers settings &gt; Personal access tokens &gt; Generate new tokensprofile page &gt; settingssettings &gt; developer settingsdeveloper settings &gt; Personal access tokens &gt; Generate new tokens2. 토큰 생성하기요약 token 설명 작성 토큰 사용기한 설정 토큰 권한 설정 토큰 복사 ( 가장 중요 ) Note : token에 대한 설명을 작성합니다. (skip 가능) Expiration : 토큰 사용기한. 저는 약 6개월이상, 넉넉히 custom하여 2022-07-15일로 설정해주었습니다. 권한 설정 권한이 많을 수록 편리합니다. 깃허브 사용에 익숙하시지 않으시다면, delete 권한은 주지마세요.제가 설정한 권한은 다음과 같습니다. 토큰 복사 ( 가장 중요 ) 사진에 가려진 부분은 생성된 토큰입니다. 현재밖에 볼 수 없으므로, 메모장등에 꼭 복사해둡니다. pull / push 할 때 항상 사용합니다. 1. 로컬 Git과 Github repository 연결하기&lt; 요약 &gt; Github repository 생성하기 로컬 Git 생성하기 Github repository와 로컬 Git 연결하기 git config 작성하기 Git branch 생성하기1. Github repository 생성하기요약 : github profile page &gt; + 버튼 클릭 &gt; New Repository &gt; repository 정보 작성하기 &gt; create repository 버튼 클릭github profile page &gt; + 버튼 &gt; New Repository&lt; 아래 그림 설명 &gt;① : repository이름 작성&lt;span style=\"color:red\"&gt;후에 로컬 Git 폴더명과 동일해야 합니다. &lt;/span&gt;저는 `this_IS_test`라는 repository를 생성했습니다.② : repository에 대한 설명③ : repository의 Access 설정④ : repository 초기화 파일⑤ : 1, 2, 3, 4 설정이 끝난 후, 눌러주면 repository가 생성됩니다.깃허브 초보자가 리눅스 커널에서 git을 사용하려 할 때, 초기화 파일을 추가 하지 않는 것을 추천드립니다.로컬 git에서 깃허브로 push할 때, 로컬 git의 상태가 깃허브의 repository 최신상태와 동일해야 합니다. (fetch &amp; pull한 상태 / 혹은 git clone한 직후.) 깃허브 repository 의 최신 상태와 동일하지 않으면 git push 명령어가 먹히지 않으므로, 초보자에게는 repository의 초기화파일을 만들지 않는 것을 추천드립니다. repository의 상태가 변경되기 때문입니다.초기화 파일을 선택하지 않았을 시, 완료 화면2. 로컬 Git 생성하기요약 리눅스에서 repository명과 동일한 폴더 만들기 git init 리눅스에서 repository명과 동일한 폴더 만들기 대소문자 구분해야합니다. 일부로 이름을 this_IS_test이렇게 한 것은, 대소문자 구분을 해주어야 함을 보여주기 위함입니다. 다음은 리눅스에서 this_IS_test 디렉토리를 만든 화면입니다. 실행화면 리눅스에서 사용한 명령어 $ mkdir 폴더명 &lt;span style=\"color:red\"&gt;이때 폴더명은 repository와 동일한 이름이어야 합니다. &lt;/span&gt; git init 이 폴더를 Git으로 사용하겠다는 설정으로, 생성한 로컬 git 폴더로 이동하여 git init명령어를 사용합니다. 실행화면 리눅스에서 사용한 명령어 $ cd 폴더명 $ git init 3. Github repository와 로컬 Git 연결하기요약 git remote add origin https://github.com/Nega0619/this_IS_test.git git remote add origin https://github.com/Nega0619/this_IS_test.git 로컬 git 폴더 내에서 위 명령어를 실행해줍니다. 이때 깃허브 repository 주소는 사진에서 참고하시면 됩니다. ① : 깃허브의 repository로 이동한 상태에서 URL 복사+.git ② : 초기 생성화면에서 복사 버튼 클릭하면 됩니다. 실행화면 리눅스에서 사용한 명령어 (이미 로컬 깃내에 존재한다는 전제하에) $ git remote add origin https://github.com/Nega0619/this_IS_test.git 4. git config 작성하기요약 git config –global user.email 설정 git config –global user.name 설정 git config 파일 확인하기이미 config 작성을 하신적이 있다면 git config -l 로 설정이 되어있는지 확인하세요. git config –global user.email 설정 “여기안에 이메일” 써야합니다. $ git config –global user.email “h0riya0619@gmail.com” git config –global user.name 설정 “여기안에 name” 써야합니다. $ git config –global user.name “Nega0619” git config 파일 확인하기 $ git config -l 실행화면 2. repository에 파일 push하기&lt; 요약 &gt; add와 commit하기 git branch 생성 push하기 1. add와 commit하기 파일 생성하기로컬 깃에서 repository로 push하기 위해선 파일이 잇어야 합니다. 저는 다음 명령어로 파일을 만들어 주었습니다. ( 이미 올릴 파일이 있으면 skip ) $ cat &gt; test.txt this is test여기까지 작성하고 ctrl+z 입력하면 test.txt에 this is test 라는 파일이 생성됩니다.실행화면 add 깃에 존재하는 전체 파일을 github에 올리려면 *을 사용하면 됩니다. $ git add * commit commit 명령어에서 -m 옵션은 commit 메시지를 설정하는 옵션입니다. 아래 명령어는 깃으로 commit할때 first commit 이란 메시지로 commit하겠다는 뜻입니다. $ git commit -m “first commit” git commit 상태 확인하기 현재 add / commit할 파일들에 대한 정보를 보여줍니다. $ git status 실행화면2. git branch 생성main 이라는 이름의 branch를 생성합니다. $ git branch -M main실행화면3. push하기main이란 branch에 git을 push 합니다. $ git push -u origin main Username for ‘https://github.com’: 이메일넣기 Password for ‘https://h0riya0619@gmail.com@github.com’: 토큰넣기유의사항 이메일은 “” 등을 사용하지 않고 바로 작성합니다. 리눅스 환경의 경우, 비밀번호 입력에는 아무것도 보이지 않습니다. 당황하지 마시고 바로 입력하세요 리눅스 붙여넣기 명령어 : ctrl + shift + v 실행화면3. 이미 생성된 repository에 추가로 commit하고 싶다면?처음 repository를 생성하고 파일을 올리는 것과 다르게, 이미 사용하고 있는 repository에 로컬 git을 만들어 파일을 올리고 싶은 경우, add / commit / push가 안되는 경우가 있습니다.이런 경우, 2가지를 확인해 보면됩니다.1. git config 파일 확인하기리눅스 커널에서 git config 파일이 초기화 된경우가 있습니다.이럴 경우, git config -l명령어로 config 설정을 먼저 확인해 본 후, 1-4 처럼 user.email과 user.name을 설정을 해주면 됩니다.2. git fetch / git pull 명령어 사용하기git의 add / commit / push 오류가 나는 경우, 로컬 git의 상태가 git repository 상태와 다른 경우가 많습니다. 이럴때는 fetch와 pull을 해준 후, add / commit / push를 해주면 해결됩니다.명령어 : $ git fetch $ git pullfetch와 pull 후, push처럼 이메일과 토큰을 작성해주면 됩니다.실행화면### 3. git clone하기1, 2로도 해결이안된다 혹은 로컬 깃을 만들지 않은 상태라면, git clone 명령어를 이용해 이미 존재하는 repository를 최신 상태로 로컬에 가져올 수 있습니다.준비물 : repository URL명령어 $ git clone 깃허브RepositoryURL.git실행화면 : 기존에 있던 this_IS_test 로컬 깃을 삭제한 후, clone을 해보았습니다.만약 안된다면 config 설정은 해주었는지 확인해보세요~!아이펠 복사러들에게 git clone 명령어 선물 git clone https://github.com/Nega0619/Aiffel_nodes.git실행화면"
    } ,
  
    {
      "title"       : "Scikit Learn ) tranform()과 fit_transform()",
      "category"    : "",
      "tags"        : "",
      "url"         : "./scikit-learn-)-tranform()%EA%B3%BC-fit_transform().html",
      "date"        : "2022-02-21 00:00:00 +0900",
      "description" : "",
      "content"     : "https://deepinsight.tistory.com/165https://velog.io/@classe55/fittransform"
    } ,
  
    {
      "title"       : "[fd_29] 사이킷런을 활용한 추천 시스템 입문",
      "category"    : "",
      "tags"        : "",
      "url"         : "./fd_29-%EC%82%AC%EC%9D%B4%ED%82%B7%EB%9F%B0%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-%EC%B6%94%EC%B2%9C-%EC%8B%9C%EC%8A%A4%ED%85%9C-%EC%9E%85%EB%AC%B8.html",
      "date"        : "2022-02-21 00:00:00 +0900",
      "description" : "",
      "content"     : "오늘은 아이펠 29번 fundamental 노드를 읽고 공부한 것을 작성해 보았습니다. 깃허브 : https://github.com/Nega0619/Aiffel_Fundamental_nodes/blob/main/%5Bfd_29%5D%20Introduction%20to%20the%20recommendation%20system%20using%20scikit-learn.ipynb목차 추천시스템이란? 유사도 계산 추천 시스템 종류 콘텐츠 기반 필터링 협업 필터링 사용자 기반 아이템 기반 잠재요인 협업 필터링 실제 추천 시스템 1. 추천 시스템이란? 사용자와 관련한 아이템을 추천해주는 것이라고 한마디로 정의할 수 있습니다.영화 추천 시스템을 예로 설명해보겠습니다.실제 추천 시스템에서는 영화를 아래 사진처럼 좌표평면에 표현합니다.수식을 이용해 정교하게 그리지만, 대략적으로 그리면 다음과 같습니다.이렇게 놓고 보면 거리가 가까운 영화들은 장르가 비슷하다고 생각할 수 있으며, 사용자의 선호도가 어디에 위치하느냐에 따라 사용자에게 맞는 영화를 추천해 줄 수 있습니다.이러한 추천은 사용자가 이용한 콘텐츠를 기반으로 추천해 주는 방식입니다. 하지만 만약 뉴비 사용자가 나타났다면 어떻게 추천을 해줄까요?보통은 취향에 대한 정보를 받도록 설문조사가 배치되어있지만 전부 다 스킵하고 정말 인적사항만 아는 사용자의 경우가 있습니다. 이 경우, 뉴비 사용자의 인적사항과 기존 회원들의 인적사항을 분석하여 유사도를 측정하고, 뉴비 사용자에게 유사한 인적사항을 가진 사용자의 선호를 추천해주게 됩니다.간단한 추천 로직이며, 이를 통해 두가지를 꼭 기억해야 합니다. 범주형 데이터를 다룬다. 이산적, 혹은 범주형이라고 합니다. 액션 / 로맨스 / 호러 등 카테고리가 나뉜 형태의 데이터입니다. (숫자 벡터로 변환한 후) 유사도를 계산한다. 범주형 데이터를 좌표에 나타내기 위하여 숫자로 이루어진 벡터로 변환합니다.(numerical vector) 그리고 그 거리를 계산해 유사도를 측정합니다. 2. 코사인 유사도가장 유명한 유사도를 계산하는 방법입니다. 코사인 유사도는 두 벡터간의 코사인값을 계산하여 유사도를 측정하게됩니다.코사인 유사도는 두 벡터가 방향이 이루는 각도에 코사인을 취해 구합니다.위 그림은 두 벡터가 이루는 각도에 따른 유사도입니다. 1에 가까울 수록 유사도가 높다고 할 수 있습니다. 수식은 다음과 같습니다.numpy를 이용한 코사인 유사도from numpy import dotfrom numpy.linalg import normdef cos_sim(A, B): return dot(A, B)/(norm(A)*norm(B))사이킷 런을 활용한 코사인 유사도from sklearn.metrics.pairwise import cosine_similarityt1 = np.array([[1, 1, 1]])t2 = np.array([[2, 0, 1]])cosine_similarity(t1,t2)다른 방법의 유사도 측정방법유클리드 거리 / 자카드 유사도 / 피어슨 상관계수 등의 방법이 있습니다.3. 추천 시스템의 종류 콘텐츠 기반 필터링 (Content Based Filtering) 협업 기반 필터링 (Collaborative Filtering 사용자 기반 아이템 기반 잠재요인 협업 필터링 (latent factor collaborative filtering) → 행렬 인수분해(matrix factorization) DeepLearning 적용 Hybrid방식 여러 방법을 결합한 방식 3-1. 콘텐츠 기반 필터링 방식콘텐츠 기반은 오직 콘텐츠의 내용만을 비교해 추천하는 방식입니다.아까 예를 들었던 영화로 말하자면, 사람들이 영화를 볼때 고려하는 점은 다음과 같습니다. 장르, 배우, 감독 등의 정보. 이러한 요소들을 영화의 특성(Feature)이라고 하고 이 특성을 바탕으로 콘텐츠의 유사도를 측정하게 됩니다. 코드해당 예제는 CODE HEROKU의 ‘Building a Movie Recemmendation Engine in Python using Scikit-Learn’ 을 바탕으로 제작되었습니다. 전체코드는 깃허브를 참고하시고, 블로그에는 새로 배운 것 위주로 작성하였습니다.import pandas as pdimport numpy as npfrom sklearn.feature_extraction.text import CountVectorizerfrom sklearn.metrics.pairwise import cosine_similarity os.getcwd() : 현재 파일이 실행되고 있는 working directory 출력 scikit-learn.CountVectorize() : 단순하게 등장 횟수를 세어 숫자 벡터로 만들어줌. out : CSR(Compressed Sparse Row) Matrix CSR Matrix란? Sparse한 matrix에서 0이 아닌 데이터로 채워지는 데이터 값과 좌표 정보로만으로 구성하여 메모리 사용량을 최소화하고 Sparse한 matrix와 동일한 행렬을 표현할 수 있도록 하는 데이터 구조 CountVectorize().fit_transform(pandas.dataframe) 데이터에 맞춘 다음 변환합니다. cosine_similarity() : 코사인 유사도를 구하는 메소드 in : CSR Matrix 3-2. 협업 필터링 방식과거의 사용자 행동 양식(User Behavior) 데이터를 기반으로 추천하는 방식으로, 잠재요인 기법을 활용하여 행동 양식을 데이터로 나타냄기본원리다음과 같은 테이블이 있다고 생각해봅시다. timestamp는 사용자가 평점을 매긴 시간입니다. 위 데이터를 사용자와 아이템간의 interaction matrix로 변환해봅시다. 그리고 해당 행렬의 데이터로 평점을 넣으면 아래와 같은 형식이 됩니다. 이를 평점행렬이라고 부르기도 합니다.이러한 행렬은 굉장히 희소(sparse)한 행렬이 됩니다.평점 행렬로 변환 후, 평점 행렬의 유사도를 계산하여 추천하는 방식 : 사용자기반, 아이템 기반 방식평점행렬을 분해하여 더 많은 정보를 고려하는 방식 : 잠재요인 필터링정리하자면 다음과 같습니다. 협업 필터링의 종류 사용자 기반 유사도 계산방식 아이템 기반 유사도 계산방식 잠재요인 기반 행렬인수분해 방식 사용자기반 당신과 비슷한 고객들이 다음 상품을 구매했습니다.위 글을 시스템으로 만든 것이 사용자 기반 추천시스템입니다.user 4가 아이템 1을 구매하고, user 4와 가장 유사한 user 2는 아이템 1-4까지 위와 같은 평점을 남겼다고 합시다. 이 경우, user2가 선호한 제품인 item 3을 4에게 추천해주는 방식입니다.아이템기반 이 상품을 선택한 다른 고객들은 다음 상품을 구매했습니다.위 말을 시스템으로 만든 것이 아이템 기반 추천 시스템입니다. 일반적으로 사용자 기반보다는 아이템 기반 방식이 정확도가 높다고 합니다.user2가 위와 같은 선호도를 가졌다할 때, 아이템 1의 선호도가 가장 높습니다.그리고 아이템 1의 선호도는 다음과 같습니다.아이템 2을 좋아한 사람은 user4이므로 user4에게 user2가 좋아한 상품인 3을 추천해줍니다.잠재요인 협업 필터링 기반평점행렬을 행렬 인수분해(matrix factorization)를 통해 잠재요인(latent factor)을 분석행렬 인수분해 종류 SVD(Singular Vector Decomposition) ALS(Alternating Least Squares) NMF(Non-Negative Factorization)SVD Singular Vector Decomposition가장 많이 사용됨, 우리말로는 특잇값 분해M*N 형태의 행렬 A를 다음과 같은 형태로 분해하여 나타냄 구현 메소드 : https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html 코드 np.diag(Sigma) : 대각선을 추출하거나 대각선 배열을 구성합니다. np.dot(np.dot(U, Sigma_mat), VT) : 두 배열간의 내적 참고 링크 특잇값분해 [선형대수학 #4] 특이값 분해(Singular Value Decomposition, SVD)의 활용 Truncated SVD = LSA Latent Sementic Analysis우리말로 번역하면 잘린 SVD, 다른 말로는 LSA(Latent semantic analysis), 잠재 의미 분석 이라고 합니다.Truncated SVD를 이용해 분해한 뒤, 복원하면 SVD처럼 완벽히 나오지않습니다. 왜냐하면 Truncated SVD가 차원을 축소한 다음 행렬을 분해하기 때문입니다.SVD(특이값 분해)를 평가행렬에 적용해 잠재요인을 분석하는 것을 도식화하면 사진과 같습니다.표기법 R: 사용자와 아이템 사이의 행렬 P: 사용자와 잠재요인 사이의 행렬 Q: 아이템과 잠재요인 사이의 행렬 —&gt; 전치 행렬 형태로 나타냄사용자가 아이템에 대한 평점을 매기는 요인으로 많은 항목들이 있습니다. 그리고 평점을 매기는 것은 매우 주관적입니다. 그렇기 때문에, 사용자가 평점을 매기는 요인을 잠재요인으로 취급한 뒤, SVD 기법을 이용해 분해하고, 다시 합치는 방법으로 영화에 평점을 매긴 이유를 벡터화하고 이를 기반으로 추천합니다. 이 기법은 넷플릭스, 왓챠, 유튜브와 같은 기업에서 효과를 입증해냈고 이후 많은 기업들도 채택하였다고 합니다.이렇게 협업 필터링을 이용하면 사용자가 아이템에 대해 평점을 매긴 평점행렬을 인수분해(Matrix Factorization)를 통해 잠재요인을 분석한 뒤 유사도를 계산할 수도 있으며 사용자의 평점도 예측할 수 있습니다. 특히 잠재요인을 고려해 행렬을 인수분해하면 파라미터수는 감소하게 됩니다.구현 메소드 : https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html3-3. 실제 추천 시스템오늘 작성한 내용들은 추천 시스템에 기본이 되는 요소입니다. 실제로 대기업에서는 더 많은 것을 추천에 고려합니다.사용자의 구매여부, 평점데이터는 기본이고 얼마나 오랜 시간 시청(혹은 사이트에 머물렀는지)했는지, 어떤 사이트에서 유입되었는지, 시청 후 구매로 이어지기까지의 시간 등 행동 족적을 다 분석합니다. 이를 전문 용어로 Digital Footprint(디지털 발자국), Digital Shadow(디지털 그림자)라고 합니다.이중 가장 중요한 지표가 **클릭룰( CTR , Click Through Rate) **입니다. 참고: Clickthrough rate (CTR): Definition어떻게든 다양한 데이터들을 모아 추천을 한 뒤, 적절한 추천이었는지 아니었는지를 평가하는 것 또한 중요한 일입니다. 추천한 제품이 구매 혹은 시청으로 이루어졌는지를 통해 추천의 성공여부를 평가하기도 하고, 모델 단계에서 평가하기도 합니다.추천시스템은 매우 큰 시스템입니다. 데이터를 기반으로 고객에게 적절한 제품 혹은 콘텐츠를 추천하고, 이것이 구매 , 결제로 이루어 지는 것은 매출과 직결되는 문제기도 합니다. 좋은 추천 시스템을 만들기 위해서는, 어떤 데이터를 사용할지 매우 고민을 해야합니다. 어떤 데이터가 사용자와 연관성이 있을지, 구매와 직결되는 각종 데이터는 무엇인지 고려하고 수집, 정렬(sorting)하여 다시 순위(ranking)를 매긴 다음 평가하는 작업을 반복하며 적합한 데이터와 추천 시스템이 만들어냅니다. 추천시스템이 머신러닝이 적용될 수도 있는거지 머신러닝안에 추천시스템이 있는것은 아닙니다.출처 AIFFEL LMS 문제시 연락 부탁드립니다. :) 5"
    } ,
  
    {
      "title"       : "Git lfs 깃에 100mb보다 큰 파일 올리기",
      "category"    : "",
      "tags"        : "",
      "url"         : "./Git-LFS-%EA%B9%83%EC%97%90-100MB%EB%B3%B4%EB%8B%A4-%ED%81%B0-%ED%8C%8C%EC%9D%BC-%EC%98%AC%EB%A6%AC%EA%B8%B0.html",
      "date"        : "2022-02-21 00:00:00 +0900",
      "description" : "",
      "content"     : "git lfs install git lfs track “*.exe” git config –global http.postBuffer 20971520 git config –global http.maxRequestBuffer 100M git lfs 사이트 https://git-lfs.github.com/ 참고 사이트 https://jamie-dev.tistory.com/79 https://goodtogreate.tistory.com/entry/Github%EC%97%90-100Mb%EC%9D%B4%EC%83%81-%ED%8C%8C%EC%9D%BC-%EC%98%AC%EB%A6%AC%EB%8A%94-%EB%B0%A9%EB%B2%95-%EC%98%A4%EB%A5%98-%ED%95%B4%EA%B2%B0 https://medium.com/@stargt/github%EC%97%90-100mb-%EC%9D%B4%EC%83%81%EC%9D%98-%ED%8C%8C%EC%9D%BC%EC%9D%84-%EC%98%AC%EB%A6%AC%EB%8A%94-%EB%B0%A9%EB%B2%95-9d9e6e3b94ef"
    } ,
  
    {
      "title"       : "[fd_15]",
      "category"    : "",
      "tags"        : "",
      "url"         : "./fd_15.html",
      "date"        : "2022-02-20 00:00:00 +0900",
      "description" : "",
      "content"     : "출처 AIFFEL LMS 문제시 연락 부탁드립니다. :) 5"
    } ,
  
    {
      "title"       : "Selenium 연습기록",
      "category"    : "",
      "tags"        : "",
      "url"         : "./Selenium-%EC%97%B0%EC%8A%B5%EA%B8%B0%EB%A1%9D.html",
      "date"        : "2022-02-20 00:00:00 +0900",
      "description" : "",
      "content"     : "전제사항 파이썬이 이미 설치된 상황이다.참고링크 https://greeksharifa.github.io/references/2020/10/30/python-selenium-usage/오늘은 셀레니움이라는 패키지의 설치 및 사용방법에 대해 공부해보았다.셀레니움은 파이썬 패키지 중 하나로, 웹 자동화에 많이 사용된다고 한다.셀레니움 시작하기설치법cmd창에서 pip install selenium드라이버 다운사용하고 있는 브라우저 별로 셀레니움 웹드라이버를 다운로드해야한다. 사용하고 있는 웹 버전과 동일한 버전의 셀레니움 브라우저를 다운받아야 한다! Google Chrome Firefox Microsoft Edge Safari브라우저 열기from selenium import webdriverchrome_driver = webdriver.Chrome('chromedriver.exe') 드라이버 다운하지 않았을 시 혹은 경로를 설정해주지 않았을 경우 뜨는 오류 해결법 지금 작성하고 있는 파이썬 파일 이름이 test.py라고 했을 때, test.py가 있는 폴더 안에(같은 폴더안에) 드라이버를 넣어주면 정상작동한다. 경로를 설정해주지 않았을 경우, 기본값은 현재 test.py가 존재하고있는 폴더의 경로이기 때문이다. 브라우저 버전과 맞지 않는 셀레니움 드라이버를 다운받았을 경우, 어떤 창이 켜졌다가 바로 꺼지며 다음과 같은 오류를 내뱉는다. 브라우저 닫기driver.close() # 현재 탭 닫기driver.quit() # 현재 브라우저 닫기페이지 앞으로 가기 / 뒤로 가기driver.forward()driver.back()엘레멘트에 접근하기엘레멘트란?"
    } ,
  
    {
      "title"       : "01. uml을 소개합니다",
      "category"    : "",
      "tags"        : "",
      "url"         : "./01.-UML%EC%9D%84-%EC%86%8C%EA%B0%9C%ED%95%A9%EB%8B%88%EB%8B%A4.html",
      "date"        : "2022-02-20 00:00:00 +0900",
      "description" : "",
      "content"     : "1장에서는 가벼운 개념 정리만 있고, 다음장에서 세분화하여 더 자세하게 설명할 예정이다.UMLUnified Modeling LanguageUML은 다이어그램을 그리는데 사용되며, 시스템 분석을 목표로 만들어진 언어이다.다이어그램시스템을 여러 가지 시각에서 볼 수 있는 뷰를 제공하는 것이며, 뷰의 집합을 모델이라고 한다.UML 1.X 버전UML 모델시스템 자체의 “목적 행동“을 설명하는 언어, X구현 방법을 설명하는 수단은 아니다X클래스 다이어그램클래스 : 비슷한 속성과 공통적인 행동 수단을 지닌 것들의 범주 혹은 그룹윗 영역에는 클래스의 이름을 넣고, 둘째 영역은 속성(특성)을, 마지막 영역에는 오퍼레이션(행동)을 넣는다.객체 다이어그램객체 : 클래스의 인스턴스인스턴스의 이름은 콜론(:)의 왼편에 쓰며, 클래스의 이름은 콜론의 오른편에 쓴다.오른쪽 그림과 같이 익명 객체도 가능하다.유스케이스 다이어그램유스케이스(use case) : 사용자의 입장에서 본 시스템의 행동을 일컫는다.막대 인간 그림을 액터Actor라고 하며, 네모칸은 시스템 범위를 의미한다.상태 다이어그램객체는 시간에 따라 각기 다른 상태에 있을 수 있으며, 그림에서 왼쪽의 기호는 시작 상태(start state)를 오른쪽의 기호는 종료상태(end state)를 의미한다.시퀀스 다이어그램클래스 다이어그램과 객체 다이어그램은 정적인 정보를 나타낸다. 하지만 특정한 행동을 수행하는 시스템에서는 여러개의 객체들이 서로 메시지를 주고받으며 작업을 진행하는 것이 보통이다. UML 시퀀스 다이어그램은 객체들끼리 주고받는 메시지의 순서를 시간의 흐름에 따라 보여 주는것이다.이때, 객체가 메시지를 보낼 때 자기 자신에게도 보낼수 있음을 유념해야 한다.활동 다이어그램유스케이스 내부 혹은 객체의 동작중에 발생하는 활동을 다이어그램으로 나타낸 것이다.통신 다이어그램시간의 경과를 보여주는 시퀀스 다이어그램과 달리, 메시지에 숫자를 붙임으로써 메시지에 대한 순서를 나타낸다.컴포넌트 다이어그램컴퓨터 시스템을 나타낼수 있도록 만들어진 다이어그램이다.(with 배포 다이어그램)배포 다이어그램컴퓨터 시스템을 나타낼수 있도록 만들어진 다이어그램이다.(with 컴포넌트 다이어그램)특히, 시스템의 물리적 구조를 나타낸다. 컴퓨터와 부가장치, 그리고 각각의 연결 관계뿐만 아니라 각각의 기계에 설치된 소프트웨어까지 표시한다. 컴퓨터는 큐브로 나타내며 컴퓨터사이의 연결 관계는 선으로 이어줌으로써 나타낸다.이외의 것들노트노트에는 부가설명을 쓸 수있으며 한쪽 귀가 접힌 사각형으로 나타내며, 연결은 점선으로 한다.키워드와 스테레오 타입스테레오 타입은 기존이 UML 요소를 기본으로 하여 다른 요소를 새로 만들 수 있게 하는 장치이다.원하는 이름을 거듭 인용표(≪≫)로 감싸는 것으로 간단히 마무리 되며, 이에 감싸여진 이름을 키워드라고 한다.키워드는 개체가 원래 존재하는 의미로서가 아닌 어떤 다른 목적으로 사용되었음을 나타낸다.UML 2.X 버전복합체 구조 다이어그램각 콤포넌트 클래스를 전체 클래스 안에 위치시킴으로써 넓이의 개념을 포함한다.교류 개요 다이어그램타이밍 다이어그램패키지 다이어그램탭이 달린 폴더로 나타낸다. 클래스 혹은 컴포넌트가 특정한 서브 시스템을 구성하는 요소로 다수 존재한다면 패키지 안에 그려주면 된다."
    } ,
  
    {
      "title"       : "Pandas dataframe과 series",
      "category"    : "",
      "tags"        : "",
      "url"         : "./Pandas-Dataframe%EA%B3%BC-Series.html",
      "date"        : "2022-02-19 00:00:00 +0900",
      "description" : "",
      "content"     : "오늘은 Pandas에서 Dataframe과 Series에 대해서 알아보고 각각의 메소드를 정리해 보았습니다.참고 자료 https://yganalyst.github.io/data_handling/Pd_1/Pandas의 기본 자료 구조판다스는 AI와 머신러닝에서 자주 쓰이는 파이썬 라이브러리입니다. 데이터 분석을 할 때, 데이터의 수집, 전처리 등의 과정이 대부분 Dataframe의 형태로 이루어지기에 많이 사용되는 라이브러리입니다.Dataframe은 데이터 테이블(표)를 자료구조로 표현해 놓은 것이며, 행과 열로 이루어져있다.Series는, Dataframe의 하위 자료형이라고 보면된다. 1개의 열이 시리즈이고, 시리즈들이 모여 Dataframe을 이루는 구조이다."
    } ,
  
    {
      "title"       : "Database",
      "category"    : "",
      "tags"        : "",
      "url"         : "./Database.html",
      "date"        : "2022-02-19 00:00:00 +0900",
      "description" : "",
      "content"     : "1장테이블 조회하기select * from 테이블명 : 정보까지 다 나옴desc 테이블명 (describe) : 속성 정보만 나옴예쁘게 조회하기는 생략 (31pg)컬럼명 별칭으로 출력하기 select Deptno, '111' \"DNAME\" FROM dept; “속성명”이 모두 동일하게 111로출력됨.‘’ 과 달라!select Deptno (as) \"1\", '111' \"DNAME\" FROM dept;Deptno를 1로 출력하는 방법Distinct 명령어로 중복된 값을 제거하고 출력하기SELECT DISTINCT deptno FROM emp; 정렬은 아니고 그냥 중복값만 없애줌. SELECT 바로 뒤에 와야함.SELECT DISTINCT job, ename FROM emp; job에만 DISTINCT적용되지 않음. job, ename전부에 적용됨.연결 연산자로 컬럼 붙여서 출력하기SELECT ename || job FROM emp;연결연산자 || 사용SELECT ename ||'''s job is '|| job \"Name and Job\" FROM emp;컬럼명 || 내용 -&gt; 컬럼값내용 출력. 이때 ` 쓰려면 `` 두 개 써야함.Where 절SELECT * FROM emp WHERE ename = ‘smith’; 문자와 날짜를 조회하고 싶을 때에는 꼭 작은 따옴표를 붙여야한다. 문자는 대소문자 구별이 필요하고 날짜는 대소문자 구별이 필요없다.sql에서 기본 연산자 사용하기 +-*/는 컬럼명에 바로 해주어도 된다. 추가적인 연산자 : IN(a,b,c) : a나 b나 c인 조건 검색`LIKE’ : 특별한 패턴이 있는 조건 검색 % : 글자 수에 제한이 없고 어떤 글자가 와도 됨. SELECT empno, sal FROM emp WHERE sal LIKE 1% 1300 1600 등 출력 _ : 글자 수는 한 글자가만 올 수 있고 어떤 글자가 와도 됨. SELECT ename, sal FROM emp WHERE ename LIKE ‘A_____’ ALLEN 출력BETWEEN a AND b : a와 b사이 조건 검색IS (NOT) NULL : WHERE ename IS NOT NULLAND 와 OR중 AND 먼저 수행 후 OR 수행 됨. 정렬하여 출력하기 – ORDER BY 오름차순 : ASC 내림차순 : DESC ORDER BY 컬럼명 DESC/ASC집합연산자 UNION (ALL) : 두 집합의 결과를 합쳐서 출력. 중복 값 제거하고 정렬(중복 값 제거 안하고 정렬 안함) INTERSECT : 교집합 결과를 출력 및 정렬 MINUS : 차집합 결과를 출력 및 정렬(순서 중요) SELECT empno, salFROM empMINUSSELECT empno, salFROM empWHERE sal &gt; 2500; 2장 단일행 함수 : 한번에 하나씩 처리하는 함수 복수행 함수 : 여러 건의 데이터를 동시에 입력받아 출력 1건을 만들어줌.문자열처리함수INITCAPSELECT ename, INITCAP(ename) “INITCAP” FROM emp;영어에서 첫 글자만 대문자로 출력하고 나머지는 전부 소문자로 출력하는 함수LOWERUPPERLENGTH , LENGTHBLENGTH : 입력된 문자열의 길이 계산.LENGTHB : 입력된 문자열의 바이트 계산.CONCAT|| 연산자와 동일한 기능SELECT CONCAT(ename, job) FROM emp WHERE deptno = 10;SUBSTR특정 길이의 문자만 골라낼 때SELECT SUBSTR(‘abcde’, 3, 2) from dual; cdSELECT SUBSTR(‘abcde’, -3, 2) from dual; cdSELECT SUBSTR(‘abcde’, -3, 4) from dual; cde 마이너스를 붙이면 파이썬처럼 검색 글자수 세는 것은 왼쪽 -&gt; 오른쪽 SUBSTRB : 세는 단위가 바이트컬럼에 ‘아아’ 넣으면?SELECT ename, ‘A-B’ FROM emp;ename개수 만큼 A-B 출력 컬럼명은 ‘A-B’INSTR주어진 문자열이나 컬럼에서 특정 글자의 위치를 찾아줌.SELECT INSTR(‘A-B-C-D-E’, ‘-’, 1, 3) FROM dual; 6SELECT INSTR('A-B-C-D-E', '-', -1, 3) FROM dual; 4SELECT INSTR('A-B-C-D-E', '-', -4, 3) FROM dual; 2 n번째부터 포함해서 검색. 위치는 맨처음부터 인덱스로 계산 -하면 검색도 마이너스 방향으로 검색LPAD (컬럼명, 맞출 글자수, 채울 문자리스트)select ename, LPAD(ename, 10, '123456789') from emp;사원이름을 총 10바이트로 출력하되 해당 빈자리에는 해당자리 숫자로 채우기.‘123456789’가 아닌 1-9 또는 ‘1-9’는 안됨. RPAD도 있음응용문제사원의 이름을 9자리로 출력하되 오른쪽 빈자리는 해당 자리수에 해당하는 숫자가 출력되도록 하시오. select ename, RPAD(ename, 9, substr(‘123456789’, length(ename)+1, 9))from empwhere deptno = 10;LTRIMselect ename, LTRIM(ename, 'J') from emp; 제거. RTRIM도 있음REPLACEREPLACE (컬럼명, 문자1, 문자2) 문자1을 문자2로 바꿔줌. student 테이블에서 학생들의 이름과 전화번호, 전화번호에서 국번부분만 처리해서 출력하시오. 국번은 3자리.select name, tel, replace(tel, substr(tel, Instr(tel, ‘)’, 1)+1,4), ‘**’) from student;숫자 관련 함수ROUND/TRUNC(숫자, 출력되길 원하는 숫자자리수) 0: 1의자리 숫자까지 나옴 -1: 10의자리 숫자까지 나옴 1: 소수점 첫째자리까지 나옴 ROUND : 반올림임 TRUNC : 버림MOD/CEIL/FLOOR(숫자) MOD : 나머지 값구하는 함수 CEIL : 주어진 숫자에서 가장 큰 정수를 구하는 함수, 올림 FLOOR : 주어진 숫자에서 가장 작은 정수를 구하는 함수, 내림POWER(숫자, 숫자2)숫자^숫자2날짜 관련 함수 91pg형변환 함수일반함수NVLNULL값을 만나면 다른 값으로 치환해서 출력하는 함수NVL(sal, 0) sal값이 NULL이면 0 출력NVL2NULL값이 아니라면 다른 값으로 치환해서 출력DECODEif문과 동일, SELECT에 사용 DECODE (컬럼1, 컬럼2, 맞으면출력, 아니면출력) DECODE (컬럼1, 컬럼2, 1=2이면 출력, 컬럼3, 1=3이면 출력, 둘다 아니면 출력) DECODE 안의 DECODE도 가능case문CASE 조건 WHEN 결과1 THEN 출력1 [WHEN 결과2 THEN 출력2] ELSE 출력3END “컬럼명”내부에 콤마가 사용되지 않는다.정규식 함수로 다양한 조건 조회하기 123pg3장GROUP 함수 종류COUNTSUMAVGMAX, MINSTDDEV , VARIANCE STDDEV : 표준편차 구하는 함수 VARIANCE : 분산 구하는 함수GROUP BY 절을 이용하여 특정 조건으로 세부적인 그룹화하기 주의점 SELECT절에 사용된 그룹 함수 이외의 컬럼이나 표현식은 반드시 GROUP BY절에 사용되어야 한다. 그렇지 않을 경우 에러 발생 =&gt; 컬럼이나 표현식 전부 사용되어야 한다.``` SQLSELECT deptno, job, AVG(NVL(sal, 0)) “AVG_SAL”FROM empGROUP BY deptno;/* 에러 */ SELECT deptno, jobFROM empGROUP BY deptno;/* 에러 */SELECT deptno, job, AVG(NVL(sal, 0)) “AVG_SAL”FROM empGROUP BY deptno, job;/* ㅇㅋ */- GROUP BY절에는 반드시 컬럼명이 사용되어야 하며 컬럼 Alias는 사용하면 안된다.# HAVING절을 사용해 그룹핑한 조건으로 검색하기그룹 함수를 조건으로 사용하고 싶을 경우에는 WHERE 대신에 HAVING 절을 사용하면 된다. WHERE절에는 사용 안됨!GROUP BY조건절 같이 써야한다.**GROUP함수를 조건으로 할 경우에는 WHERE을 사용하면 안된다.** ``` SQLSELECT deptno, AVG(NVL(sal,0))FROM empWHERE deptno &gt; 10GROUP BY deptnoHAVING AVG(NVL(sal, 0)) &gt; 2000;반드시 알아야 하는 다양한 분석 함수들 167 pgROLLUP()각 기준별 소계를 요약해서 보여줌SELECT EMPNO, JOB, AVG(SAL)FROM EMPWHERE EMPNO &gt; 7500GROUP BY ROLLUP(EMPNO, JOB);직원번호별(직원번호, NULL), 직원과 직업별(직원번호, 직업별) 전체그룹집계 (NULL, NULL)SELECT DEPTNO, JOB, AVG(SAL)FROM EMPWHERE EMPNO &gt; 7700GROUP BY ROLLUP(DEPTNO, JOB);직원번호별(EMPNO, 직원번호, NULL), 직원과 직업별(EMPNO, 직원번호, 직업별) 전체그룹집계 (EMPNO, NULL, NULL)CUBE()소계와 전체 합계까지 출력하는 함수rollup과 비슷하지만 cube(deptno, job)하면, deptno별 job별 deptno,job별, 전체그룹별GROUPING SETS()SELECT NAME, GRADE, DEPTNO1, COUNT(*), SUM(WEIGHT)FROM STUDENTGROUP BY GROUPING SETS(GRADE, DEPTNO1), NAME; NAME, GRADE, NULL NAME, NULL, GRADESELECT NAME, GRADE, DEPTNO1, COUNT(*), SUM(WEIGHT)FROM STUDENTGROUP BY GROUPING SETS(GRADE), NAME, DEPTNO1; 그냥 테이블 조회와 동일SELECT NAME, GRADE, DEPTNO1, COUNT(*), SUM(WEIGHT)FROM STUDENTGROUP BY GROUPING SETS(GRADE, DEPTNO1, NAME); GRADE, NULL, NULL NULL, DEPTNO1, NULL NULL, NULL, NAMELISTAGG()SELECT에 사용LISTAGG(컬럼명, ‘구분자’) WITHIN GROUP (ORDER BY 컬럼명 혹은 조건)이때 오더바이 조건에 HIREDATE 들어가면 고참-&gt;신참 순PIVOT / UNPIVOT 185pg(중간확인용)SELECT MAX (DECODE(DAY, ‘SUN’, DAYNO)) SUN, … MAX (DECODE(DAY, ‘SAY’, DAYNO)) SATFROM CALGROUP BY WEEKNOORDER BY WEEKNO;SELECT * FROM (SELECT WEEKNO “WEEK”, DAY, DAYNO FROM CAL)PIVOT( MAX (DAYNO) FOR DAY IN (‘SUN’ AS “SUN”, ‘MON’ AS “MON”, ‘TUE’ AS “TUE”, ‘WED’ AS “WED”, ‘THUR’ AS “THUR”, ‘FRI’ AS “FRI”, ‘SAT’ AS “SAT” )달력만듬.DAYNO가 가로로 나옴./* 학년 별로 사람수세기/SELECT * FROM (SELECT STUDNO, GRADE FROM STUDENT)PIVOT (COUNT(STUDNO) FOR GRADE IN (4, 3, 2, 1));COUNT () 하면 안됨.SELECT * FROM UPIVOT [PIVOT 결과를 테이블로 만든 것]UNPIVOT ( EMPNO FOR JOB IN (CLERK, MANAGER, PRESIDENT, ANALYST, SALESMAN));LAG() 함수SELECT에 사용LAG (컬럼명, N, M) : N개를 미루고 1-N까지는 M으로 출력LEAD() 함수LAG와 반대LEAD(컬럼명, N, M) : N개를RANK() 함수순위 출력함수SELECT RANK(‘SMITH’) WITHIN GROUP (ORDER BY ENAME) “RANK” FROM EMP;이때 RANK(값)은 ORDER BY(값의 컬럼명) 이어야 한다.SELECT RANK() OVER (ORDER BY ENAME) \"RANK\" FROM EMP;괄호 조심DESC_RANK 202pg4장 조인조인사용시에는 테이블이름.컬럼이름을 습관으로!EQUI JOIN(등가조인)동일한 조건을 가진 데이터를 후행 테이블에서 꺼내오는 방법. ORACLESELECT E.EMPNO, E.ENAME, D.DNAMEFROM EMP E, DEPT DWHERE E.DEPTNO = D.DEPTNO; ANSI JOIN SELECT E.EMPNO, E.ENAME, D.DNAMEFROM EMP E JOIN DEPT DON E.DEPTNO = D.DEPTNO;3개 묶기SELECT S.NAME, D.DNAME, P.NAMEFROM STUDENT S JOIN DEPARTMENT DON S.DEPTNO1 = D.DEPTNOJOIN PROFESSOR PON S.PROFNO = P.PROFNO;NON-EQUI JOIN 비등가조인같은 조건이 아닌 크거나 작거나하는 경우의 조건으로 조회할 때 씀. ORACLESELECT C.GNAME, TO_CHAR(C.POINT, ‘999.999’), G.GNAMEFROM CUSTOMER C, GIFT GWHERE C.POINT BETWEEN G.G_START AND G.G_END; ANSI JOIN 구문SELECT C.GNAME, TO_CHAR(C.POINT, ‘999,999’), G.GNAMEFROM CUSTOMER C JOIN GIFT GON C.POINT BETWEEN G.G_START AND G.G_END;OUTER JOIN 아우터조인한쪽 테이블에는 데이터가 있는데 한쪽 테이블에는 없는 경우 데이터가 있는 쪽 테이블 내용을 전부 출력하게 하는 방법 LEFT 조인 ORACLESELECT S.NAME, P.NAMEFROM STUDENT S, PROFESSOR PWHERE S.PROFNO = P.PROFNO(+);-&gt; 데이터가 없는 쪽에 (+) 데이터 표시 ANSI JOINSELECT S.NAME, P.NAMEFROM STUDENT S LEFT OUTER JOIN PROFESSOR PON S.PROFNO = P.PROFNO; RIGHT 조인SELECT S.NAME, P.NAMEFROM STUDENT S, PROFESSOR PWHERE S.PROFNO(+) = P.PROFNO;-&gt; 데이터가 없는 쪽에 (+) 데이터 표시 ANSI JOINSELECT S.NAME, P.NAMEFROM STUDENT S RIGHT OUTER JOIN PROFESSOR PON S.PROFNO = P.PROFNO; FULL OUTER 조인SELECT S.NAME, P.NAMEFROM STUDENT S, PROFESSOR PWHERE S.PROFNO = P.PROFNO(+)UNIONSELECT S.NAME, P.NAMEFROM STUDENT S, PROFESSOR PWHERE S.PROFNO(+) = P.PROFNO;-&gt; 데이터가 없는 쪽에 (+) 데이터 표시-&gt; FULL OUTER 조인이 없어서 UNION 사용 ANSI JOINSELECT S.NAME, P.NAMEFROM STUDENT S FULL OUTER JOIN PROFESSOR PON S.PROFNO = P.PROFNO; ORACLE 아우터 조인 주의사항부서에 대한 정보를 모두 보여주고 부서 번호가 20인 사원의 사원 번호 이름 급여를 보여주시오NO!!!!!!SELECT D.DEPTNO, D.DNAME, D.LOC, E.EMPNO, E.ENAME, E.SALFROM DEPT D, EMP EWHERE D.DEPTNO = E.DEPTNO(+) AND E.DEPTNO = 20ORDER BY 1;YES!!!!!!!!!!!!!!!!!!!!!!!!!!!!!SELECT D.DEPTNO, D.DNAME, D.LOC, E.EMPNO, E.ENAME, E.SALFROM DEPT D, EMP EWHERE D.DEPTNO = E.DEPTNO(+) AND E.DEPTNO(+) = 20ORDER BY 1; ANSI 아우터 조인 주의사항직업이 CLERK인 사우너 정보(번호, 이름, 직업)을 출력하고 그 중에 CHICAGO에 위치한 부서에 소속된 사원의 부서 정보(번호, 명, 위치)를 출력하세요.NO!!!!!!SELECT E.EMPNO, E.ENAME, E.JOB, D.DEPTNO, D.NAME, D.LOCFROM EMP E LEFT OUTER JOIN DEPT DON( E.DEPTNO = D.DEPTNO AND D.LOC = ‘CHICAGO’ )WHERE E.JOB = ‘CLERK’;YES!!!!!!!!!!!!!!!!!!!!!!!!!!!!!SELECT E.EMPNO, E.ENAME, E.JOB, D.DEPTNO, D.NAME, D.LOCFROM EMP E LEFT OUTER JOIN DEPT DON( E.DEPTNO = D.DEPTNO AND D.LOC = ‘CHICAGO’ AND E.JOB = ‘CLERK’); SELF 조인원하는 데이터가 하나의 테이블에 다 들어있을 경우사용 ORACLESELECT E1.ENAME, E2.ENAMEFROM EMP E1, EMP E2WHERE E1.MGR = E2.EMPNO; ANSISELECT E1.ENAME, E2.ENAMEFROM EMP E1 JOIN EMP E2ON E1.MGR = E2.EMPNO;SET PAGES 5050개까지 값 출력cascade &lt;-&gt; restrict연쇄 삭제drop table 테이블명 cascade constraint;Alter table emp drop foreign key dept_no references test(dept_no) on delete cascade on update cascade;Delete 조건 on delete cascade on delete set null : 부모 테이블의 데이터가 지워질 경우 자식 테이블의 값을 null"
    } ,
  
    {
      "title"       : "Database final",
      "category"    : "",
      "tags"        : "",
      "url"         : "./Database-final.html",
      "date"        : "2022-02-19 00:00:00 +0900",
      "description" : "",
      "content"     : "4장 조인조인사용시에는 테이블이름.컬럼이름을 습관으로!EQUI JOIN(등가조인)동일한 조건을 가진 데이터를 후행 테이블에서 꺼내오는 방법. ORACLESELECT E.EMPNO, E.ENAME, D.DNAMEFROM EMP E, DEPT DWHERE E.DEPTNO = D.DEPTNO; ANSI JOIN SELECT E.EMPNO, E.ENAME, D.DNAMEFROM EMP E JOIN DEPT DON E.DEPTNO = D.DEPTNO;3개 묶기SELECT S.NAME, D.DNAME, P.NAMEFROM STUDENT S JOIN DEPARTMENT DON S.DEPTNO1 = D.DEPTNOJOIN PROFESSOR PON S.PROFNO = P.PROFNO;NON-EQUI JOIN 비등가조인같은 조건이 아닌 크거나 작거나하는 경우의 조건으로 조회할 때 씀. ORACLESELECT C.GNAME, TO_CHAR(C.POINT, ‘999.999’), G.GNAMEFROM CUSTOMER C, GIFT GWHERE C.POINT BETWEEN G.G_START AND G.G_END; ANSI JOIN 구문SELECT C.GNAME, TO_CHAR(C.POINT, ‘999,999’), G.GNAMEFROM CUSTOMER C JOIN GIFT GON C.POINT BETWEEN G.G_START AND G.G_END;OUTER JOIN 아우터조인한쪽 테이블에는 데이터가 있는데 한쪽 테이블에는 없는 경우 데이터가 있는 쪽 테이블 내용을 전부 출력하게 하는 방법 LEFT 조인 ORACLESELECT S.NAME, P.NAMEFROM STUDENT S, PROFESSOR PWHERE S.PROFNO = P.PROFNO(+);-&gt; 데이터가 없는 쪽에 (+) 데이터 표시 ANSI JOINSELECT S.NAME, P.NAMEFROM STUDENT S LEFT OUTER JOIN PROFESSOR PON S.PROFNO = P.PROFNO; RIGHT 조인SELECT S.NAME, P.NAMEFROM STUDENT S, PROFESSOR PWHERE S.PROFNO(+) = P.PROFNO;-&gt; 데이터가 없는 쪽에 (+) 데이터 표시 ANSI JOINSELECT S.NAME, P.NAMEFROM STUDENT S RIGHT OUTER JOIN PROFESSOR PON S.PROFNO = P.PROFNO; FULL OUTER 조인SELECT S.NAME, P.NAMEFROM STUDENT S, PROFESSOR PWHERE S.PROFNO = P.PROFNO(+)UNIONSELECT S.NAME, P.NAMEFROM STUDENT S, PROFESSOR PWHERE S.PROFNO(+) = P.PROFNO;-&gt; 데이터가 없는 쪽에 (+) 데이터 표시-&gt; FULL OUTER 조인이 없어서 UNION 사용 ANSI JOINSELECT S.NAME, P.NAMEFROM STUDENT S FULL OUTER JOIN PROFESSOR PON S.PROFNO = P.PROFNO; ORACLE 아우터 조인 주의사항부서에 대한 정보를 모두 보여주고 부서 번호가 20인 사원의 사원 번호 이름 급여를 보여주시오NO!!!!!!SELECT D.DEPTNO, D.DNAME, D.LOC, E.EMPNO, E.ENAME, E.SALFROM DEPT D, EMP EWHERE D.DEPTNO = E.DEPTNO(+) AND E.DEPTNO = 20ORDER BY 1;YES!!!!!!!!!!!!!!!!!!!!!!!!!!!!!SELECT D.DEPTNO, D.DNAME, D.LOC, E.EMPNO, E.ENAME, E.SALFROM DEPT D, EMP EWHERE D.DEPTNO = E.DEPTNO(+) AND E.DEPTNO(+) = 20ORDER BY 1; ANSI 아우터 조인 주의사항직업이 CLERK인 사우너 정보(번호, 이름, 직업)을 출력하고 그 중에 CHICAGO에 위치한 부서에 소속된 사원의 부서 정보(번호, 명, 위치)를 출력하세요.NO!!!!!!SELECT E.EMPNO, E.ENAME, E.JOB, D.DEPTNO, D.NAME, D.LOCFROM EMP E LEFT OUTER JOIN DEPT DON( E.DEPTNO = D.DEPTNO AND D.LOC = ‘CHICAGO’ )WHERE E.JOB = ‘CLERK’;YES!!!!!!!!!!!!!!!!!!!!!!!!!!!!!SELECT E.EMPNO, E.ENAME, E.JOB, D.DEPTNO, D.NAME, D.LOCFROM EMP E LEFT OUTER JOIN DEPT DON( E.DEPTNO = D.DEPTNO AND D.LOC = ‘CHICAGO’ AND E.JOB = ‘CLERK’); SELF 조인원하는 데이터가 하나의 테이블에 다 들어있을 경우사용 ORACLESELECT E1.ENAME, E2.ENAMEFROM EMP E1, EMP E2WHERE E1.MGR = E2.EMPNO; ANSISELECT E1.ENAME, E2.ENAMEFROM EMP E1 JOIN EMP E2ON E1.MGR = E2.EMPNO;정의SQL 정의 데이터를 Access하기 위해 데이터 베이스와 통신하는 언어ProC 정의 SQL과 연동하여 DB의 데이터를 처리할 수 있게 해주는 C언어 기반의 프로그래밍 언어 ProC로 작성된 소스코드는 오라클 프로씨 프리컴파일러에 의해 보통 C/C++ 소스코드로 해석된다. C로 작성된 소스코드에 오라클 SQL의 DML명령문이나 DDL 명령문이 EXEC구문과 결합된 형태가 추가된 것임을 알 수 있다.PL/SQL 정의 DB내에서 집합적으로 처리할 수 밖에 없는 SQL의 한계를 넘어, 일반 프로그래밍 언어처럼 DB내에서 절차형으로 비교적 자유롭게 프로그래밍을 구현할 수 있도록 오라클 등의 DBMS에서 지원해 주는 절차형 프로그래밍 툴호스트 변수 데이터 베이스에서 처리한 결과를 가져와서 사용할 변수 선언절에서 명시적으로 선언한다. 선언한 대로 대문자 / 소문자의 포맷을 사용한다. SQL문에서는 앞에 :을 붙인다. C문에서는 :붙이지 않는다. SQL의 예약어를 사용하면 안된다.indicator 변수 오라클 문의 실행 결과를 측정하기 위한 용도로 사용한다. connect를 통해 디비 서버와 연결을 시도했을 때 성공/실패에 대한 리턴값이라고 보면 됨. 선언 : short 이름_ind; 사용 : :호스트변수:이름_ind, ...ProC SQL문 전에 Exec 붙이고 마지막에 꼭 ; 붙여야 함.ProC 연결부분 exec sql connect 'username' identified by 'passwd';' exec sql commit work release; 바로 전 작업뿐만 아니라 commit한 마지막구간 ~ 최근 작업 전부를 commit하는 것. (Insert/ delete/ alter할 때 꼭 고려!)exec sql rollback work release; ProC Commit commit 설명 프로그램에서 commit / rollback사용 X : 프로그램은 전체가 하나의 transcaction commit을 통해 디비 변경을 확정 commit전에 다른 유저는 변경된 데이터에 접근할 수 없고, 변경전 상태의 data만 열람할 수 있음. commit 되면 수행되는 작업들 현재의 트랜젝션에서 변경된사항을 디비에 반영 변경사항을 다른 유저가 열람할 수 있게 함 모든 savepoint를 삭제 모든 레코드 및 테이블의 락을 해제 모든 커서를 해제 트렌젝션 종료 DDL 문 실행 전후에 auto Commit ProC RollBack tosavepoint를 사용하면 현재의 트랜잭션의 처음이 아닌 중간까지 되돌리는 것이 가능함 Rollback되면 수행되는 작업들 현재 트랜잭션 내에서 디비에 대해 발생했던 변경이 전부 취소됨. 모든 savepoint가 삭제됨 현재 트랜잭션이 종료됨 모든 레코드 및 테이블의 락이 해제됨 모든 커서를 해제함 ProC 예외처리 exec sql whenever [condition] [action]; condition : sqlerror / sqlwarning / not found action : continue / do [error_logic] / do break / do continue / goto [label], stop continue : 가능하면 프로그램의 다음 문장을 실행 do continue : 루프에서 사용. 다음 루프 실행. ProC SQL문exec sql select 컬럼명, 컬럼명 into 호스트변수명, 호스트변수명from 테이블이름 where 조건;exec sql update 테이블 이름 set 컬럼명 = :호스트변수 where 조건exec sql commit work;exec sql insert into 테이블명(컬럼1, 컬럼2) values (:호스트변수1, :호스트변수2);exec sql commit work;exec sql delete from 테이블 이름 where 조건;exec sql commit work; ==우리조에서 쓴 커서==exec sql begin declare section;호스트변수1;호스트변수2;exec sql end declaere section;초기화영역. varchar은 memset해준다.exec sql declare 커서이름 cursor forselect 컬럼1, 컬럼2 from customer;exec sql open 커서이름;exec sql whenever not found do break;for(;;){ exec sql fetch 커서이름 into :호스트변수1, : 호스트변수2;호스트변수로 각각 할 것들}exec sql close 커서이름;ProC에서 가변문자길이 쓰기 declare 섹션에서 id varchar[100]선언 strcpy((char *) id.arr, “카피할 문자열”); id.len = (short) strlen((char *)id.arr);PL/SQL문의 실행 18pg DEVELOPER에서. exec sql 호출될 PL/SQL블록 name; exec sql begin sql문 end; exec sql declare cursor_name cursor forselect 문 exec sql open cursor_name; exec sql fetch cursor_name into variable; exec sql close cursor_name;PL/SQL 선언부(declare) 실행부(begin) 예외처리부(exception)으로 구성됨. PL/SQL 블록 작성시 기본 규칙과 권장사항 문장은 여러줄에 걸쳐질 수 있으나 키워드는 분리X 예약어는 식별자명으로 사용불가하나 alias로는 사용가능 리터럴(날짜, 문자)는 ` 사용 주석처리 – select select 컬럼 리스트 into 변수 리스트 from 테이블 where 조건;set serverouput on; -- 결과값을 화면에 출력하기 위해서 사용declare v_empid employee.employ_id%type -- employ_id와 동일한 데이터 형으로 선언함 v_salary employee.salary&amp;typeBegin select employee_id, salary into v_empid, v_salary from employees where employee_id = 197; -- 혹은 employee_id = &amp;id; DBMS_OUTPUT.PUT_LINE(v_empid || ' == '|| v_salary);END;/insertBegin insert into pl_test values (pl_seq.Nextval, 'aaa');End;/5장 DDL명령create 테이블 복사하기 create table dept4 AS select dcode, dname from dept2; Alter 새로운 컬럼 추가하기 alter table dept6 add (Loc Varchar(20); 컬럼 이름 변경하기 alter table dept6 rename column location2 to loc; location2 -&gt; loc 이름변경 데이터 크기 변경하기 alter table dept7 modify (loc varchar(20); 컬럼 삭제하기 alter table dept7 drop column loc (cascade constraints ); ()부분은 외래키일시에 추가해야함. Truncate 명령 사용중인 공간 전부 내놓기 truncate table dept7;drop drop table dept7;data dictionary 딕셔너리에 저장되어 있는 정보 : 제약조건 정보 사용자에 대한 정보 권한이나 프로파일 롤에대한 정보 감사(audit)에 대한 정보 오브젝트들이 사용하고 있는 공간의 정보 오라클 데이터베이스의 메모리 구조와 파일에 대한 구조 정보 Dictionary static dictionary USER_XXX : 해당 사용자가 생성한 오브젝트들만 조회가능 ALL_XXX : 해당 사용자가 접근할 수 있는 모든 오브젝트 DBA_XXX : DBA권한을 가진 사람만 접근할 수 있는 오브젝트 dynamic dictionary V$_XXX ANALYZE TABLE ST_TABLE COMPUTE STATICS; 위를 실행시켜야 아래 코드가 정상작동한다. SELET NUM_ROWS(레코드수), BLOCKS(블록주소) FROM USER_TABLES WHERE TABLE_NAME = 'ST_TABLE';6장 DMLINSERT INSERT INTO 테이블명(속성명1, 2, ..) VALUES (속성명1, 2, ..); insert와 서브쿼리 이용, 여러 행 넣기 insert into professor3 select * from professor; insert와 all을 이용하여 여러 테이블에 여러 행 생성하기 insert all when profno between 1999 then into prof3 values (profno, name) when profno between 2000 and 2999 then into prof 4 values (profno, name) select profno, name from professor; update update 테이블명 set column = 값 where 조건;delete delete from 테이블명 where 조건;merge 테이블1 + 테이블2 = 테이블1 merge into 테이블1 (total : 전체를 부른다는 것.) using 테이블2 on (병합 조건절) when matched then : 서로 같다면 조건이 만족한다면 update / delete 실행될 것. update set 업데이트내용 delete where 조건 when not matched then : 서로 다르다면 조건이 만족하지 않는다면 insert문 실행 insert values (컬럼이름);transcaction 관리하기7장 Constraint 제약 조건 주키 : 개체 무결성 , Not Null , unique 외래키 : 참조 무결성, 생략 가능, Null가능 상대 테이블에 존재하는 data를 가져온다. (보통 주키를 가져옴.) 주키가 아닌 키를 가져온다면 키 속성이 unique여야 한다. 제약조건 정의 테이블에 올바른 데이터만 입력받고 잘못된 데이터는 들어오지 못하도록 컬럼마다 정하는 규칙을 의미 default는 제약조건 아님 제약조건 종류 NOT NULL UNIQUE PRIMARY KEY : NOT NULL + UNIQUE FOREIGN KEY : 다른 테이블의 컬럼을 참조해서 검사 CHECK : 이 조건에서 설정된 값만 입력을 허용하고 나머지는 거부 테이블에서 컬럼별로 설정 PRIMARY KEY제외하고는 중복 설정 가능 제약조건 조회하기 테이블 명은 꼭 대문자로 입력해야한다. SELECT * FROM ALL_CONSTRAINTS WHERE table_name = 'EMP';제약조건 삭제하기 ALTER TABLE 테이블명 DROP CONSTRAINT 제약조건명;제약조건 생성하기 테이블 생성 할 때 조건 주기 제약조건 이름 주기 create table NewEmp1 (no Number(4) Constraint emp1_no_pk primary key, name Varchar(12) Constraint emp_name_nn Not Null ); 제약조건 이름 안주기 create table NewEmp1( no Num(4) primary key, name varchar(12) not null ); 제약조건의 이름을 안주면 제약조건을 (비)활성화, 삭제 등의 관리 작업을 못함. 그래서 주는 것을 권장 테이블 생성 후 조건 주기 기본 : alter 테이블 명 add Constraint 제약조건별칭 제약조건(컬럼명) alter table NewEmp1 Add Constraint emp1_nk UNIQUE(name); Not Null 은 특별하게도 Modify 써야함 alter table NewEmp Modify ( p_no constraint pNo_nn Not NUll); foreign key 제약조건 생성시, 부모 reference key에 primary key / unique 설정 해줘야함. 제약조건 관리하기 DISABLE UNIQUE INDEX가 자동으로 삭제됨. 기본 옵션은 NOVALIDATE NOVALIDATE : 해당 제약조건이 없는 듯이 행동 ALTER TABLE 테이블명 DISABLE NOVALIDATE CONSTRAINT 제약조건명; VALIDATE : 해당 제약조건이 없지만 컬럼의 데이터를 변경할 순 없게 한다. 하지만 INSERT, DELETE, ALTER 다 안됨..?????? 왜필요한건가. ALTER TABLE 테이블명 DISABLE VALIDATE CONSTRAINT 제약조건명; 테이블의 내용을 변경할 수 없도록 ENABLE NOVALIDATE ENABLE한 시점부터 새롭게 테이블로 입력되는 데이터만 제약조건을 적용하여 검사 VALIDATE 테이블 이미 존재하는 애들 + 새롭게 입력되는 데이터 제약조건 검사 8장 INDEX인덱스 종류 UNIQUE INDEX 속도가 아주 빠름 중복되는 데이터 들어올 수 없다. create unique index 인덱스명 on 테이블이름(컬럼명1 asc/desc, ... ); NON-UNIQUE INDEX 중복되는 데이터가 들어가야할 때 create index 인덱스명 on 테이블명(컬럼명1 asc/desc, ..); Function Based INDEX Index Suppressing Error : 인덱스를 where절에 오는 조건 컬럼이나 조인 컬럼에 만들어야 하는데 이렇게 만들어놓고 sql에서 다른 조건을 사용하거나 하면 인덱스를 사용할 수 없게되는 에러 쿼리 조건이 변경되면 기존의 인덱스를 활용할 수없다. create index idx_prof_pay_fbi on professor(pay+1000); DESCENDING INDEX desc : 큰값 -&gt; 작은값 날짜 큰값 = 최근 create index idx_prof_pay on professor pay(desc); Composition INDEX 두개 이상의 컬럼을 합쳐서 인덱스를 만드는 것 where절의 조인 컬럼이 2개 이상 and로 연결되어 함께 사용되는 경우에 많이 사용 인덱스 순서가 중요함. create index idx_emp_com on emp (sex, ename); Bitmap INDEX create bitmap index idx_emp_sex_bit on emp(sex); 인덱스 주의사항 DML에 취약하다 insert index split 현상 : 데이터가 테이블 도중에 들어가야한다면 2개의 블록으로 나뉘어서 잠깐 저장되는 현상. 느려짐. delete 보통 : 그 자리가 비워지고 다른 데이터가 사용가능하다고 말해줌. 인덱스는 사용안된다는 표시만 함. 즉 데이터가 1만건 있어도 인덱스는 10만건 있을수있음. update 인덱스에는 update 없음. 타 SQL 실행에 악영향인덱스 조회하기 사용자가 생성한 인덱스 조회 : user_indexes , user_ind_columns select table_name, column_name, index_name from user_ind_colmuns where table_name = 'EMP1'; select table_name, index_name from user_indexs where table_name = 'EMP1'; 컬럼명과 from 주의! 결과 값은 동일함. 데이터베이스 전체에 생성된 내역을 조회하려면 DBA_INDEXES / DBA_IND_COLUMNS인덱스 모니터링 시작하기 alter index idx_emp_ename monitoring usage; 중단하기 alter index idx_emp_ename nomonitoring usage; 사용여부 확인 select index_name, used from v$object_usage where index_name = 'idx_emp_ename';INDEX REBUILD하기INVISIBLE INDEXIndex활용 399pg 정렬한 효과 최대 최소 찾기 ROWID10장 서브쿼리 438 서브쿼리가 먼저 수행되어 결과를 만들고 그 결과값을 메인쿼리에 전달 그 반대의 경우도 있음 구별 select (sub query) : 스칼라 서브 쿼리 from (sub query) : 인라인 뷰 where (sub query) : 서브쿼리Sub Query where절 , 괄호로 묶기 order by 올수없음단일행 수행값이 1개의 행 where절에서 사용되는 연산자 &lt;&gt; 같지않다. = = &lt; &lt;= 다중행 수행값에서 행 수가 2개이상 where절에서 사용되는 연산자 : IN : 서브 쿼리와 같은 값을 찾는다. EXISTS : 서브 쿼리의 값이 있을 때만 메인 쿼리 실행 ANY : 서브쿼리 결과중에서 최솟값 반환 &lt;ANY : 서브쿼리 결과중에서 최댓값을 반환 &lt;ALL : 서브쿼리 결과중에서 최솟값을 반환 ALL : 서브쿼리 결과중에서 최댓값을 반환 주의 ! SAL &gt; ANY (100, 200, 300) : SAL &gt; 100 SAL &lt; ALL(100, 200, 300) : SAL &lt; 100 ==ALL은 방향의 반대값을 출력해준다== 다중컬럼 서브쿼리의 결과가 여러 컬럼인 경우 실습해야함???????!???!?!?!???상호연관 메인 쿼리 값을 서브쿼리에 주고 서브쿼리를 수행한 후 그 결과를 다시 메인 쿼리로 반환해서 수행하는 서브쿼리스칼라 서브 쿼리 2건 이상의 데이터 반환을 요청하는 경우 – 에러 2개 이상의 컬럼을 조회할 경우 – 에러 cascade &lt;-&gt; restrict연쇄 삭제drop table 테이블명 cascade constraint;Alter table emp drop foreign key dept_no references test(dept_no) on delete cascade on update cascade;Delete 조건 on delete cascade on delete set null : 부모 테이블의 데이터가 지워질 경우 자식 테이블의 값을 null외래키 주기 create 시 foreign key (생성할테이블의 컬럼명) references 참조할테이블(참조할컬럼명) alter 시 alter table 테이블명 add constraint 제약조건명 foreign key(속성명) references 참조할테이블(참조할컬럼명) on delete cascade; 연봉표시 to_char(pay, ‘$999,999,999’)1장테이블 조회하기select * from 테이블명 : 정보까지 다 나옴desc 테이블명 (describe) : 속성 정보만 나옴예쁘게 조회하기는 생략 (31pg)컬럼명 별칭으로 출력하기 select Deptno, '111' \"DNAME\" FROM dept; “속성명”이 모두 동일하게 111로출력됨.‘’ 과 달라!select Deptno (as) \"1\", '111' \"DNAME\" FROM dept;Deptno를 1로 출력하는 방법Distinct 명령어로 중복된 값을 제거하고 출력하기SELECT DISTINCT deptno FROM emp; 정렬은 아니고 그냥 중복값만 없애줌. SELECT 바로 뒤에 와야함.SELECT DISTINCT job, ename FROM emp; job에만 DISTINCT적용되지 않음. job, ename전부에 적용됨.연결 연산자로 컬럼 붙여서 출력하기SELECT ename || job FROM emp;연결연산자 || 사용SELECT ename ||'''s job is '|| job \"Name and Job\" FROM emp;컬럼명 || 내용 -&gt; 컬럼값내용 출력. 이때 ` 쓰려면 `` 두 개 써야함.Where 절SELECT * FROM emp WHERE ename = ‘smith’; 문자와 날짜를 조회하고 싶을 때에는 꼭 작은 따옴표를 붙여야한다. 문자는 대소문자 구별이 필요하고 날짜는 대소문자 구별이 필요없다.sql에서 기본 연산자 사용하기 +-*/는 컬럼명에 바로 해주어도 된다. 추가적인 연산자 : IN(a,b,c) : a나 b나 c인 조건 검색`LIKE’ : 특별한 패턴이 있는 조건 검색 % : 글자 수에 제한이 없고 어떤 글자가 와도 됨. SELECT empno, sal FROM emp WHERE sal LIKE 1% 1300 1600 등 출력 _ : 글자 수는 한 글자가만 올 수 있고 어떤 글자가 와도 됨. SELECT ename, sal FROM emp WHERE ename LIKE ‘A_____’ ALLEN 출력BETWEEN a AND b : a와 b사이 조건 검색IS (NOT) NULL : WHERE ename IS NOT NULLAND 와 OR중 AND 먼저 수행 후 OR 수행 됨. 정렬하여 출력하기 – ORDER BY 오름차순 : ASC 내림차순 : DESC ORDER BY 컬럼명 DESC/ASC집합연산자 UNION (ALL) : 두 집합의 결과를 합쳐서 출력. 중복 값 제거하고 정렬(중복 값 제거 안하고 정렬 안함) INTERSECT : 교집합 결과를 출력 및 정렬 MINUS : 차집합 결과를 출력 및 정렬(순서 중요) SELECT empno, salFROM empMINUSSELECT empno, salFROM empWHERE sal &gt; 2500; 2장 단일행 함수 : 한번에 하나씩 처리하는 함수 복수행 함수 : 여러 건의 데이터를 동시에 입력받아 출력 1건을 만들어줌.문자열처리함수INITCAPSELECT ename, INITCAP(ename) “INITCAP” FROM emp;영어에서 첫 글자만 대문자로 출력하고 나머지는 전부 소문자로 출력하는 함수LOWERUPPERLENGTH , LENGTHBLENGTH : 입력된 문자열의 길이 계산.LENGTHB : 입력된 문자열의 바이트 계산.CONCAT|| 연산자와 동일한 기능SELECT CONCAT(ename, job) FROM emp WHERE deptno = 10;SUBSTR특정 길이의 문자만 골라낼 때SELECT SUBSTR(‘abcde’, 3, 2) from dual; cdSELECT SUBSTR(‘abcde’, -3, 2) from dual; cdSELECT SUBSTR(‘abcde’, -3, 4) from dual; cde 마이너스를 붙이면 파이썬처럼 검색 글자수 세는 것은 왼쪽 -&gt; 오른쪽 SUBSTRB : 세는 단위가 바이트컬럼에 ‘아아’ 넣으면?SELECT ename, ‘A-B’ FROM emp;ename개수 만큼 A-B 출력 컬럼명은 ‘A-B’INSTR주어진 문자열이나 컬럼에서 특정 글자의 위치를 찾아줌.SELECT INSTR(‘A-B-C-D-E’, ‘-’, 1, 3) FROM dual; 6SELECT INSTR('A-B-C-D-E', '-', -1, 3) FROM dual; 4SELECT INSTR('A-B-C-D-E', '-', -4, 3) FROM dual; 2 n번째부터 포함해서 검색. 위치는 맨처음부터 인덱스로 계산 -하면 검색도 마이너스 방향으로 검색LPAD (컬럼명, 맞출 글자수, 채울 문자리스트)select ename, LPAD(ename, 10, '123456789') from emp;사원이름을 총 10바이트로 출력하되 해당 빈자리에는 해당자리 숫자로 채우기.‘123456789’가 아닌 1-9 또는 ‘1-9’는 안됨. RPAD도 있음응용문제사원의 이름을 9자리로 출력하되 오른쪽 빈자리는 해당 자리수에 해당하는 숫자가 출력되도록 하시오. select ename, RPAD(ename, 9, substr(‘123456789’, length(ename)+1, 9))from empwhere deptno = 10;LTRIMselect ename, LTRIM(ename, 'J') from emp; 제거. RTRIM도 있음REPLACEREPLACE (컬럼명, 문자1, 문자2) 문자1을 문자2로 바꿔줌. student 테이블에서 학생들의 이름과 전화번호, 전화번호에서 국번부분만 처리해서 출력하시오. 국번은 3자리.select name, tel, replace(tel, substr(tel, Instr(tel, ‘)’, 1)+1,4), ‘**’) from student;숫자 관련 함수ROUND/TRUNC(숫자, 출력되길 원하는 숫자자리수) 0: 1의자리 숫자까지 나옴 -1: 10의자리 숫자까지 나옴 1: 소수점 첫째자리까지 나옴 ROUND : 반올림임 TRUNC : 버림MOD/CEIL/FLOOR(숫자) MOD : 나머지 값구하는 함수 CEIL : 주어진 숫자에서 가장 큰 정수를 구하는 함수, 올림 FLOOR : 주어진 숫자에서 가장 작은 정수를 구하는 함수, 내림POWER(숫자, 숫자2)숫자^숫자2날짜 관련 함수 91pg형변환 함수일반함수NVLNULL값을 만나면 다른 값으로 치환해서 출력하는 함수NVL(sal, 0) sal값이 NULL이면 0 출력NVL2NULL값이 아니라면 다른 값으로 치환해서 출력DECODEif문과 동일, SELECT에 사용 DECODE (컬럼1, 컬럼2, 맞으면출력, 아니면출력) DECODE (컬럼1, 컬럼2, 1=2이면 출력, 컬럼3, 1=3이면 출력, 둘다 아니면 출력) DECODE 안의 DECODE도 가능case문CASE 조건 WHEN 결과1 THEN 출력1 [WHEN 결과2 THEN 출력2] ELSE 출력3END “컬럼명”내부에 콤마가 사용되지 않는다.정규식 함수로 다양한 조건 조회하기 123pg3장GROUP 함수 종류COUNTSUMAVGMAX, MINSTDDEV , VARIANCE STDDEV : 표준편차 구하는 함수 VARIANCE : 분산 구하는 함수GROUP BY 절을 이용하여 특정 조건으로 세부적인 그룹화하기 주의점 SELECT절에 사용된 그룹 함수 이외의 컬럼이나 표현식은 반드시 GROUP BY절에 사용되어야 한다. 그렇지 않을 경우 에러 발생 =&gt; 컬럼이나 표현식 전부 사용되어야 한다.``` SQLSELECT deptno, job, AVG(NVL(sal, 0)) “AVG_SAL”FROM empGROUP BY deptno;/* 에러 */ SELECT deptno, jobFROM empGROUP BY deptno;/* 에러 */SELECT deptno, job, AVG(NVL(sal, 0)) “AVG_SAL”FROM empGROUP BY deptno, job;/* ㅇㅋ */- GROUP BY절에는 반드시 컬럼명이 사용되어야 하며 컬럼 Alias는 사용하면 안된다.# HAVING절을 사용해 그룹핑한 조건으로 검색하기그룹 함수를 조건으로 사용하고 싶을 경우에는 WHERE 대신에 HAVING 절을 사용하면 된다. WHERE절에는 사용 안됨!GROUP BY조건절 같이 써야한다.**GROUP함수를 조건으로 할 경우에는 WHERE을 사용하면 안된다.** ``` SQLSELECT deptno, AVG(NVL(sal,0))FROM empWHERE deptno &gt; 10GROUP BY deptnoHAVING AVG(NVL(sal, 0)) &gt; 2000;반드시 알아야 하는 다양한 분석 함수들 167 pgROLLUP()각 기준별 소계를 요약해서 보여줌SELECT EMPNO, JOB, AVG(SAL)FROM EMPWHERE EMPNO &gt; 7500GROUP BY ROLLUP(EMPNO, JOB);직원번호별(직원번호, NULL), 직원과 직업별(직원번호, 직업별) 전체그룹집계 (NULL, NULL)SELECT DEPTNO, JOB, AVG(SAL)FROM EMPWHERE EMPNO &gt; 7700GROUP BY ROLLUP(DEPTNO, JOB);직원번호별(EMPNO, 직원번호, NULL), 직원과 직업별(EMPNO, 직원번호, 직업별) 전체그룹집계 (EMPNO, NULL, NULL)CUBE()소계와 전체 합계까지 출력하는 함수rollup과 비슷하지만 cube(deptno, job)하면, deptno별 job별 deptno,job별, 전체그룹별GROUPING SETS()SELECT NAME, GRADE, DEPTNO1, COUNT(*), SUM(WEIGHT)FROM STUDENTGROUP BY GROUPING SETS(GRADE, DEPTNO1), NAME; NAME, GRADE, NULL NAME, NULL, GRADESELECT NAME, GRADE, DEPTNO1, COUNT(*), SUM(WEIGHT)FROM STUDENTGROUP BY GROUPING SETS(GRADE), NAME, DEPTNO1; 그냥 테이블 조회와 동일SELECT NAME, GRADE, DEPTNO1, COUNT(*), SUM(WEIGHT)FROM STUDENTGROUP BY GROUPING SETS(GRADE, DEPTNO1, NAME); GRADE, NULL, NULL NULL, DEPTNO1, NULL NULL, NULL, NAMELISTAGG()SELECT에 사용LISTAGG(컬럼명, ‘구분자’) WITHIN GROUP (ORDER BY 컬럼명 혹은 조건)이때 오더바이 조건에 HIREDATE 들어가면 고참-&gt;신참 순PIVOT / UNPIVOT 185pg(중간확인용)SELECT MAX (DECODE(DAY, ‘SUN’, DAYNO)) SUN, … MAX (DECODE(DAY, ‘SAY’, DAYNO)) SATFROM CALGROUP BY WEEKNOORDER BY WEEKNO;SELECT * FROM (SELECT WEEKNO “WEEK”, DAY, DAYNO FROM CAL)PIVOT( MAX (DAYNO) FOR DAY IN (‘SUN’ AS “SUN”, ‘MON’ AS “MON”, ‘TUE’ AS “TUE”, ‘WED’ AS “WED”, ‘THUR’ AS “THUR”, ‘FRI’ AS “FRI”, ‘SAT’ AS “SAT” )달력만듬.DAYNO가 가로로 나옴./* 학년 별로 사람수세기/SELECT * FROM (SELECT STUDNO, GRADE FROM STUDENT)PIVOT (COUNT(STUDNO) FOR GRADE IN (4, 3, 2, 1));COUNT () 하면 안됨.SELECT * FROM UPIVOT [PIVOT 결과를 테이블로 만든 것]UNPIVOT ( EMPNO FOR JOB IN (CLERK, MANAGER, PRESIDENT, ANALYST, SALESMAN));LAG() 함수SELECT에 사용LAG (컬럼명, N, M) : N개를 미루고 1-N까지는 M으로 출력LEAD() 함수LAG와 반대LEAD(컬럼명, N, M) : N개를RANK() 함수순위 출력함수SELECT RANK(‘SMITH’) WITHIN GROUP (ORDER BY ENAME) “RANK” FROM EMP;이때 RANK(값)은 ORDER BY(값의 컬럼명) 이어야 한다.SELECT RANK() OVER (ORDER BY ENAME) \"RANK\" FROM EMP;괄호 조심DESC_RANK 202pSET PAGES 5050개까지 값 출력cascade &lt;-&gt; restrict연쇄 삭제drop table 테이블명 cascade constraint;Alter table emp drop foreign key dept_no references test(dept_no) on delete cascade on update cascade;Delete 조건 on delete cascade on delete set null : 부모 테이블의 데이터가 지워질 경우 자식 테이블의 값을 null"
    } ,
  
    {
      "title"       : "20220212 추천 시스템 세미나",
      "category"    : "",
      "tags"        : "",
      "url"         : "./20220212-%EC%B6%94%EC%B2%9C-%EC%8B%9C%EC%8A%A4%ED%85%9C-%EC%84%B8%EB%AF%B8%EB%82%98.html",
      "date"        : "2022-02-12 00:00:00 +0900",
      "description" : "",
      "content"     : "Explore좋은 논문 찾는 방법 키워드로 찾기 google scholar 키워드로 검색 학회에서 찾기 Top-tier Conference 세계 최고 권위 학회에서 찾기 논문의 저자로 찾기 유명 논문의 저자 혹은 연구 그룹 키워드란? 기술 키워드 추천 시스템의 경우 deep learning + recommender systems 도메인 키워드 원하는 분야가 있는 경우 music + recommender system 처음 입문 하는경우 survey 혹은 review로 검색! 연구 주제를 보는 방법 연구단체 홈페이지를 가면 연구 주제 목록이있습니다. 요즘 학한 키워드를 찾는 법 학회 + 연도 + accepted paers로 구글에 검색 세계 최고 권위 학회 저널 밑에는 주제별 유명 학회입니다. h5 인덱스 값이란? 최근 5년간 n번 이상 인용된 논문이 n개 Impact Factor IF 최근 2년간 평균 인용 수 Acceptance rate 저기 링크는 학회별 논문에 대해 꾸준히 업뎃중임저자 또는 연구 그룹저자의 관심 키워드 Research Interest를 찾기!구글 스칼라에 label해서 검색하면 프로필이 뜹니다. 유명한 저자가 나온다고합니다.논문은 어떻게 읽어야 할까요?꼭 순서대로 읽어야 할필요도, 사람마다 다 다르니, 논문의 구조를 먼저 알려줄예정입니다. 제목 저자 및 소속 논문에 대해 질문이나 잘못된거나 코드 요청등을 하는 곳은 corresponding Author abstract introduction 서론 Introduction figure (그림) 그림 바로 밑에 혹은 본문 내용에 적혀져있음. contributions (기여) Methodology (방법론) 수식을 많으면 많을 수록 좋다,, related work( 관련 연구 ) 과거의 연구를 어디까지 봐야하는지를 알 수 있는 부분. RQ와 실험결과는 이어집니다. 타고 타고 올라가 볼 논문들이 나와있는 경우가 많습니다.논문 어떻게 정리해야 할까요?논문 관리 툴 End note www.endnote.com Mendeley www.mendeley.com논문을 어떻게 구현을 해야할까요 논문 구현체 찾기 크롬 extension중에 뭐를 설치하면 구글 스칼라에서 아래 사진 왼쪽 그림처럼 코드가 있으면 있다고 뜨는게 있습니다. https://chrome.google.com/webstore/detail/aiml-papers-with-code-eve/aikkeehnlfpamidigaffhfmgbkdeheil AI/ML Papers with Code Everywhere - CatalyzeX 저자에게 요청 혹은 직접 구현 (정중하고 연구목적 , 관심있다는 얘기로. ㅇㅇ) 논문 작성 순서Exploit"
    } ,
  
    {
      "title"       : "[fd_18]  딥러닝 들여다보기",
      "category"    : "",
      "tags"        : "",
      "url"         : "./fd_18-%EB%94%A5%EB%9F%AC%EB%8B%9D-%EB%93%A4%EC%97%AC%EB%8B%A4%EB%B3%B4%EA%B8%B0.html",
      "date"        : "2022-02-08 00:00:00 +0900",
      "description" : "",
      "content"     : "이 포스팅은 아이펠 fundamentals 18번 노드를 학습하고 적은 기록물 입니다.신경망이란?과학자들은 문제에 대한 해답을 종종 자연에서 찾아냅니다. 머신러닝 / 딥러닝 과학자들은 해당 문제를 해결하기 위하여 신경망의 뉴런 하나를 본 딴, 퍼셉트론의 개념을 도입하였고, 후에 이를 연결하여 다층 신경망 구조, 인공신경망을 구축해내게 됩니다.신경망은 모두가 가지고 있는 뇌의 뉴런들을 본딴 것입니다. 뉴런들이 모여 거대한 그물망과 같은 형태를 띄고 있는데, 이를 컴퓨터 프로그램으로 풀어낸 것입니다.다음은 다층 퍼셉트론 구조로 학습한 mnist 코드의 부분입니다.# 모델에 맞게 데이터 가공x_train_norm, x_test_norm = x_train / 255.0, x_test / 255.0x_train_reshaped = x_train_norm.reshape(-1, x_train_norm.shape[1]*x_train_norm.shape[2])x_test_reshaped = x_test_norm.reshape(-1, x_test_norm.shape[1]*x_test_norm.shape[2])# 딥러닝 모델 구성 - 2 Layer Perceptronmodel=keras.models.Sequential()model.add(keras.layers.Dense(50, activation='sigmoid', input_shape=(784,))) # 입력층 d=784, 은닉층 레이어 H=50model.add(keras.layers.Dense(10, activation='softmax')) # 출력층 레이어 K=10model.summary()궁금증 1. x_train_reshaped에서 왜 -1 값을 줄까?​ 찾아보니 reshape에서 (-1) 옵션을 주면, 주어진 옵션값에 맞는 값을 찾아준다고 한다.궁금증 2. 전에 Dense()는 뭐였을까 궁금했었다.​ Dense : 다층 퍼셉트론을 keras 코드로 사용하는 거였습니다.Bias란?bias에 관한 링크를 보다 궁금증이 생겼었습니다.참고 링크 : https://stackoverflow.com/questions/2480650/what-is-the-role-of-the-bias-in-neural-networks질문 1. 제가 알고 있는 출력값 y = w(가중치)*x(입력데이터) + b(bias)인데, 링크의 첫번째 답변에서 bias를 이용한 시그모이드를 sigmoid(w0*x + w1*1.0)이라 표현했어요. 제가 알기론 bias에 관해서는 가중치를 안쓴다고 알고있는데 쓰는경우도 있는건지, 원래 쓰는데 안쓰는걸로 제가 잘못알고있는건지 궁금해요. 이 질문에 가장 도움이 되었던 링크 두개를 먼저 첨부합니다. https://devlog.jwgo.kr/2018/04/16/sigmoid-graph-according-to-slope-change/ https://blog.naver.com/PostView.nhn?blogId=winddori2002&amp;logNo=221937861519 가장 먼저 이해가 안됐던 부분은 bias가 성공적인 학습에 매우 중요한 요소이며, bias가 바뀜에 따라 함수가 왼쪽 혹은 오른쪽으로 이동이 된다는 것이었습니다. 제가 알고있는 출력 함수의 식은 w*x + b인데, 결국 w가 0일때 활성화함수가 b인곳을 꼭 지나게 됩니다. w=0일때, bias값의 절편에 따라 출력함수값이 거기에 묶이게 되는데 어떻게 성공적인 학습에 어떻게 기여를 하는지가 이해가 안됐고 특히 bias값에 따라 그래프가 왼쪽 혹은 오른쪽으로 움직이는 것이 아니라 위 아래로 움직여야 한다고생각했습니다. 이 질문이 들었던 가장 큰 오해는 출력함수는 wx+b라는 가장 간단한 식으로 나타내는것이 아니라는 것입니다. 예를 들어, sigmoid함수에서 wx+b함수를 적용해서 그리면 아래 링크와 같습니다. https://devlog.jwgo.kr/2018/04/16/sigmoid-graph-according-to-slope-change/ bias를 -10에서 10으로 변화시킬때 시그모이드 함수의 형태를 보여주고 있습니다. 세로가 아니라 가로로 움직이게 된다는 것을 시각적으로 확인하였고, wx+b에서 학습결과가 꼭 0, b를 지나게되지 않는다는 것도 이해를 하게되었습니다. 두번째로 이해가 안되었던, 시그모이드에서 bias 1.0을 sigmoid(w0*x + w1*1.0)과 같이 가중치가 있는것 처럼 표현하였습니다. 이부분은 아래 질문에서 답하겠습니다. 질문2. bias에 관해서 그렇게 깊게 생각해보지 않았었는데, (그냥 주는구나 싶은정도?) bias 값은 어떻게 결정해서 주게되나용? 이거도 그냥 많이 쓰는숫자에서 시행착오로 돌려보고 결정하나요? https://brunch.co.kr/@coolmindory/32 여기 링크를 보고 이해가 되었습니다. 여기서 보면 backpropagation에서 업데이트 대상은 가중치와 bias 둘 다입니다. 그랬기 때문에, 바이어스에도 가중치가 있다는 듯이 표현을 한것이라 이해했습니다. 그리고 최근에 backpropagation에 대해서 발표를 한 적이 있는데, 그때 이런 질문을 받은 적이 있습니다. 그렇다면 Backpropagation에서 bias는 어떻게 업데이트되냐 라는 질문이었습니다. 저는 그때 출력함수 wx+b라는 매우 간단한 식만을 이해하고 있었기 때문에, backpropagation에서 b는 상수항이다. 그래서, 미분하면 0이 된다. 라고 답변을 드렸었는데, 제가 잘못된 답변을 드린것을 알게되었습니다. 가중치를 갱신할 때 w=w-α*dL/dw 인것처럼 bias도 b= b-α*dL/db로 갱신된다는 것을 알게 되었습니다. 활성화 함수신경망은 수 많은 뉴런으로 이루어져 있고,손실함수경사하강법오늘 배운 메소드 np.dot(X,Y) : np.array 객체를 곱할때 사용합니다. 1차원 행렬(Vector) 이라면 : 각 자리 숫자끼리 곱하여 더합니다. 2차원 행렬(matrix) 라면 : 일반적인 행렬 곱을 수행합니다. 출처 : https://ko.wikipedia.org/wiki/%ED%96%89%EB%A0%AC_%EA%B3%B1%EC%85%88 X나 Y 중 하나가 상수라면 : 상수곱 연산을 합니다. np.exp(x) : numpy.exp() 함수는 밑이 자연상수 e인 지수함수(e^x)로 변환해줍니다. 출처 AIFFEL LMS 문제시 연락 부탁드립니다. :) https://m.blog.naver.com/shwotjd14/221435180635 https://reniew.github.io/12/"
    } ,
  
    {
      "title"       : "머신러닝 ) 앙상블 ensemble",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-)-%EC%95%99%EC%83%81%EB%B8%94-Ensemble.html",
      "date"        : "2022-02-08 00:00:00 +0900",
      "description" : "",
      "content"     : "Kaggle 사이트에서출처 Introduction to Ensemble Learning https://subinium.github.io/introduction-to-ensemble-1/#:~:text=%EC%95%99%EC%83%81%EB%B8%94(Ensemble)%20%ED%95%99%EC%8A%B5%EC%9D%80%20%EC%97%AC%EB%9F%AC,%EB%A5%BC%20%EA%B0%80%EC%A7%80%EA%B3%A0%20%EC%9D%B4%ED%95%B4%ED%95%98%EB%A9%B4%20%EC%A2%8B%EC%8A%B5%EB%8B%88%EB%8B%A4. Kaggle Ensembling Guide 한글 번역 https://jamm-notnull.tistory.com/16 ㅎ"
    } ,
  
    {
      "title"       : "머신러닝 ) 다변량회귀분석 multivariate regression",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-)-%EB%8B%A4%EB%B3%80%EB%9F%89%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D-Multivariate-regression.html",
      "date"        : "2022-02-08 00:00:00 +0900",
      "description" : "",
      "content"     : "fundamental 노드 19번을 공부할 때, 아이펠에서 나왔던 질문을 바탕으로 공부했던 것을 작성해보고자 합니다.출처 https://techblog-history-younghunjo1.tistory.com/118"
    } ,
  
    {
      "title"       : "동적 웹 페이지와 정적 웹 페이지",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%EB%8F%99%EC%A0%81-%EC%9B%B9-%ED%8E%98%EC%9D%B4%EC%A7%80%EC%99%80-%EC%A0%95%EC%A0%81-%EC%9B%B9-%ED%8E%98%EC%9D%B4%EC%A7%80.html",
      "date"        : "2022-02-08 00:00:00 +0900",
      "description" : "",
      "content"     : "https://titus94.tistory.com/4 예시https://hogni.tistory.com/75"
    } ,
  
    {
      "title"       : "Kaggle ) imagenet embeddings 발표자료",
      "category"    : "",
      "tags"        : "",
      "url"         : "./kaggle-)-Imagenet-embeddings-%EB%B0%9C%ED%91%9C%EC%9E%90%EB%A3%8C.html",
      "date"        : "2022-02-08 00:00:00 +0900",
      "description" : "",
      "content"     : "오늘은 새로 가입한 kaggle 스터디에서 발표준비를 하면서 공부한 내용을 정리해 보았습니다.주제는 kaggle의 Pawpularity Contest이며, 링크는 다음과 같습니다. kaggle의 주제 사이트 분석할 코드 링크&lt; 목차 &gt; 사전지식 이미지 임베딩 Rapids SVR Finetuned models StatifiedKFold 코드 분석0. 소개(라 하고 약치기라고 이해한다) 아팠음! 결론 -&gt; 다못했음! 어차피 다들 코드 한번 본다고 바로 못쓰자나?!(당당) 전 그래요!ㅋㅋㅋㅋ이번 발표의 목표 후에 kaggle이나 익스노드를 진행할 때, 이런 아이디어가 있더라 하는 아이디어만이라도 줍줍해가시면 좋겠습니다 :&gt; 가끔 보이는 파이썬 기법들 줍줍해가시면 좋겠습니다 map이라던지 lambda라던지. 코드 이해는 못하셔도 됩니다! 저도 못했거든요^^,,,PetFinder.my 는 180,000마리 이상의 동물과 54,000마리가 행복하게 입양된 말레이시아 최고의 동물 복지 플랫폼입니다. PetFinder는 동물 애호가, 미디어, 기업 및 글로벌 조직과 긴밀하게 협력하여 동물 복지를 개선합니다.현재 PetFinder.my는 기본 귀여움 측정기 를 사용하여 애완 동물 사진의 순위를 매깁니다. 수천 개의 애완 동물 프로필의 성능과 비교하여 사진 구성 및 기타 요소를 분석합니다. 이 기본 도구는 유용하지만 아직 실험 단계에 있으며 알고리즘을 개선할 수 있습니다.1. 이미지 임베딩이란? image Embedding임베딩 Embedding이란?머신러닝의 핵심은 데이터에서 패턴을 찾는 것입니다. 머신러닝은 데이터의 특징으로부터 데이터의 패턴을 찾아 유사 데이터를 뽑아내거나 분류합니다. 하지만, 데이터는 우리가 적절한 특징을 찾도록 구조화되지 않는 경우가 많습니다. 이러한 경우 구조화되있지 않은 데이터의 특징을 나타내는 벡터가 필요한데 이러한 벡터를 임베딩 벡터 라고 부릅니다.고차원의 정보를 저차원으로 변환하면서 필요한 정보만 임베딩 벡터에 보존하는 것을 임베딩이라고 합니다. 임베딩 과정을 통해 컴퓨터는 데이터에 대한 저차원 임베딩 벡터를 생산하고 이를 통해 새로운 데이터에 대한 예측을 진행하게 됩니다.다음 예를 통해 쉽게 이해하실 수 있습니다.위와 같이 범주형 데이터가 있다고 할때, ID에는 아무 의미가 없기 때문에 이 데이터를 학습하기 위해선 다른 전처리가 필요합니다. 보통 일반적인 방법은 one-hot-encoding입니다.위의 변환된 벡터 값을 이용하여 참치김치찌개와 다른 메뉴 사이의 거리를 계산하면 다음과 같습니다.김치찌개 종류가 참치초밥과 된장찌개와는 다를텐데 전부 거리가 같습니다.이런 경우, 메뉴 추천시스템에서 어제 참치김치찌개를 먹어도 비슷한 생고기 김치찌개를 추천해주게 됩니다.임베딩을 적용하면 다음과 같습니다. 변환되는 벡터의 차원은 3차원으로 정하고 각각 찌개 강함 정도, 고기 선호도, 건강한 정도를 나타낸다고 합시다.참치김치찌개와 다른 메뉴 사이의 거리를 다시 계산해보면 다음과 같습니다.복합적인 모든 특성이 통합되어있는 고차원 데이터를 각각의 특성요소를 떼서 저차원 벡터를 만드는 과정을 임베딩이라고 합니다. https://simonezz.tistory.com/43 https://velog.io/@dongho5041/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%98-Embedding%EC%9D%B4%EB%9E%80 임베딩 추천 글 : https://blog.linewalks.com/archives/6408이미지 임베딩잡지 표지 이미지를 보고 비슷한 잡지를 찾는다고 가정할 때, pixel-by-pixel로 유사성을 측정하는 것은 의미가 없습니다. 이런 경우, 임베딩을 추출하면 semantic information을 통해 효과적으로 측정할 수 있다.2. Transfer learning많은 사람들이 ML 모델들을 이용해서 고차원이거나 복잡하거나 구조화되지 않은 데이터를 임베딩으로 인코딩하고자한다.텍스트나 이미지, 그 외 데이터들을 인코딩하기 위해 미리 트레이닝된 네트워크를 이용하여 임베딩 시키는 것을 transfer learning이라 한다.그러므로, Transfer Learning을 통해 지식을 재사용할 수 있을 뿐만 아니라, 트레이닝 시키는 시간을 대폭 줄일 수 있다.임베딩은 모델 다음방법으로 적용가능합니다. 임베딩을 인풋 feature vectors로 사용하는 것이다. Base input featrues를 미리 트레이닝된 임베딩에 쌓아서 인풋 feature vector를 형성할 수 있다. 키포인트는 이러한 임베딩이 트레이닝이 가능하지 않다는 것이다. 즉, 모델을 트레이닝시킬 때 튜닝되지 않는다. https://simonezz.tistory.com/443. RAPIDs SVR래피즈(Rapids)는 엔비디아에서 개발하고 관리하는 오픈 소스 GPU 가속 데이터 과학 및 머신러닝 라이브러리입니다. Panda, Scikit-learn, numpy 등과 같은 기존의 많은 CPU 도구와 호환되도록 설계되었습니다. 많은 데이터 과학 및 기계 학습 작업을 대규모 가속화할 수 있으며, 종종 100X 또는 그 이상의 비율로 가속화할 수 있는 툴입니다. Rapids는 여전히 개발 중에 있다고 합니다.https://rapids.ai/4. Finetuned models fine tuning이란? - 기존에 학습되어져 있는 모델을 기반으로 아키텍쳐를 새로운 목적(나의 이미지 데이터에 맞게)변형하고 이미 학습된 모델 Weights로 부터 학습을 업데이트하는 방법을 말합니다. 예시 고양이와 개 분류기를 만드는데 다른 데이터로 학습된 모델(VGG16, ResNet 등) 을 가져다 쓰는 경우 VGG16 모델의 경우 1000 개의 카테고리를 학습시켰기 때문에 고양이와 개, 2개의 카테고리만 필요한 우리 문제를 해결하는데 모든 레이어를 그대로 쓸 수는 없습니다. 따라서 가장 쉽게 이용하려면 내 데이터를 해당 모델로 예측(predict)하여 보틀넥 피쳐만 뽑아내고, 이를 이용하여 풀리 커넥티드 레이어(Fully-connected layer) 만 학습시켜서 사용하는 방법을 취하면 됩니다. 보틀넥 피처 : 가장 마지막 CNN 블록을 의미(모델에서 가장 추상화된 피처)하며, Fully connected layer 직전의 CNN블록의 결과를 의미한다. 출처: https://eehoeskrap.tistory.com/186 [Enough is not enough]5. Stratifiedkfold k-fold : 학습데이터셋과 검증 데이터 셋을 나누어서 진행하는 것 startifiedkfold : 불균형한 dataset일 경우 사용하는 kfold 방법 k개의 fold를 분할한 이후에도 전체 훈련 데이터의 class 비율과 각 fold가 가지고 있는 class 비율을 맞춰줍니다. 파라미터 n_splits : Fold의 개수 k 값 (정수형, 기본 : 5) shuffle : 데이터를 쪼갤 때 섞을지 유무 (True/False, 기본 : False) random_state : 난수 설정 https://8888-wpkwtqrc21bbba6iczeu3g152.e.prod.connect.ainize.ai/notebooks/aiffel/Untitled.ipynb features = iris.datalabels = iris.targetdt_clf = DecisionTreeClassifier(random_state=11) iris_df = pd.DataFrame(data=iris.data, columns = iris.feature_names)iris_df['label']=iris.target skf = StratifiedKFold(n_splits=5)n_iter = 0for train_index, val_index in skf.split(iris_df, iris_df['label']): print(train_index) print('\\n\\n\\n') print(val_index, '\\n\\n\\n') n_iter += 1 label_train = iris_df['label'].iloc[train_index] label_val = iris_df['label'].iloc[val_index] X_train, X_val = features[train_index], features[val_index] Y_train, Y_val = labels[train_index], labels[val_index] dt_clf.fit(X_train, Y_train) print('###### cross validation ##### : {}'.format(n_iter)) print('교차 검증 정확도 : {}'.format(accuracy_score(Y_val, dt_clf.predict(X_val)))) print('학습 레이블 데이터 분포 \\n', label_train.value_counts()) print('검증 레이블 데이터 분포 \\n', label_val.value_counts()) print('\\n\\n\\n') 코드 : https://guru.tistory.com/35 코드 : https://jinnyjinny.github.io/deep%20learning/2020/04/02/Kfold/ 이론 : https://steadiness-193.tistory.com/287 5. 코드 분석 OpenAI CLIPCLIP(Contrastive Language-Image Pre-Training)은 다양한 (이미지, 텍스트) 쌍으로 훈련된 신경망입니다. (이미지, 물체 분류) 데이터 대신 (이미지, 텍스트)의 데이터를 사용하는데, 수작업 labeling 없이 웹 크롤링을 통해 자동으로 이미지와 그에 연관된 자연어 텍스트를 추출하여 4억개의 (이미지-텍스트) 쌍을 가진 거대 데이터셋을 구축하였습니다.작업에 대한 직접 최적화 없이 주어진 이미지에서 가장 관련성 높은 텍스트 스니펫을 예측하도록 할 수 있습니다. CLIP의 성능은 ResNet50정도로 뛰어난 편에 속합니다.CLIP은 ImageNet의 대용량 데이터셋으로 미리 학습시킨 네트워크를 가져와 약간의 추가적인 학습만 수행하여도 원하는 task에서 훌륭한 성능을 낼 수 있어, 여러 분야에서 활발히 활용되고 있습니다.CLIP 제작자 설명 : https://github.com/openai/CLIP https://inforience.net/2021/02/09/clip_visual-model_pre_training/train['path'] = train['Id'].map(lambda x: '../input/petfinder-pawpularity-score/train/'+x+'.jpg') train과 test 데이터에 path 컬럼 추가. path+id.jpg 값을 넣어줌. if test.shape[0]&lt;10: test = pd.concat([ test, test, test, test, test, ]) test = test.reset_index(drop=True) 기존 test 데이터의 수를 늘려줌. if 블록의 역할은 기존의 test데이터 수를 늘려주는 것입니다.8 + 8 + 8 + 8 + 8로, 그대로 test데이터를 연장시켜주었습니다.(근데 같은 데이터값이고 index colum만 달라지는데 굳이..? 왜..?) train['bins'] = (train['Pawpularity']//5).round()train['fold0'] = -1skf = StratifiedKFold(n_splits = 20, shuffle=True, random_state = 1)for i, (_, test_index) in enumerate(skf.split(train.index, train['bins'])): train.iloc[test_index, -1] = itrain['fold0'] = train['fold0'].astype('int')gc.collect()train.groupby(['fold0'])['Pawpularity'].agg(['mean','std','count']) bins 열 stratified K fold 라벨을 20개로 맞춰주기도 하고, stratified K fold에서 pawpularity의 라벨링 비율을 맞춰주기 위해 사용 (1-100 까지의 정답라벨 존재, //5 하면 20개로 나뉘어짐) StratifiedKFold 불균형한 dataset일 경우 사용하는 kfold 방법 k개의 fold를 분할한 이후에도 전체 훈련 데이터의 class 비율과 각 fold가 가지고 있는 class 비율을 맞춰줍니다. enumerate문 나도 모르겠음. astype train[‘fold0’] 컬럼을 int형으로 변환 groupby train.groupby(['fold0'])['Pawpularity'].agg(['mean','std','count']) fold마다 pawpularity의 mean(평균), std(표준편차), count(개수)값을 나타내줍니다. fold마다 제대로 stratified k fold 잘 되었는지 확인하기 위해 쓴듯 timm libraryPyTorch Image Models(timm)는 이미지 모델, 레이어, 유틸리티, 옵티마이저, 스케줄러, 데이터 로더/증강 및 참조 교육/검증 스크립트의 모음으로, ImageNet 교육 결과를 재현할 수 있는 다양한 SOTA 모델을 통합하는 것을 목표로 합니다.https://github.com/rwightman/pytorch-image-models#introduction 보다시피 timm 라이브러리에서 사용할 수 있는 사전 훈련된 모델 아키텍처가 575개 있습니다.¶ 솔루션의 첫 번째 부분은 기본적으로 해당 모델의 마지막 계층에서 특징을 추출하고 추출된 특징에 대해 SVR을 실행하는 것입니다 timm의 모델들은 대부분 Imagenet에서 1000개의 클래스를 사용하여 훈련되기 때문에 출력 모양은 모델마다 1000개입니다.모든 575 모델에서 기능을 추출하는 것은 특히 제출 시간이 9시간이라는 점을 고려할 때 말도 안되고 상상도 할 수 없는 일이다. 따라서 RMSE 측면에서 성능이 좋은 모델의 하위 집합을 찾아보았습니다. 이를 위해 RMSE 힐 클라이밍 알고리즘에 따라 전진 모델 선택 알고리즘을 사용했다. 한 모델부터 시작하여 RMSE 성능 향상을 중지할 때까지 모델을 계속 추가합니다. 이제 Imagenet 사전 훈련된 모델 기능 추출을 시작하겠습니다.이 솔루션에 사용된 전진 모델 선택 알고리즘에 의해 발견된 사전 교육된 모델은 위에 나열되어 있습니다.modelpath = { m.split('/')[-1].split('.')[0] :m for m in glob('../input/pytorch-pretrained-0/*.pt')+glob('../input/pytorch-pretrained-1/*.pt')+glob('../input/pytorch-pretrained-2/*.pt')+glob('../input/pytorch-pretrained-3/*.pt')}modelpathstr = ../input/pytorch-pretrained-0/resnetv2_101x1_bitm.pt 일때,str.split(‘/’) 의 결과 [’..’, ‘input’, ‘pytorch-pretrained-0’, ‘resnetv2_101x1_bitm.pt’]str.split(‘/’)[-1] 의 결과 ‘resnetv2_101x1_bitm.pt’str.split(‘/’)[-1].split(‘.’)의 결과 [‘resnetv2_101x1_bitm’, ‘pt’]&lt; 참고 &gt;# EMB TEST가 뭘까EMB_TEST = {}for arch in names: starttime = time.time() model = timm.create_model(arch, pretrained=False).to('cuda') model.load_state_dict(torch.load(modelpath[arch])) model.eval() train_dataset = PawpularDataset( images = test.Id.values, base_path='../input/petfinder-pawpularity-score/test/', modelcfg = resolve_data_config({}, model=model), aug = 0, ) BS = 10 if arch in ['tf_efficientnet_l2_ns'] else 16 train_dataloader = DataLoader(train_dataset, batch_size=BS, num_workers= 2, shuffle=False) with torch.no_grad(): res = [model(img.to('cuda')).cpu().numpy() for img in train_dataloader] res = np.concatenate(res, 0) EMB_TEST[arch] = res print( arch, ', Done in:', int(time.time() - starttime), 's' ) del model, res torch.cuda.empty_cache() # PyTorch thing to clean RAM gc.collect()print(time.time() ) len(EMB_TEST), EMB_TEST.keys() timm.create_model(arch, pretrained=False).to(‘cuda’) timm.create_model timm is a deep-learning library created by Ross Wightman and is a collection of SOTA computer vision models, layers, utilities, optimizers, schedulers, data-loaders, augmentations and also training/validating scripts with ability to reproduce ImageNet training results. https://fastai.github.io/timmdocs/ pretrained=false인 상태로 names의 객체들을 불러와 모델을 생성해줍니다. gpu를 사용할 수 있도록 .to('cuda')명령어를 사용해 주었습니다. 그밖의 gpu를 사용할 수있는 명령어는 다음과 같습니다. x = torch.tensor([1., 2.], device=”cuda”) x = torch.tensor([1., 2.]).cuda() https://y-rok.github.io/pytorch/2020/10/03/pytorch-gpu.html load_state_dict(torch.load(modelpath[arch])) load_state_dict는 가중치를 불러오는 메소드입니다. 새로운 모델 인스턴스를 생성한 후에 가중치를 불러올 경우, pretrained =False를 주고, load_state_dict메소드를 이용해 가중치 값을불러오게 됩니다. https://tutorials.pytorch.kr/beginner/basics/saveloadrun_tutorial.html model.eval() 추론(inference)을 하기 전에 model.eval() 메소드를 호출하여 드롭아웃(dropout)과 배치 정규화(batch normalization)를 평가 모드(evaluation mode)로 설정해야 합니다. 그렇지 않으면 일관성 없는 추론 결과가 생성됩니다. .eval() 함수는 evaluation 과정에서 사용하지 않아야 하는 layer들을 알아서 off 시키도록 하는 함수이며 evaluation/validation 과정에선 보통 model.eval()과 torch.no_grad()를 함께 사용합니다. 사용 방법 관련 url : https://tutorials.pytorch.kr/beginner/basics/saveloadrun_tutorial.html eval 이 무엇인지 : https://stackoverflow.com/questions/60018578/what-does-model-eval-do-in-pytorch/60018731#60018731 eval 이 무엇인지 : https://bluehorn07.github.io/2021/02/27/model-eval-and-train.html DataLoader(train_dataset, batch_size=BS, num_workers= 2, shuffle=False) dataloader는 기본적으로 단일 프로세스 데이터 로드를 사용합니다. num_workers 에 양의 정수를 설정하게되면, 지정된 수의 로더 작업자 프로세스로 멀티 프로세스 데이터 로드가 켜집니다. https://tutorials.pytorch.kr/beginner/basics/data_tutorial.html with torch.no_grad(): 자원을 획득하고 사용 후 반납해야 하는 경우에 주로 사용합니다. 프로세스 : 현재 실행되고 있는 프로그램 자원 : 기억장치 Ram이나 프로세서 디스크 등의 하드웨어 장치나 메시지, 파일등의 소프트웨어 요소들 https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&amp;blogId=wideeyed&amp;logNo=221653260516 Now its time to fit a GPU accelerated SVR using cuml¶for fold in range(train[kfoldcol].max()+1): ind_train = train[kfoldcol] != fold ind_valid = train[kfoldcol] == fold 이해못함 ^_^/model = SVR(C=16.0, kernel='rbf', degree=3, max_iter=4000, output_type='numpy') model.fit(TRAIN[ind_train], train.Pawpularity[ind_train].clip(1, 85) ) ypredtrain_[ind_valid] = np.clip(model.predict(TRAIN[ind_valid]), 1 , 100) ypredtest_ += np.clip(model.predict(TEST), 1, 100) SVR Rapids SVR을 사용하는 방법은 쉽네요. cuml.svm 패키지에서 SVR을 import 해서 사용해주었습니다. np.clip(array, min, max) array내의 element에 대하여 min값보다 작은 값들을 min으로 바꿔주고, max보다 큰 값을 max로 만들어줍니다. 첫째, 각 아키텍처에 대해 하나의 SVR을 독립적으로 장착합니다.for col in names: TRAIN = EMB_TRAIN[col].copy() TEST = EMB_TEST[col].copy() scaler = StandardScaler() scaler.fit( np.vstack((TRAIN, TEST)) ) TRAIN = scaler.transform(TRAIN) TEST = scaler.transform(TEST) ypredtrain, ypredtest = fit_gpu_svr(TRAIN, TEST, 'fold0') print(rmse(train.Pawpularity,ypredtrain), col) StandartScaler() -&gt; 평균 0 , 분산 1로 조정합니다 보통의 싸이킷런 함수들과 비슷하게 fit, transform, fit_transform 다 지원합니다. https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&amp;blogId=demian7607&amp;logNo=222009975984 np.vstack 두 배열을 위에서 아래로 붙이기 np.hstack 두 배열을 왼쪽에서 오른쪽으로 붙이기 https://rfriend.tistory.com/352 StandardScaler.transform() 정규화 / 표준화 Standardization, z = (𝑥-𝜇)/𝜎 위에서 볼 수 있듯이, 개별 아키텍처에서 추출된 기능의 SVR RMSE 범위는 17.56 ~ 18.52입니다. 하지만 SVR에 맞추기 전에 아키텍처 기능을 나란히 쌓으면 어떻게 될까요? 일부 기능 연결 및 표준화 모든 K 폴드를 사용하여 SVR A를 장착합니다.RMSE를 약간 증가시킨 85에서 목표물을 클립하는 것을 발견했어요. 또한 1.032의 승수를 사용하면 CV와 LB가 모두 향상된다는 것을 알게 되었습니다. 이는 SRV가 RMSE가 아닌 평균 제곱 오차를 최적화하기 때문일 수 있다. 이제 두 번째 feature 부분 집합을 사용하여 SVR B를 장착합니다. 더 많은 하위 집합을 적합시키는 아이디어는 후방 모델 앙상블에 다양성을 추가하고 형상 수를 너무 많이 증가시키는 차원성의 저주를 피하기 위한 것이다. 이제 딥 러닝 미세 조정된 이미지 모델을 사용하여 추론을 실행합니다.from torch.utils.data import Dataset, DataLoaderimport albumentations as Adevice = torch.device('cuda')class Config: model_name = \"swin_large_patch4_window7_224\" base_dir = \"../input/petfinder-pawpularity-score\" data_dir = base_dir model_dir = \"exp\" output_dir = model_dir img_test_dir = os.path.join(data_dir, \"test\") model_path = \"swin_large_patch4_window7_224\" im_size = 384 batch_size = 16class PetDataset(Dataset): def __init__(self, image_filepaths, targets, transform=None): self.image_filepaths = image_filepaths self.targets = targets self.transform = transform def __len__(self): return len(self.image_filepaths) def __getitem__(self, idx): image_filepath = self.image_filepaths[idx] with open(image_filepath, 'rb') as f: image = Image.open(f) image_rgb = image.convert('RGB') image = np.array(image_rgb) if self.transform is not None: image = self.transform(image = image)[\"image\"] image = image / 255 image = np.transpose(image, (2, 0, 1)).astype(np.float32) target = self.targets[idx] image = torch.tensor(image, dtype = torch.float) target = torch.tensor(target, dtype = torch.float) return image, target def get_inference_fixed_transforms(mode=0, dim = 224): if mode == 0: # do not original aspects, colors and angles return A.Compose([ A.SmallestMaxSize(max_size=dim, p=1.0), A.CenterCrop(height=dim, width=dim, p=1.0), ], p=1.0) elif mode == 1: return A.Compose([ A.SmallestMaxSize(max_size=dim+16, p=1.0), A.CenterCrop(height=dim, width=dim, p=1.0), A.HorizontalFlip(p = 1.0) ], p=1.0)class PetNet(nn.Module): def __init__( self, model_name = Config.model_path, out_features = 1, inp_channels = 3, pretrained = False, ): super().__init__() self.model = timm.create_model(model_name, pretrained=False, in_chans=3, num_classes = 1) def forward(self, image): output = self.model(image) return output def tta_fn(filepaths, model, ttas=[0, 1]): print('Image Size:', Config.im_size) model.eval() tta_preds = [] for tta_mode in ttas:#range(Config.tta_times): print(f'tta mode:{tta_mode}') test_dataset = PetDataset( image_filepaths = filepaths, targets = np.zeros(len(filepaths)), transform = get_inference_fixed_transforms(tta_mode, dim = Config.im_size ) ) test_loader = DataLoader( test_dataset, batch_size = Config.batch_size, shuffle = False, num_workers = 2, pin_memory = True ) #stream = tqdm(test_loader) tta_pred = [] for images, target in test_loader:#enumerate(stream, start = 1): images = images.to(device, non_blocking = True).float() target = target.to(device, non_blocking = True).float().view(-1, 1) with torch.no_grad(): output = model(images) pred = (torch.sigmoid(output).detach().cpu().numpy() * 100).ravel().tolist() tta_pred.extend(pred) tta_preds.append(np.array(tta_pred)) fold_preds = tta_preds[0] for n in range(1, len(tta_preds)): fold_preds += tta_preds[n] fold_preds /= len(tta_preds) del test_loader, test_dataset gc.collect() torch.cuda.empty_cache() return fold_preds cuML SVR A, B, C 및 이미지 모델 앙상블의 OOF 예측을 사용하여 전체 RMSE를 최적화합니다.OOF out of fold 머신러닝 모델의 성능을 평가하는 방법 OOF 방식은 실무보다는 Kaggle, Dacon과 같은 예측 알고리즘 대회에서 자주 사용되는 방식이며 fold를 사용 두가지 방식 Stacking 5 Fold 기반으로 개별 모델이 5회 번갈아 가면 4/5 학습 데이터로 학습하고, 1/5 학습 데이터로 예측하여 별도의 학습 데이터를 만듭니다. 그리고 이렇게 만들어진 학습 데이터를 다시 메타 모델이(아마도 Model 6) 학습하여 최종 예측 하는 방식입니다. 개별 예측값 평균하는 방법 K-Fold로 학습을 수행 한 뒤 예측을 테스트 데이터에 K번 만큼 수행한 뒤 개별 예측값을 평균하여 최종 예측 https://techblog-history-younghunjo1.tistory.com/142 위 그림의 좌측은 4개의 Fold로 교차검증하는 K=4 일때의 K-fold 교차검증 방법이다. 이렇게 총 4번의 교차검증으로 (동일한 하이퍼파라미터를 사용한) 예측 알고리즘으로 각 검증 데이터로 평가한 Model 1~4를 만들어낸다.(이 때, Model 1,2,3,4에서 각각 최적화된 파라미터 값들은 다를 것이다. 왜냐하면 각 모델이 학습한 데이터가 서로 다르기 때문이다.) 그리고 난 후, Model 1~4를 동일한 테스트 데이터에 대해 예측하도록 하여 각 Model 별 테스트 데이터에 대한 예측값을 계산한다. 이렇게 되면 위 그림에서 노란색 박스의 Model 1~4 예측값이 나오게 된다. 마지막으로 할 일은 이 4개의 예측값들의 평균값을 취하여 테스트 데이터에 대한 최종 예측값을 계산한다."
    } ,
  
    {
      "title"       : "[kaggle 참가하고 싶은 대회 리스트",
      "category"    : "",
      "tags"        : "",
      "url"         : "./kaggle-%EC%B0%B8%EA%B0%80%ED%95%98%EA%B3%A0-%EC%8B%B6%EC%9D%80-%EB%8C%80%ED%9A%8C-%EB%A6%AC%EC%8A%A4%ED%8A%B8.html",
      "date"        : "2022-02-08 00:00:00 +0900",
      "description" : "",
      "content"     : "1. I’m Something of a Painter Myselfhttps://www.kaggle.com/c/gan-getting-startedGAN은 생성기 모델과 판별기 모델의 두 개 이상의 신경망으로 구성됩니다. 생성기는 이미지를 생성하는 신경망입니다. 경쟁을 위해 Monet 스타일로 이미지를 생성해야 합니다. 이 생성기는 판별자를 사용하여 학습됩니다.생성자는 판별자를 속이려고 하고 판별자는 실제 대 생성된 이미지를 정확하게 분류하려고 하는 두 모델이 서로에 대해 작동합니다.당신의 임무는 7,000~10,000개의 모네 스타일 이미지를 생성하는 GAN을 만드는 것입니다.2. PetFinder.my - Pawpularity Contesthttps://www.kaggle.com/c/petfinder-pawpularity-scorePetFinder.my 는 180,000마리 이상의 동물과 54,000마리가 행복하게 입양된 말레이시아 최고의 동물 복지 플랫폼입니다. PetFinder는 동물 애호가, 미디어, 기업 및 글로벌 조직과 긴밀하게 협력하여 동물 복지를 개선합니다.현재 PetFinder.my는 기본 귀여움 측정기 를 사용하여 애완 동물 사진의 순위를 매깁니다. 수천 개의 애완 동물 프로필의 성능과 비교하여 사진 구성 및 기타 요소를 분석합니다. 이 기본 도구는 유용하지만 아직 실험 단계에 있으며 알고리즘을 개선할 수 있습니다.3. Lyft Motion Prediction for Autonomous Vehicleshttps://www.kaggle.com/c/lyft-motion-prediction-autonomous-vehicles이 대회에서는 데이터 과학 기술을 적용하여 자율 주행 차량용 모션 예측 모델을 구축합니다. 모델을 훈련하고 테스트하기 위해 출시된 가장 큰 예측 데이터 세트 에 액세스할 수 있습니다 . 그런 다음 자동 학습 환경에서 자동차, 자전거 타는 사람 및 보행자가 움직이는 방식을 예측하려면 기계 학습에 대한 지식이 필요합니다.4. Natural Language Processing with Disaster Tweetshttps://www.kaggle.com/c/nlp-getting-started트위터는 비상시에 중요한 커뮤니케이션 채널이 되었습니다.스마트폰의 보편화로 인해 사람들은 실시간으로 관찰 중인 긴급 상황을 알릴 수 있습니다. 이 때문에 더 많은 기관이 Twitter를 프로그래밍 방식으로 모니터링하는 데 관심이 있습니다(예: 재난 구호 단체 및 뉴스 기관).그러나 사람의 말이 실제로 재난을 알리는 것인지 항상 명확하지 않습니다. 다음 예를 들어보세요.이 대회에서는 실제 재해에 대한 트윗과 그렇지 않은 트윗을 예측하는 기계 학습 모델을 구축해야 합니다. 손으로 분류한 10,000개의 트윗 데이터 세트에 액세스할 수 있습니다. NLP 문제에 대한 작업이 처음인 경우 시작하고 실행할 수 있도록 빠른 자습서 를 만들었습니다.면책 조항: 이 대회의 데이터 세트에는 모독적이거나 저속하거나 공격적인 것으로 간주될 수 있는 텍스트가 포함되어 있습니다.5. Lux AIhttps://www.kaggle.com/c/lux-ai-2021/overview/description밤은 어둡고 공포로 가득 차 있습니다. 두 팀은 어둠과 싸워야 하고, 자원을 수집하고, 시대를 초월해 전진해야 합니다. 낮에는 도시를 성장시키면서 임박한 밤을 보낼 수 있는 자원을 모으기 위해 필사적으로 서두르게 됩니다. 신중하게 계획하고 확장하십시오. 충분한 빛을 생산하지 못하는 도시는 어둠에 휩싸일 것입니다.6. Shopee - Price Match Guaranteehttps://www.kaggle.com/c/shopee-product-matching유사한 상품의 두 가지 다른 이미지는 동일한 제품 또는 완전히 다른 두 개의 항목을 나타낼 수 있습니다. 소매업체는 서로 다른 두 제품을 혼합하여 발생할 수 있는 허위 진술 및 기타 문제를 방지하기를 원합니다. 현재 딥 러닝과 기존 머신 러닝을 결합하여 이미지와 텍스트 정보를 분석하여 유사성을 비교합니다. 그러나 이미지, 제목 및 제품 설명의 주요 차이점으로 인해 이러한 방법이 완전히 효과적이지 않습니다.Shopee는 동남아시아와 대만의 선도적인 전자 상거래 플랫폼입니다. 고객은 해당 지역에 맞는 쉽고 안전하며 빠른 온라인 쇼핑 경험을 높이 평가합니다. 이 회사는 또한 수천 개의 Shopee 목록에 있는 제품에 대한 ‘최저가 보장’ 기능과 함께 강력한 지불 및 물류 지원을 제공합니다.이 대회에서는 기계 학습 기술을 적용하여 동일한 제품인 항목을 예측하는 모델을 구축합니다.7. Peking University/Baidu - Autonomous Drivinghttps://www.kaggle.com/c/pku-autonomous-drivingBaidu의 Robotics and Autonomous Driving Lab(RAL)은 Peking University와 함께 이 도전을 통해 격차를 완전히 좁힐 수 있기를 희망합니다. 그들은 Kaggler에게 산업 등급 CAD 자동차 모델을 기반으로 하는 5,277개의 실제 이미지에서 60,000개 이상의 레이블이 지정된 3D 자동차 인스턴스를 제공하고 있습니다.과제: 실제 교통 환경에서 단일 이미지에서 차량의 절대 자세(6자유도)를 추정하는 알고리즘을 개발하십시오.성공하면 컴퓨터 비전을 개선하는 데 도움이 됩니다. 이는 차례로 자율주행차가 널리 채택되는 데 한 걸음 더 다가가게 하여 성장하는 사회의 환경적 영향을 줄이는 데 도움이 될 것입니다.8. TalkingData AdTracking Fraud Detection Challengehttps://www.kaggle.com/c/talkingdata-adtracking-fraud-detection사기 위험은 어디에나 있지만 온라인 광고를 하는 회사의 경우 클릭 사기가 압도적으로 발생하여 잘못된 클릭 데이터와 비용 낭비를 초래할 수 있습니다. 광고 채널은 단순히 광고를 대규모로 클릭함으로써 비용을 증가시킬 수 있습니다. 매달 10억 개 이상의 스마트 모바일 장치가 사용되는 중국은세계에서 가장 큰 모바일 시장이므로 엄청난 양의 사기 트래픽으로 고통받고 있습니다.중국 최대의 독립 빅데이터 서비스 플랫폼인 TalkingData 는 전국 활성 모바일 기기의 70% 이상을 커버합니다. 그들은 하루에 30억 번의 클릭을 처리하며 그 중 90%는 잠재적인 사기입니다. 앱 개발자를 위한 클릭 사기를 방지하기 위한 현재의 접근 방식은 포트폴리오 전체에서 사용자 클릭의 여정을 측정하고 많은 클릭을 생성하지만 앱을 설치하지 않는 IP 주소에 플래그를 지정하는 것입니다. 이 정보를 바탕으로 IP 블랙리스트와 기기 블랙리스트를 구축했습니다."
    } ,
  
    {
      "title"       : "Kaggle ) imagenet embeddings+rapids svr+finetuned models 분석",
      "category"    : "",
      "tags"        : "",
      "url"         : "./Kaggle-)-Imagenet-embeddings+RAPIDS-SVR+Finetuned-models-%EB%B6%84%EC%84%9D.html",
      "date"        : "2022-02-08 00:00:00 +0900",
      "description" : "",
      "content"     : "오늘은 새로 가입한 kaggle 스터디에서 발표준비를 하면서 공부한 내용을 정리해 보았습니다.주제는 kaggle의 Pawpularity Contest이며, 링크는 다음과 같습니다. kaggle의 주제 사이트 분석할 코드 링크&lt; 목차 &gt;1. Pawpularity Datasample_submission.csv[id, pawpularity] 로 구성test.csvtrain에서 pawpularity가 없는 데이터입니다.id값 빼고는 전부 0, 1 이진데이터입니다. id The photos unique Pet Profile ID corresponding to the photo’s file name. Subject Focus Pet stands out against uncluttered background, not too close / far. Eyes Both eyes are facing front or near-front, with at least 1 eye / pupil decently clear. Face Decently clear face, facing front or near-front. Near Single pet taking up significant portion of photo (roughly over 50% of photo width or height). Action Pet in the middle of an action (e.g., jumping). Accessory Accompanying physical or digital accessory / prop (i.e. toy, digital sticker), excluding collar and leash. Group More than 1 pet in the photo. Collage Digitally-retouched photo (i.e. with digital photo frame, combination of multiple photos). Human Human in the photo. Occlusion Specific undesirable objects blocking part of the pet (i.e. human, cage or fence). Note that not all blocking objects are Info Custom-added text or labels (i.e. pet name, description). Blur Noticeably out of focus or noisy, especially for the pet’s eyes and face. For Blur entries, “Eyes” column is always set to 0. train.csvid값, pawpularity 빼고는 전부 0, 1 이진데이터입니다.pawpulairty는 1-100까지의 int형 data입니다. id The photos unique Pet Profile ID corresponding to the photo’s file name. Subject Focus Pet stands out against uncluttered background, not too close / far. Eyes Both eyes are facing front or near-front, with at least 1 eye / pupil decently clear. Face Decently clear face, facing front or near-front. Near Single pet taking up significant portion of photo (roughly over 50% of photo width or height). Action Pet in the middle of an action (e.g., jumping). Accessory Accompanying physical or digital accessory / prop (i.e. toy, digital sticker), excluding collar and leash. Group More than 1 pet in the photo. Collage Digitally-retouched photo (i.e. with digital photo frame, combination of multiple photos). Human Human in the photo. Occlusion Specific undesirable objects blocking part of the pet (i.e. human, cage or fence). Note that not all blocking objects are Info Custom-added text or labels (i.e. pet name, description). Blur Noticeably out of focus or noisy, especially for the pet’s eyes and face. For Blur entries, “Eyes” column is always set to 0. Pawpularity The Pawpularity Score is derived from each pet profile’s page view statistics at the listing pages, using an algorithm 2. OpenAI CLIPCLIP(Contrastive Language-Image Pre-Training)은 다양한 (이미지, 텍스트) 쌍으로 훈련된 신경망입니다. GPT-2 및 3의 제로샷 기능과 유사하게 작업에 대한 직접 최적화 없이 주어진 이미지에서 가장 관련성 높은 텍스트 스니펫을 예측하도록 자연어로 지시할 수 있습니다. 우리는 CLIP이 원본 ResNet50의 성능과 일치함을 발견했습니다. ImageNet에서 원래 128만 개의 레이블이 지정된 예제를 사용하지 않고 “제로샷”을 수행하여 컴퓨터 비전의 몇 가지 주요 문제를 극복했습니다.참고 링크 : https://github.com/openai/CLIPImage Embedding임베딩 Embedding이란?고차원 벡터를 저차원 공간으로 변환하는 것입니다. 이상적으로, 임베딩은 임베딩 공간에서 의미적으로 비슷한 입력 사항들을 가깝게 배치함으로써 입력에 포함된 의미 중 일부를 포착합니다. https://cloud.google.com/architecture/overview-extracting-and-serving-feature-embeddings-for-machine-learning?hl=ko이미지 임베딩텍스트 시스템과 달리, 이미지 처리 시스템은 개별적인 원시 픽셀 강도를 지닌 이미지를 나타내는 풍부하고 고차원적인 데이터세트로 작동합니다. 하지만 원래의 밀도가 높은 형태의 이미지는 일부 작업에 그리 유용하지 않을 수 있습니다. 예를 들어 잡지 표지 이미지를 보고 비슷한 잡지를 찾거나 참조 사진과 비슷한 사진을 찾아야 한다고 가정해보세요. 입력 사진의 원시 픽셀(2,048 ✕ 2,048)을 다른 사진과 비교하여 비슷한지 여부를 찾는 것은 효율적이거나 효과적이지 않습니다. 하지만 이미지의 저차원적 특성 벡터(임베딩)를 추출하면 이미지에 포함된 내용이 무엇인지를 나타내는 일정한 지표를 얻고, 더 효과적으로 비교할 수 있습니다.핵심은 대규모 이미지 데이터 세트(예: ImageNet)에서 Inception, ResNet(Deep Residual Learning) 또는 NASNet(Network Architecture Search)과 같은 이미지 분류 모델을 학습시키는 것입니다. 그런 다음 마지막 softmax 분류 기준이 없는 모델을 사용하여 입력 테스트에 따라 특징 벡터를 추출합니다. 이 유형의 특징 벡터로 검색 작업 또는 유사성 일치 작업에서 이미지를 효과적으로 표현할 수 있습니다. 특징 벡터는 다른 특성과 함께 ML 작업용 추가 입력 특성(벡터)으로 작동할 수 있습니다. 예를 들어 의류를 쇼핑 중인 고객에게 패션 아이템을 추천하는 시스템에서는 색상, 치수, 가격, 유형, 하위유형을 비롯한 개별 항목을 기술하는 속성이 사용될 수 있습니다. 패션 아이템 이미지에서 추출된 특성과 함께 이러한 모든 특성을 추천 모델에서 사용할 수 있습니다.RAPID SVRStratifiedKFold k-fold : 학습데이터셋과 검증 데이터 셋을 나누어서 진행하는 것 startifiedkfold : 불균형한 dataset일 경우 사용하는 kfold 방법 k개의 fold를 분할한 이후에도 전체 훈련 데이터의 class 비율과 각 fold가 가지고 있는 class 비율을 맞춰줍니다. 코드 : https://guru.tistory.com/35 이론 : https://steadiness-193.tistory.com/287코드 분석train = pd.read_csv('../input/petfinderdata/train-folds-1.csv')test = pd.read_csv('../input/petfinder-pawpularity-score/test.csv')sub = pd.read_csv('../input/petfinder-pawpularity-score/sample_submission.csv')train['path'] = train['Id'].map(lambda x: '../input/petfinder-pawpularity-score/train/'+x+'.jpg')test['path'] = test['Id'].map(lambda x: '../input/petfinder-pawpularity-score/test/'+x+'.jpg')#print(test)# If its Public LB run, then augment Testset to chack batch size memory consumption.if test.shape[0]&lt;10: test = pd.concat([ test, test, test, test, test, ]) test = test.reset_index(drop=True) # print(test) print(train.shape, test.shape, sub.shape)print(train.head(50)) train과 test 데이터에 path 컬럼 추가.path+id.jpg 값을 넣어줌.기존 test 데이터의 수를 늘려줌.if 블록의 역할은 기존의 test데이터 수를 늘려주는 것입니다.8 + 8 + 8 + 8 + 8로, 그대로 test데이터를 연장시켜주었습니다.(근데 같은 데이터값이고 index colum만 달라지는데 굳이..? 왜..?)test.reset_index(drop = True)drop매개변수를 사용하면 DataFrame에서 인덱스를 완전히 삭제할지 여부를 지정할 수 있습니다 . drop = True하면 reset_index가 인덱스를 DataFrame의 열에 다시 삽입하는 대신 삭제합니다. 를 설정 drop = True하면 현재 인덱스가 완전히 삭제되고 숫자 인덱스가 이를 대체합니다. https://www.sharpsightlabs.com/blog/pandas-reset-index/# print(train.head(50)) train['bins'] = (train['Pawpularity']//5).round()# print(train['Pawpularity'])# print(train['Pawpularity'].value_counts())# print(train.head(50)) print(train['bins'].value_counts())train['fold0'] = -1skf = StratifiedKFold(n_splits = 20, shuffle=True, random_state = 1)for i, (_, test_index) in enumerate(skf.split(train.index, train['bins'])): train.iloc[test_index, -1] = iprint('test_index', test_index) print('test_index len', len(test_index))# print(train.head(50)) train['fold0'] = train['fold0'].astype('int')gc.collect()train.groupby(['fold0'])['Pawpularity'].agg(['mean','std','count'])bins 열pawpularity의 정답라벨을 20개로 맞춰주기 위함 (1-100 까지의 정답라벨 존재, //5 하면 20개로 나뉘어짐)StratifiedKFold 불균형한 dataset일 경우 사용하는 kfold 방법 k개의 fold를 분할한 이후에도 전체 훈련 데이터의 class 비율과 각 fold가 가지고 있는 class 비율을 맞춰줍니다. enumerate문나도 모르겠음.astypetrain[‘fold0’] 컬럼을 int형으로 변환groupbytrain.groupby(['fold0'])['Pawpularity'].agg(['mean','std','count'])fold마다 pawpularity의 mean(평균), std(표준편차), count(개수)값을 나타내줍니다.modelpath = { m.split('/')[-1].split('.')[0] :m for m in glob('../input/pytorch-pretrained-0/*.pt')+glob('../input/pytorch-pretrained-1/*.pt')+glob('../input/pytorch-pretrained-2/*.pt')+glob('../input/pytorch-pretrained-3/*.pt')}modelpath예시 str = ../input/pytorch-pretrained-0/resnetv2_101x1_bitm.pt 일때,str.split(‘/’) 의 결과 ../input/pytorch-pretrained-0/resnetv2_101x1_bitm.ptstr.split(‘/’)[-1] 의 결과 [’..’, ‘input’, ‘pytorch-pretrained-0’, ‘resnetv2_101x1_bitm.pt’]str.split(‘/’)[-1].split(‘.’)의 결과 [‘resnetv2_101x1_bitm’, ‘pt’]# EMB TEST가 뭘까EMB_TEST = {}for arch in names: starttime = time.time() model = timm.create_model(arch, pretrained=False).to('cuda') model.load_state_dict(torch.load(modelpath[arch])) model.eval() train_dataset = PawpularDataset( images = test.Id.values, base_path='../input/petfinder-pawpularity-score/test/', modelcfg = resolve_data_config({}, model=model), aug = 0, ) BS = 10 if arch in ['tf_efficientnet_l2_ns'] else 16 train_dataloader = DataLoader(train_dataset, batch_size=BS, num_workers= 2, shuffle=False) with torch.no_grad(): res = [model(img.to('cuda')).cpu().numpy() for img in train_dataloader] res = np.concatenate(res, 0) EMB_TEST[arch] = res print( arch, ', Done in:', int(time.time() - starttime), 's' ) del model, res torch.cuda.empty_cache() # PyTorch thing to clean RAM gc.collect()print(time.time() ) len(EMB_TEST), EMB_TEST.keys()timm.create_model(arch, pretrained=False).to(‘cuda’) timm.create_model timm is a deep-learning library created by Ross Wightman and is a collection of SOTA computer vision models, layers, utilities, optimizers, schedulers, data-loaders, augmentations and also training/validating scripts with ability to reproduce ImageNet training results. https://fastai.github.io/timmdocs/ pretrained=false인 상태로 names의 객체들을 불러와 모델을 생성해줍니다. gpu를 사용할 수 있도록 .to('cuda')명령어를 사용해 주었습니다. 그밖의 gpu를 사용할 수있는 명령어는 다음과 같습니다. x = torch.tensor([1., 2.], device=”cuda”) x = torch.tensor([1., 2.]).cuda() https://y-rok.github.io/pytorch/2020/10/03/pytorch-gpu.html load_state_dict(torch.load(modelpath[arch])) load_state_dict는 가중치를 불러오는 메소드입니다. 새로운 모델 인스턴스를 생성한 후에 가중치를 불러올 경우, pretrained =False를 주고, load_state_dict메소드를 이용해 가중치 값을불러오게 됩니다. https://tutorials.pytorch.kr/beginner/basics/saveloadrun_tutorial.htmlmodel.eval() 추론(inference)을 하기 전에 model.eval() 메소드를 호출하여 드롭아웃(dropout)과 배치 정규화(batch normalization)를 평가 모드(evaluation mode)로 설정해야 합니다. 그렇지 않으면 일관성 없는 추론 결과가 생성됩니다. .eval() 함수는 evaluation 과정에서 사용하지 않아야 하는 layer들을 알아서 off 시키도록 하는 함수이며 evaluation/validation 과정에선 보통 model.eval()과 torch.no_grad()를 함께 사용합니다. 사용 방법 관련 url : https://tutorials.pytorch.kr/beginner/basics/saveloadrun_tutorial.html eval 이 무엇인지 : https://stackoverflow.com/questions/60018578/what-does-model-eval-do-in-pytorch/60018731#60018731 eval 이 무엇인지 : https://bluehorn07.github.io/2021/02/27/model-eval-and-train.htmlDataLoader(train_dataset, batch_size=BS, num_workers= 2, shuffle=False) dataloader는 기본적으로 단일 프로세스 데이터 로드를 사용합니다. num_workers 에 양의 정수를 설정하게되면, 지정된 수의 로더 작업자 프로세스로 멀티 프로세스 데이터 로드가 켜집니다. https://tutorials.pytorch.kr/beginner/basics/data_tutorial.html with torch.no_grad(): 자원을 획득하고 사용 후 반납해야 하는 경우에 주로 사용합니다. https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&amp;blogId=wideeyed&amp;logNo=221653260516 class PawpularDataset_HFLIP: def __init__(self, images, base_path='../input/petfinder-pawpularity-score/train/', modelcfg=None, doflip=False ): self.images = images.copy() self.base_path = base_path self.transform = modelcfg self.doflip=doflip def __len__(self): return len(self.images) def __getitem__(self, item): img = Image.open(self.base_path + self.images[item] + '.jpg').convert('RGB') if self.doflip==True: img = img.transpose(PIL.Image.FLIP_LEFT_RIGHT) width, height = img.size img = img.crop((0.0*width, 0.02*height, 0.98*width, 0.98 * height)) img = self.transform(img) return imgfor arch in names_hflip_crop: starttime = time.time() archname = arch.split('_hflip_')[0] if arch == 'tf_efficientnet_l2_ns_512': archname = 'tf_efficientnet_l2_ns' model = timm.create_model(archname, pretrained=False).to('cuda') model.load_state_dict(torch.load(modelpath[archname])) model.eval() # Get model default transforms transf = resolve_data_config({}, model=model) sz = int(arch.split('_')[-1]) transf['input_size'] = (3, sz, sz) transf['crop_pct'] = 1.0 transf = create_transform(**transf) doflip = True if arch.split('_')[-2] == 'hflip' else False train_dataset = PawpularDataset_HFLIP( images = test.Id.values, base_path='../input/petfinder-pawpularity-score/test/', modelcfg = transf, doflip = doflip, ) BS = 10 if archname in ['tf_efficientnet_l2_ns'] else 16 train_dataloader = DataLoader(train_dataset, batch_size=BS, num_workers= 2, shuffle=False) with torch.no_grad(): res = [model(img.to('cuda')).cpu().numpy() for img in train_dataloader] res = np.concatenate(res, 0) EMB_TEST[arch] = res print( arch, 'imge size:', sz, 'Hflip:', doflip, ',Done in:', int(time.time() - starttime), 's' ) del model, res torch.cuda.empty_cache() # PyTorch thing to clean RAM gc.collect()print(time.time() ) len(EMB_TEST), EMB_TEST.keys()img.transpose(PIL.Image.FLIP_LEFT_RIGHT)이미지를 가로방향으로 뒤집기arch.split()archname = arch.split('_hflip_')[0] if arch == 'tf_efficientnet_l2_ns_512': archname = 'tf_efficientnet_l2_ns'archname에 모델이름만 들어가도록 처리sz = int(arch.split(‘_’)[-1])새로 알게된 함수 round() 반올림 함수입니다. import math를 하면 사용할 수 있으며, 숫자데이터.round()형식으로 사용합니다. pandas.astype(‘타입’) 컬럼의 데이터 타입을 변경 궁금한 점 Load Train and Test # If its Public LB run, then augment Testset to chack batch size memory consumption.if test.shape[0]&lt;10: test = pd.concat([ test, test, test, test, test, ]) test = test.reset_index(drop=True) 기본 제공 데이터에서 test 데이터 수가 8개가 있습니다. 코드를 실행하면 test 데이터 수가 10개보다 작기 때문에 test 한세트를 5번 연장 시켜 총 40개의 test셋으로 늘려주었습니다. 하지만 이렇게 해봤자 같은 test데이터이기 때문에 각각 같은 결과가 나올 것 같은데 굳이 왜 이렇게 해주었는지 모르겠습니다. StratifiedKFold train['bins'] = (train['Pawpularity']//5).round() train['fold0'] = -1skf = StratifiedKFold(n_splits = 20, shuffle=True, random_state = 1)for i, (_, test_index) in enumerate(skf.split(train.index, train['bins'])): train.iloc[test_index, -1] = i train['fold0'] = train['fold0'].astype('int')gc.collect() train.groupby(['fold0'])['Pawpularity'].agg(['mean','std','count']) enumerate부분 잘 몰겠다. 어떤 효과인지 ㅎ_ㅎ"
    } ,
  
    {
      "title"       : "Cs231 5강. cnn",
      "category"    : "",
      "tags"        : "",
      "url"         : "./CS231-5%EA%B0%95.-CNN.html",
      "date"        : "2022-02-08 00:00:00 +0900",
      "description" : "",
      "content"     : "Mirror paddinghttps://joungheekim.github.io/2020/09/28/paper-review/https://stackoverflow.com/questions/42317238/why-do-we-use-fully-connected-layer-at-the-end-of-cnnhttps://www.cs.ryerson.ca/~aharley/vis/conv/"
    } ,
  
    {
      "title"       : "Alexnet이란",
      "category"    : "",
      "tags"        : "",
      "url"         : "./AlexNet%EC%9D%B4%EB%9E%80.html",
      "date"        : "2022-02-08 00:00:00 +0900",
      "description" : "",
      "content"     : "오늘은 AlexNet에 대해서 정리해보겠습니다.참고한 자료는 다음과 같습니다. chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/viewer.html?pdfurl=https%3A%2F%2Fproceedings.neurips.cc%2Fpaper%2F2012%2Ffile%2Fc399862d3b9d6b76c8436e924a68c45b-Paper.pdf&amp;clen=1418820 https://bskyvision.com/4211. AlexNet이란?2012년 Image Classification Task에서 Geoffrey Hinton 교수님이 이끄는 토론토 대학의 SuperVision팀이 오류율 16%로 1등을 달성합니다. 작년의 우승자는 오류율 26%, 제작년은 28%였으니 엄청난 차이를 보여주었습니다.superVision팀이 사용한 모델이 AlexNet입니다.AlexNet의 구조기본 구조는 LeNet-5와 크게 다르지 않습니다. 2개의 GPU로 병렬연산을 수행하기 위해 병렬 구조로 설계되었다는 점이 가장 큰 변화입니다.총 8개의 레이어로 이루어져 있으며, 5개는 Conv 레이어, 3개는 FC 레이어로 구성되어 있습니다. (맨 왼쪽은 input data입니다.)2, 4, 5번째 컨볼루션 레이어들은 전 단계의 feature map과 연결되어 있지만, 3번째 레이어에서는 병렬처리로 나뉜 두 단계의 feature맵과 모두 연결되어 있는 것이 특징입니다.input data227 * 227 * 3 의 이미지가 들어옵니다. (그림에선 224라고 되어있는데, 잘못된 것입니다.)첫번째 Conv 층96개의 11*11*3 사이즈의 필터로 영상을 처리합니다.stride = 4zero-padding 사용 X-&gt; 55*55*96 feature map 생성Activation func : ReluRelu 활성화 함수를 지나고 나면, 3*3 overlapping max pooling이 stride 2값으로 시행됩니다.그 결과 27*27*96 fearture map을 가지게 됩니다.local response normalization이 시행됩니다. 이는 수렴 속도를 가속화 해준다고 합니다.또한, 특성맵의 차원을 변화시키지 않아 27*27*96의 feature map이 유지된다고 합니다."
    } ,
  
    {
      "title"       : "[fd_22] 딥네트워크, 서로 뭐가 다른 거죠",
      "category"    : "",
      "tags"        : "",
      "url"         : "./fd_22-%EB%94%A5%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC,-%EC%84%9C%EB%A1%9C-%EB%AD%90%EA%B0%80-%EB%8B%A4%EB%A5%B8-%EA%B1%B0%EC%A3%A0.html",
      "date"        : "2022-02-04 00:00:00 +0900",
      "description" : "",
      "content"     : "해당 포스팅은, 아이펠 Fundamental 22번 노드를 학습하고 작성한 기록물 입니다.&lt; 목차 &gt; pre-trainded 모델이란? AlexNet vGG-16 1. pre-trained 모델이란?이미 배워왔듯, 신경망을 깊이 쌓는다고 해서 무조건 성능이 좋아지는 것은 아닙니다. 신경망을 깊게 쌓을 수록 overfitting문제와 학습될 parameter들이 기하급수적으로 증가하는 문제, 이와 더불어 연산속도와 학습속도에 오래걸리는 등 다양한 문제가 발생합니다.그렇기 때문에, 연구자들이 더 좋은 성능을 내는 딥러닝 신경망을 만들기 위해 다양한 방법을 시도하였고, 많은 네트워크들이 탄생했습니다. 다음은 그 중 몇가지 예시인 pre-trained 모델 리스트입니다.[Pre-trained 모델의 구조] 여기서 더 볼수있습니다 : https://github.com/tensorflow/models/tree/master/research/slim2. AlexNet2012년 Image Classification Task에서 Geoffrey Hinton 교수님이 이끄는 토론토 대학의 SuperVision팀이 오류율 16%로 1등을 달성합니다. 작년의 우승자는 오류율 26%, 제작년은 28%였으니 엄청난 차이를 보여준것입니다. superVision팀이 사용한 모델이 AlexNet입니다.AlexNet 구조기본 구조는 LeNet-5와 크게 다르지 않습니다. 2개의 GPU로 병렬연산을 수행하기 위해 병렬 구조로 설계되었다는 점이 가장 큰 변화입니다.총 8개의 레이어로 이루어져 있으며, 5개는 Conv 레이어, 3개는 FC 레이어로 구성되어 있습니다. (맨 왼쪽은 input data입니다.)2, 4, 5번째 컨볼루션 레이어들은 전 단계의 feature map과 연결되어 있지만, 3번째 레이어에서는 병렬처리로 나뉜 두 단계의 feature맵과 모두 연결되어 있는 것이 특징입니다.더 자세한 내용은 다음 링크에 설명이 되어있습니다. 링크용어 검색 overlapping max pooling local response normalization출처 AIFFEL LMS 문제시 연락 부탁드립니다. :) 참고사이트 https://bskyvision.com/421"
    } ,
  
    {
      "title"       : "220204 알고리즘 기록",
      "category"    : "",
      "tags"        : "",
      "url"         : "./220204-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EA%B8%B0%EB%A1%9D.html",
      "date"        : "2022-02-04 00:00:00 +0900",
      "description" : "",
      "content"     : "오늘은, 프로그래머스 문제를 풀면서 새로 배운 함수와 새로 배운 것을 정리해보았습니다.목차 datetime 패키지 숫자 판별함수 sort()와 sorted()의 차이1. datetime 패키지datetime 객체만들기import datetime# 2022년 2월 4일 12시 30분 30초 0 밀리세컨즈day2 = datetime.datetime(2022, 2, 4, 12, 30, 30, 0)day1 = datetime.datetime(2022,2,4) 년, 월, 일, 시, 분, 초, millis 혹은 년, 월, 일 넣어서 datetime 객체를 만들 수 있습니다.요일 가져오기days = ['MON','TUE','WED','THU','FRI','SAT','SUN']print(days[day1.weekday()])# 출력값 : FRI weekday()의 값이 int로 나오기 때문에 days와 같이 리스트를 선언해서 사용해주었습니다.2. 숫자 판별 함수isdigit(), isnumeric(), isdecimal()메소드는 string이 숫자로 이루어져있는지 검색하기 위해 사용됩니다. 같은 기능인데 함수가 3개나 존재한다는 것은 전부 다르다는 의미이므로, 오늘은 이 각각의 함수가 어떻게 다른지 살펴보았습니다.isdigit()문자열에 사용된 글자들이 ‘digit’이거나 특수문자로 작성된 숫자인지 확인 ( 분수는 불가)isnumeric()문자열에 사용된 글자들이 ‘숫자’로 해석될 수 있는지를 확인isdecimal()문자열에 사용된 글자들이 ‘digit’인지를 확인s1 = '123456's2 = '00123456's3 = '3²'s4 = '²'s5 = 'ⅲ's6 = '½'s7 = '3₂'s8 = '₂'s9 = 'log5'print('s1', s1.isdigit(), s1.isnumeric(), s1.isdecimal())print('s2', s2.isdigit(), s2.isnumeric(), s2.isdecimal())print('s3', s3.isdigit(), s3.isnumeric(), s3.isdecimal())print('s4', s4.isdigit(), s4.isnumeric(), s4.isdecimal())print('s5', s5.isdigit(), s5.isnumeric(), s5.isdecimal())print('s6', s6.isdigit(), s6.isnumeric(), s6.isdecimal())print('s7', s7.isdigit(), s7.isnumeric(), s7.isdecimal())print('s8', s8.isdigit(), s8.isnumeric(), s8.isdecimal())print('s9', s9.isdigit(), s9.isnumeric(), s9.isdecimal())실행결과 s1 True True Trues2 True True Trues3 True True Falses4 True True Falses5 False True Falses6 False True Falses7 True True Falses8 True True Falses9 False False False3. sort()와 sorted() 차이list.sort()list 원본을 정렬하고 수정한 후, None 을 리턴합니다.sorted(list)list 원본은 유지하고, 정렬한 새 리스트를 리턴합니다.참고 사이트 https://soooprmx.com/%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%9D%98-%EC%88%AB%EC%9E%90%ED%8C%90%EB%B3%84%ED%95%A8%EC%88%98/ https://blog.naver.com/PostView.nhn?isHttpsRedirect=true&amp;blogId=wideeyed&amp;logNo=221745416992&amp;redirect=Dlog&amp;widgetTypeCall=true&amp;directAccess=false"
    } ,
  
    {
      "title"       : "강화학습이란",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5%EC%9D%B4%EB%9E%80.html",
      "date"        : "2022-02-03 00:00:00 +0900",
      "description" : "",
      "content"     : "요즘 한창 열심히 하는 것 중 하나가 강화학습 스터디-N강 성공참여입니다. 무려 스터디장님이 열심히 해주셔서 감사하다는 말씀도 해주셔서 기분이 좋은 오늘, 영어강의의 이해 한계를 극복하고자 한국어 책을 보고 강화학습 이론을 정리해 보았습니다.참고한 책은 다음과 같습니다.밑바닥부터 시작하는 딥러닝 1딥러닝의 정석 : 텐서플로우와 최신 기법으로 배우는 딥러닝 알고리즘 설계'시작합니다.강화학습 개요강화학습의 주체에는 크게 에이전트와 환경이 있습니다.에이전트가 환경에 맞게 행동을 선택하면, 행동에 맞게 환경이 변하는 것이 기본적인 틀입니다. 환경이 변화하면 에이전트는 보상을 얻게 되는데, 강화학습의 목적은 더 나은 보상을 받도록 에이전트의 행동 강령을 세우는 것입니다.여기서 주의할 점은 보상은 정해진 것이 아니라 예상보상입니다. 보상은 어떤 환경이었는지에 따라 천차만별이기 때문입니다. 예를 들어 요리를 하다 불을 끈다.라는 행동의 보상은 요리를 다해서 불을 끈다, 후라이팬에 불이 붙어서 불을 끈다. 두 상황에 따라 보상이 천차만별일 것입니다. 그렇기 때문에, 이러한 불명확한 상황에서는 안전의 위험 혹은 요리의 완성과 같은 명확한 지표로부터 역산하여 예상 보상을 정해야 합니다."
    } ,
  
    {
      "title"       : "[ex_09] 폐렴아 기다려라",
      "category"    : "",
      "tags"        : "",
      "url"         : "./ex_09-%ED%8F%90%EB%A0%B4%EC%95%84-%EA%B8%B0%EB%8B%A4%EB%A0%A4%EB%9D%BC.html",
      "date"        : "2022-02-03 00:00:00 +0900",
      "description" : "",
      "content"     : "해당 포스팅은, 아이펠 Exploration 9번 노드를 학습하고 작성한 기록물 입니다.의료영상에 딥러닝을 접목시켜 분석하는 실습을 해보았으며, 사용한 데이터는 캐글의 폐렴환자 데이터입니다.Chest X-Ray Images (Pneumonia)https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia목차 의료 영상에 대한 기초 지식 쌓기 의료 영상 데이터 처리하기1. 의료영상에 대한 기초 지식 쌓기1-1. 의료 영상 분석과 일반적인 이미지 처리가 다른 점 의료 이미지는 개인정보문제가 있어 데이터를 구하는 것 자체가 쉽지 않음 라벨링 작업 자체가 의학적 전문 지식을 요하므로 데이터 셋 구축 비용이 비쌈 장기 판별 /수술 기구 판별 희귀질병의 경우, 데이터 입수 자체가 어려움 음성/양성 데이터간 imbalance가 심함 -&gt; 학습에 주의 필요 이미지만으로 진단은 어려움 -&gt; 다른 데이터와 결합하여 학습해야 하는 경우 有 즉, 딥러닝 영상처리 기술 + 의학적 도메인 지식 + 의료 영상에 대한 명확한 이해 필요 1-2. 의료 영상 종류X-ray전자를 물체에 충돌시킬 때 발생하는 투과력이 강한 복사선(전자기파)방사선의 일종방, 근육, 천, 종이같이 밀도가 낮은 것은 수월하게 통과하지만, 밀도가 높은 뼈, 금속 같은 물질은 잘 통과하지 못합니다.사진 출처 : http://health.cdc.go.kr/healthinfo/index.jspCTComputed Tomography환자를 중심으로 X-RAY를 빠르게 회전하여 3D 이미지를 만들어내는 영상3 차원 이미지를 형성하여 기본 구조와 종양 또는 이상을 쉽게 식별하고 위치를 파악가능신체의 단면 이미지는 slice라고 함Slice는 단층 촬영 이미지라고도 함기존의 X-RAY보다 더 자세한 정보를 포함http://www.nibib.nih.gov/science-education/science-topics/computed-tomography-cthttp://en.wikipedia.org/wiki/CT_scanMRIMagnetic Resonance Imaging(자기 공명 영상)신체의 해부학적 과정과 생리적 과정을 보기 위해 사용강한 자기장를 사용 -&gt; 신체 기관의 이미지 생성방사선을 사용X -&gt; CT나 X-ray보다는 안전http://en.wikipedia.org/wiki/Magnetic_resonance_imaging#:~:text=Magnetic%20resonance%20imaging%20(MRI)%20is,the%20organs%20in%20the%20body1-3. X-ray 이미지X-ray 자세 분류 체계https://ko.wikipedia.org/wiki/%ED%95%B4%EB%B6%80%ED%95%99_%EC%9A%A9%EC%96%B4 Sagittal plane : 시상면. 사람을 왼쪽과 오른쪽을 나누는 면. Coronal plane : 관상면. 인체를 앞뒤로 나누는 면. Transverse plane : 횡단면(수평면). 인체를 상하로 나누는 면.해부학적 위치https://ko.wikipedia.org/wiki/%ED%95%B4%EB%B6%80%ED%95%99_%EC%9A%A9%EC%96%B4영상을 볼 때는 보통 정면을 보고 있는 것으로 가정을 하며 위의 이미지에서 오른쪽에 해당합니다.또한 영상을 볼 때는, 환자중심으로 보기 때문에 오른쪽 얼굴 이라고하면 제가 환자를 마주보는 상황에서 왼쪽 얼굴을 보아야 합니다.흉부 X-rayhttp://health.cdc.go.kr/health/Resource/Module/Content/Printok.do?idx=2110&amp;subIdx=4 갈비뼈 : 하얀색 폐 : 검은색 어깨 쪽의 지방 및 근육 : 연한 회색1-4. 폐렴 진단해보기 사용한 데이터 셋Chest X-Ray Images (Pneumonia)https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia(에디터 주) 캐글에서 다운로드한 데이터는 chest_xray 하위에 chest_xray 폴더가 중복으로 포함되어 있어서 전체 데이터의 크기가 2.5GB인 경우가 있습니다. 중복된 데이터는 필요하지 않습니다.해당 이미지는 중국 광저우에 있는 광저우 여성 및 어린이 병원의 1~5 세 소아 환자의 흉부 X선 영상입니다.폐렴이란? 폐렴(pneumonia 뉴모니아)은 폐에 염증이 생긴 상태로 중증의 호흡기 감염병이다. 세균을 통한 감염이 가장 많으며, 바이러스, 균류, 또는 기타 미생물도 원인이 될 수가 있다. 드물게는 알레르기 반응이나 자극적인 화학 물질을 흡입해 발생하기도 한다. 노인이나 어린아이, 혹은 전체적으로 상태가 안 좋은 환자들이나 기침 반사가 약한 사람들에게는 흡인성 폐렴이 발생한다. 그리고 세균이 원인인 경우는 항생제로 치료를 할 수 있다. 항생제가 생기기 전에는 50~90%가 사망할 정도로 위험한 질환이었으나, 현재는 거의 사망하지 않는다. 1940년대에 항생제가 개발되기 전까지는 폐렴 환자의 1/3 정도가 사망하였다. 오늘날에는 적절한 의학적 치료로 폐렴 환자의 95% 이상이 회복된다. 그러나 일부 저개발국(개발 도상국)에서는 폐렴이 여전히 주요 사망 원인 중 하나이다.출처 : https://ko.wikipedia.org/wiki/폐렴 염증은 유해한 자극에 대한 생체반응 중 하나로 면역세포, 혈관, 염증 매개체들이 관여하는 보호반응이다. 염증의 목적은 세포의 손상을 초기 단계에서 억제하고, 상처 부분의 파괴된 조직 및 괴사된 세포를 제거하며, 동시에 조직을 재생하는 것이다.출처 : https://ko.wikipedia.org/wiki/염증정리하자면, 폐렴은 폐에 염증이 생기는 것이며, 염증은 백혈구들이 세균 등에 맞서 싸우고 있는 장소입니다.폐렴 구별법폐렴의 경우, X-ray 사진에서 다양한 양상의 음영이 보입니다. 깨끗해야 할 폐 부위에 희미한 그림자가 관찰되는 것입니다. 하지만, 실제 영상을 보면, 희미한 케이스가 많으므로 그 희미함이 폐렴에 의한 것인지, 다른 이유인지는 파악하기 어렵습니다.왼쪽은 정상, 중간은 세균성 폐렴, 오른쪽은 바이러스 폐렴입니다.세균성 폐렴은 일반적으로 왼쪽 상부 엽(하얀 화살표)에 하얀 부분이 관찰되며, 바이러스성 폐렴은 확산된 “interstitial(조직 사이에 있는)” 패턴으로 나타납니다.2. 의료 영상 데이터 처리하기실습 코드는 아래 URL에서 확인하실 수 있으며, 포스팅에는 제가 정확도를 향상시키기 위하여 사용한 아이디어 혹은 새로 배운점 위주로 작성하였습니다.2-1. inbalance한 데이터를 학습할 때 주의점inbalance한 데이터란?해당 코드글에서 보실 수 있겠지만, 캐글 폐렴 환자 데이터에서train dataCNN 모델의 경우, 데이터가 클래스 별 balance가 좋을 수록 train을 잘 합니다. 그렇기 때문에 imbalance 문제를 꼭 해결해 주어야합니다.test datatest 데이터는 평가 하기 위해서 사용하기 때문에 학습과 관련이 없어서 inbalance한 데이터 셋이어도 상관없습니다.validation datavalidation 데이터는 평가 하기 위해서 사용하기 때문에 학습과 관련이 없어서 inbalance한 데이터 셋이어도 상관없습니다.inbalance한 데이터 처리 방법Weight balancing 테크닉 사용Weight balancing 은 training set의 각 데이터에서 loss를 계산할 때 특정 클래스의 데이터에 더 큰 loss 값을 갖도록 가중치를 부여하는 방법입니다. Keras는 model.fit()을 호출할 때 파라미터로 넘기는 class_weight 에 이러한 클래스별 가중치를 세팅할 수 있도록 지원하고 있습니다.참고 링크딥러닝에서 클래스 불균형을 다루는 방법 : https://3months.tistory.com/4142-2. 새로 배운 코드train_filenames = tf.io.gfile.glob(TRAIN_PATH)print(f'Normal image path\\n{filenames[0]}')print(f'Pneumonia image path\\n{filenames[2000]}')train_list_ds = tf.data.Dataset.from_tensor_slices(train_filenames)val_list_ds = tf.data.Dataset.from_tensor_slices(val_filenames)# Train 데이터셋, validation 데이터셋 개수 확인을 해보겠습니다.TRAIN_IMG_COUNT = tf.data.experimental.cardinality(train_list_ds).numpy()print(f\"Training images count: {TRAIN_IMG_COUNT}\")VAL_IMG_COUNT = tf.data.experimental.cardinality(val_list_ds).numpy()print(f\"Validating images count: {VAL_IMG_COUNT}\")train_ds = train_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)val_ds = val_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)def prepare_for_training(ds, shuffle_buffer_size=1000): ds = ds.shuffle(buffer_size=shuffle_buffer_size) ds = ds.repeat() ds = ds.batch(BATCH_SIZE) ds = ds.prefetch(buffer_size=AUTOTUNE) return dstrain_ds = prepare_for_training(train_ds)val_ds = prepare_for_training(val_ds)def conv_block(filters): block = tf.keras.Sequential([ tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'), tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'), tf.keras.layers.BatchNormalization(), tf.keras.layers.MaxPool2D() ]) return blockdef augment(image,label): image = tf.image.random_flip_left_right(image) # 랜덤하게 좌우를 반전합니다. return image,labeldef prepare_for_training(ds, shuffle_buffer_size=1000): # augment 적용 부분이 배치처리 함수에 추가되었습니다. ds = ds.map( augment, # augment 함수 적용 num_parallel_calls=2 ) ds = ds.shuffle(buffer_size=shuffle_buffer_size) ds = ds.repeat() ds = ds.batch(BATCH_SIZE) ds = ds.prefetch(buffer_size=AUTOTUNE) return ds저 9-7에서 augment 함수 궁금한게.. 저 함수만 입력해도 훈련데이터가 늘어나는 건가요? 아니면 따로 데이터 합치는 코드를 써야하나요?이래저래 시도했을 때 제가 지금까지 내린 결론은 1) 데이터 양은 계속 동일한데, 2) random하게 flip해주는 걸 추가하고, 3) 덧붙여 repeat()이란 걸 붙여놓으면, 4) 데이터 개수가 무한개로 많아지는데 그 중에 flipped된 종류가 생긴다 인 것 같아요음… ㅋㅋ 일단 좌우로 flip하는 게 성능 향상에 도움을 준다는 의견이 있긴 해서 한번 따라보겠습니다…ㅋㅋ https://www.kaggle.com/c/rsna-pneumonia-detection-challenge/discussion/661302-3. Batch Normalization과 Dropout을 같이 써도 되는가? 안되는가?참고 논문 1.Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/viewer.html?pdfurl=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_CVPR_2019%2Fpapers%2FLi_Understanding_the_Disharmony_Between_Dropout_and_Batch_Normalization_by_Variance_CVPR_2019_paper.pdf&amp;clen=1272425&amp;chunk=true참고 논문 2.Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/viewer.html?pdfurl=https%3A%2F%2Farxiv.org%2Fpdf%2F1905.05928.pdf&amp;clen=856720&amp;chunk=true출처 AIFFEL LMS 문제시 연락 부탁드립니다. :)"
    } ,
  
    {
      "title"       : "[ex_09] 폐렴아 기다려라   성능개선",
      "category"    : "",
      "tags"        : "",
      "url"         : "./ex_09-%ED%8F%90%EB%A0%B4%EC%95%84-%EA%B8%B0%EB%8B%A4%EB%A0%A4%EB%9D%BC-%EC%84%B1%EB%8A%A5%EA%B0%9C%EC%84%A0.html",
      "date"        : "2022-02-03 00:00:00 +0900",
      "description" : "",
      "content"     : "해당 포스팅은, 아이펠 Exploration 9번 노드를 학습하고 작성한 기록물 입니다.의료영상에 딥러닝을 접목시켜 분석하는 실습을 해보았으며, 사용한 데이터는 캐글의 폐렴환자 데이터입니다.Chest X-Ray Images (Pneumonia)https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia목차 기존의 성능 성능 개선 계획 성능 개선 결과기존의 성능전처리모델 아키텍처성능 개선 계획전처리모델 아키텍처출처 AIFFEL LMS 문제시 연락 부탁드립니다. :)"
    } ,
  
    {
      "title"       : "[ex_08] 뉴스 요약봇 만들기",
      "category"    : "",
      "tags"        : "",
      "url"         : "./ex_08-%EB%89%B4%EC%8A%A4-%EC%9A%94%EC%95%BD%EB%B4%87-%EB%A7%8C%EB%93%A4%EA%B8%B0.html",
      "date"        : "2022-02-03 00:00:00 +0900",
      "description" : "",
      "content"     : "해당 포스팅은, 아이펠 Exploration 9번 노드를 학습하고 작성한 기록물 입니다.의료영상에 딥러닝을 접목시켜 분석하는 실습을 해보았으며, 사용한 데이터는 캐글의 폐렴환자 데이터입니다.Chest X-Ray Images (Pneumonia)https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia목차 의료 영상에 대한 기초 지식 쌓기 의료 영상 데이터 처리하기1. 의료영상에 대한 기초 지식 쌓기1-1. 의료 영상 분석과 일반적인 이미지 처리가 다른 점 의료 이미지는 개인정보문제가 있어 데이터를 구하는 것 자체가 쉽지 않음 라벨링 작업 자체가 의학적 전문 지식을 요하므로 데이터 셋 구축 비용이 비쌈 장기 판별 /수술 기구 판별 희귀질병의 경우, 데이터 입수 자체가 어려움 음성/양성 데이터간 imbalance가 심함 -&gt; 학습에 주의 필요 이미지만으로 진단은 어려움 -&gt; 다른 데이터와 결합하여 학습해야 하는 경우 有 즉, 딥러닝 영상처리 기술 + 의학적 도메인 지식 + 의료 영상에 대한 명확한 이해 필요 1-2. 의료 영상 종류X-ray전자를 물체에 충돌시킬 때 발생하는 투과력이 강한 복사선(전자기파)방사선의 일종방, 근육, 천, 종이같이 밀도가 낮은 것은 수월하게 통과하지만, 밀도가 높은 뼈, 금속 같은 물질은 잘 통과하지 못합니다.사진 출처 : http://health.cdc.go.kr/healthinfo/index.jspCTComputed Tomography환자를 중심으로 X-RAY를 빠르게 회전하여 3D 이미지를 만들어내는 영상3 차원 이미지를 형성하여 기본 구조와 종양 또는 이상을 쉽게 식별하고 위치를 파악가능신체의 단면 이미지는 slice라고 함Slice는 단층 촬영 이미지라고도 함기존의 X-RAY보다 더 자세한 정보를 포함http://www.nibib.nih.gov/science-education/science-topics/computed-tomography-cthttp://en.wikipedia.org/wiki/CT_scanMRIMagnetic Resonance Imaging(자기 공명 영상)신체의 해부학적 과정과 생리적 과정을 보기 위해 사용강한 자기장를 사용 -&gt; 신체 기관의 이미지 생성방사선을 사용X -&gt; CT나 X-ray보다는 안전http://en.wikipedia.org/wiki/Magnetic_resonance_imaging#:~:text=Magnetic%20resonance%20imaging%20(MRI)%20is,the%20organs%20in%20the%20body1-3. X-ray 이미지X-ray 자세 분류 체계https://ko.wikipedia.org/wiki/%ED%95%B4%EB%B6%80%ED%95%99_%EC%9A%A9%EC%96%B4 Sagittal plane : 시상면. 사람을 왼쪽과 오른쪽을 나누는 면. Coronal plane : 관상면. 인체를 앞뒤로 나누는 면. Transverse plane : 횡단면(수평면). 인체를 상하로 나누는 면.해부학적 위치https://ko.wikipedia.org/wiki/%ED%95%B4%EB%B6%80%ED%95%99_%EC%9A%A9%EC%96%B4영상을 볼 때는 보통 정면을 보고 있는 것으로 가정을 하며 위의 이미지에서 오른쪽에 해당합니다.또한 영상을 볼 때는, 환자중심으로 보기 때문에 오른쪽 얼굴 이라고하면 제가 환자를 마주보는 상황에서 왼쪽 얼굴을 보아야 합니다.흉부 X-rayhttp://health.cdc.go.kr/health/Resource/Module/Content/Printok.do?idx=2110&amp;subIdx=4 갈비뼈 : 하얀색 폐 : 검은색 어깨 쪽의 지방 및 근육 : 연한 회색1-4. 폐렴 진단해보기 사용한 데이터 셋Chest X-Ray Images (Pneumonia)https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia(에디터 주) 캐글에서 다운로드한 데이터는 chest_xray 하위에 chest_xray 폴더가 중복으로 포함되어 있어서 전체 데이터의 크기가 2.5GB인 경우가 있습니다. 중복된 데이터는 필요하지 않습니다.해당 이미지는 중국 광저우에 있는 광저우 여성 및 어린이 병원의 1~5 세 소아 환자의 흉부 X선 영상입니다.폐렴이란? 폐렴(pneumonia 뉴모니아)은 폐에 염증이 생긴 상태로 중증의 호흡기 감염병이다. 세균을 통한 감염이 가장 많으며, 바이러스, 균류, 또는 기타 미생물도 원인이 될 수가 있다. 드물게는 알레르기 반응이나 자극적인 화학 물질을 흡입해 발생하기도 한다. 노인이나 어린아이, 혹은 전체적으로 상태가 안 좋은 환자들이나 기침 반사가 약한 사람들에게는 흡인성 폐렴이 발생한다. 그리고 세균이 원인인 경우는 항생제로 치료를 할 수 있다. 항생제가 생기기 전에는 50~90%가 사망할 정도로 위험한 질환이었으나, 현재는 거의 사망하지 않는다. 1940년대에 항생제가 개발되기 전까지는 폐렴 환자의 1/3 정도가 사망하였다. 오늘날에는 적절한 의학적 치료로 폐렴 환자의 95% 이상이 회복된다. 그러나 일부 저개발국(개발 도상국)에서는 폐렴이 여전히 주요 사망 원인 중 하나이다.출처 : https://ko.wikipedia.org/wiki/폐렴 염증은 유해한 자극에 대한 생체반응 중 하나로 면역세포, 혈관, 염증 매개체들이 관여하는 보호반응이다. 염증의 목적은 세포의 손상을 초기 단계에서 억제하고, 상처 부분의 파괴된 조직 및 괴사된 세포를 제거하며, 동시에 조직을 재생하는 것이다.출처 : https://ko.wikipedia.org/wiki/염증정리하자면, 폐렴은 폐에 염증이 생기는 것이며, 염증은 백혈구들이 세균 등에 맞서 싸우고 있는 장소입니다.폐렴 구별법폐렴의 경우, X-ray 사진에서 다양한 양상의 음영이 보입니다. 깨끗해야 할 폐 부위에 희미한 그림자가 관찰되는 것입니다. 하지만, 실제 영상을 보면, 희미한 케이스가 많으므로 그 희미함이 폐렴에 의한 것인지, 다른 이유인지는 파악하기 어렵습니다.왼쪽은 정상, 중간은 세균성 폐렴, 오른쪽은 바이러스 폐렴입니다.세균성 폐렴은 일반적으로 왼쪽 상부 엽(하얀 화살표)에 하얀 부분이 관찰되며, 바이러스성 폐렴은 확산된 “interstitial(조직 사이에 있는)” 패턴으로 나타납니다.2. 의료 영상 데이터 처리하기실습 코드는 아래 URL에서 확인하실 수 있으며, 포스팅에는 제가 정확도를 향상시키기 위하여 사용한 아이디어 혹은 새로 배운점 위주로 작성하였습니다.2-1. inbalance한 데이터를 학습할 때 주의점inbalance한 데이터란?해당 코드글에서 보실 수 있겠지만, 캐글 폐렴 환자 데이터에서train dataCNN 모델의 경우, 데이터가 클래스 별 balance가 좋을 수록 train을 잘 합니다. 그렇기 때문에 imbalance 문제를 꼭 해결해 주어야합니다.test datatest 데이터는 평가 하기 위해서 사용하기 때문에 학습과 관련이 없어서 inbalance한 데이터 셋이어도 상관없습니다.validation datavalidation 데이터는 평가 하기 위해서 사용하기 때문에 학습과 관련이 없어서 inbalance한 데이터 셋이어도 상관없습니다.inbalance한 데이터 처리 방법Weight balancing 테크닉 사용Weight balancing 은 training set의 각 데이터에서 loss를 계산할 때 특정 클래스의 데이터에 더 큰 loss 값을 갖도록 가중치를 부여하는 방법입니다. Keras는 model.fit()을 호출할 때 파라미터로 넘기는 class_weight 에 이러한 클래스별 가중치를 세팅할 수 있도록 지원하고 있습니다.참고 링크딥러닝에서 클래스 불균형을 다루는 방법 : https://3months.tistory.com/4142-2. 새로 배운 코드train_filenames = tf.io.gfile.glob(TRAIN_PATH)print(f'Normal image path\\n{filenames[0]}')print(f'Pneumonia image path\\n{filenames[2000]}')train_list_ds = tf.data.Dataset.from_tensor_slices(train_filenames)val_list_ds = tf.data.Dataset.from_tensor_slices(val_filenames)# Train 데이터셋, validation 데이터셋 개수 확인을 해보겠습니다.TRAIN_IMG_COUNT = tf.data.experimental.cardinality(train_list_ds).numpy()print(f\"Training images count: {TRAIN_IMG_COUNT}\")VAL_IMG_COUNT = tf.data.experimental.cardinality(val_list_ds).numpy()print(f\"Validating images count: {VAL_IMG_COUNT}\")train_ds = train_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)val_ds = val_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)def prepare_for_training(ds, shuffle_buffer_size=1000): ds = ds.shuffle(buffer_size=shuffle_buffer_size) ds = ds.repeat() ds = ds.batch(BATCH_SIZE) ds = ds.prefetch(buffer_size=AUTOTUNE) return dstrain_ds = prepare_for_training(train_ds)val_ds = prepare_for_training(val_ds)def conv_block(filters): block = tf.keras.Sequential([ tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'), tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'), tf.keras.layers.BatchNormalization(), tf.keras.layers.MaxPool2D() ]) return blockdef augment(image,label): image = tf.image.random_flip_left_right(image) # 랜덤하게 좌우를 반전합니다. return image,labeldef prepare_for_training(ds, shuffle_buffer_size=1000): # augment 적용 부분이 배치처리 함수에 추가되었습니다. ds = ds.map( augment, # augment 함수 적용 num_parallel_calls=2 ) ds = ds.shuffle(buffer_size=shuffle_buffer_size) ds = ds.repeat() ds = ds.batch(BATCH_SIZE) ds = ds.prefetch(buffer_size=AUTOTUNE) return ds저 9-7에서 augment 함수 궁금한게.. 저 함수만 입력해도 훈련데이터가 늘어나는 건가요? 아니면 따로 데이터 합치는 코드를 써야하나요?이래저래 시도했을 때 제가 지금까지 내린 결론은 1) 데이터 양은 계속 동일한데, 2) random하게 flip해주는 걸 추가하고, 3) 덧붙여 repeat()이란 걸 붙여놓으면, 4) 데이터 개수가 무한개로 많아지는데 그 중에 flipped된 종류가 생긴다 인 것 같아요음… ㅋㅋ 일단 좌우로 flip하는 게 성능 향상에 도움을 준다는 의견이 있긴 해서 한번 따라보겠습니다…ㅋㅋ https://www.kaggle.com/c/rsna-pneumonia-detection-challenge/discussion/661302-3. Batch Normalization과 Dropout을 같이 써도 되는가? 안되는가?참고 논문 1.Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/viewer.html?pdfurl=https%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_CVPR_2019%2Fpapers%2FLi_Understanding_the_Disharmony_Between_Dropout_and_Batch_Normalization_by_Variance_CVPR_2019_paper.pdf&amp;clen=1272425&amp;chunk=true참고 논문 2.Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/viewer.html?pdfurl=https%3A%2F%2Farxiv.org%2Fpdf%2F1905.05928.pdf&amp;clen=856720&amp;chunk=true출처 AIFFEL LMS 문제시 연락 부탁드립니다. :)"
    } ,
  
    {
      "title"       : "[ex_07] 인물 사진 만들어보기",
      "category"    : "",
      "tags"        : "",
      "url"         : "./ex_07-%EC%9D%B8%EB%AC%BC-%EC%82%AC%EC%A7%84-%EB%A7%8C%EB%93%A4%EC%96%B4%EB%B3%B4%EA%B8%B0.html",
      "date"        : "2022-02-01 00:00:00 +0900",
      "description" : "",
      "content"     : "이번 글에서는 아이펠 Exploration node 7번, 인물 사진을 만들어보자출처 AIFFEL LMS 문제시 연락 부탁드립니다. :) 5"
    } ,
  
    {
      "title"       : "Cs231 ) 5강 convolution neural networks",
      "category"    : "",
      "tags"        : "",
      "url"         : "./CS231-)-5%EA%B0%95-Convolution-Neural-Networks.html",
      "date"        : "2022-02-01 00:00:00 +0900",
      "description" : "",
      "content"     : "이번 포스팅에서는 stanford university의 CS231 강의를 보고 이해한 내용을 정리해 보았습니다."
    } ,
  
    {
      "title"       : "Cs231 ) 4강 introduction to neural networks",
      "category"    : "",
      "tags"        : "",
      "url"         : "./CS231-)-4%EA%B0%95-Introduction-to-Neural-Networks.html",
      "date"        : "2022-02-01 00:00:00 +0900",
      "description" : "",
      "content"     : "이번 포스팅에서는 stanford university의 CS231 강의를 보고 이해한 내용을 정리해 보았습니다."
    } ,
  
    {
      "title"       : "Cs231 ) 3강 loss functions and optimization",
      "category"    : "",
      "tags"        : "",
      "url"         : "./CS231-)-3%EA%B0%95-Loss-functions-and-optimization.html",
      "date"        : "2022-02-01 00:00:00 +0900",
      "description" : "",
      "content"     : "이번 포스팅에서는 stanford university의 CS231 강의를 보고 이해한 내용을 정리해 보았습니다."
    } ,
  
    {
      "title"       : "Cs231 ) 2강 image classification",
      "category"    : "",
      "tags"        : "",
      "url"         : "./CS231-)-2%EA%B0%95-Image-Classification.html",
      "date"        : "2022-02-01 00:00:00 +0900",
      "description" : "",
      "content"     : "이번 포스팅에서는 stanford university의 CS231 강의를 보고 이해한 내용을 정리해 보았습니다."
    } ,
  
    {
      "title"       : "Aiffel 6주차 ) 잘 하고 있는건가",
      "category"    : "",
      "tags"        : "",
      "url"         : "./AIFFEL-6%EC%A3%BC%EC%B0%A8-)-%EC%9E%98-%ED%95%98%EA%B3%A0-%EC%9E%88%EB%8A%94%EA%B1%B4%EA%B0%80.html",
      "date"        : "2022-02-01 00:00:00 +0900",
      "description" : "",
      "content"     : "오늘은 황금같은 설날 당일.정말 황금보다 귀했던 것이 처리하고 싶은 작업이 정말 많았기 때문이다.아이펠을 시작하고 끝없는 욕심에 욕심을 부려왔다. 내가 배워야 할 것도 정말 많았고, 배우고 싶은것도 많았다. 매주 노드 프로젝트도 진행하고 있었고, 봐야 할 강의, 보고싶은 강의는 늘어갔다. 그저 시청으로 끝나지 않고 그 강의의 정리, 발표준비도 해야했다. 그리고 그 과정에서 알게되는 모든 지식들은 다 소중해서 내 발자취 글로 남기고 싶었다. 그렇게 할 일 리스트는 하나 둘 쌓여갔다. 어느새 잠깐의 쉼도 아까울정도로.오늘은 주말부터 설 당일 까지 4일째 연휴. 이제 오늘내일 하루남았다..집안일 하는 시간을 빼면 전부 컴퓨터 앞에 앉아 조사하고, 강의를 보며 계속 혼자서 공부했다. 본가도 가지않고서 혼자서.맘 놓고 논 적없이 많은 지식들을 습득했고 또 많은 지식들이 스쳐지나갓을텐데 그 지식들은 어디로 갔는지,, exploration node는 손도 댈 수 없었다. 어떻게 모델을 발전시켜야 할 지 어디서부터 손을 대야할지 모를 그 막막함에 멍 때리다가 ㅊㅅㅇ 퍼실님이 게더에 들어오셨다. 그리고 이 황금연휴에 공부하러 오신 수연님을 붙잡고 다짜고짜 고민을 토로했다. ㅋㅋㅋㅋ...프로젝트를 개선할 방법을 어디서 찾을 수 있을지를 고민하고 있고, 점점 exploration node가 막막해진다고 말씀드렸더니, 내가 이런 생각을 하고 있을지 몰랐단다. 내 exploration node들을 보고 있으면 고민을 많이 했다는 것이 느껴지고, 최대한 내 생각을 담아 프로젝트를 진행해보려 했다는 것이 보여서 노력하고 있다는 것을 느끼셨다고 한다. 이렇게 하면 많이 힘드시긴 하겠다. 라는 생각도 하셨대요 ㅋㅋㅋㅍㅎㅎㅎ 지금 생각해도 기분 좋은 말이다. 점차 난이도는 올라가는데, 그걸 해결할 지식은 점점 내 수준을 벗어나고 있다고 느꼈기 때문이다. 나름 고민했지만 너무 미약한 변화란 생각이 들어 좌절했던 순간들에 보상을 받은 기분이었다.물론 프로젝트를 개선할 방법에 대한 방법 의논도 해주셨지만 저 한마디가 나의 모든 막막함과 답답함을 해결해주었다. 내가 정말 힘들었던 것은 프로젝트 개선 계획이 서는게 아니라 내가 잘 하고 있다는 믿음, 정체되지 않고 나아가고 있는지가 불안했던것 같다.고민을 토로해보는게 정말 큰 힘이 되는 것같다. ㅅㅇ님 감사해요! 덕분에 또 앞으로 열심히 할 용기가 생겼습니다!"
    } ,
  
    {
      "title"       : "깃 블로그 ) 3. 생성된 블로그에 첫 포스팅하기",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%EA%B9%83-%EB%B8%94%EB%A1%9C%EA%B7%B8-)-3.-%EC%83%9D%EC%84%B1%EB%90%9C-%EB%B8%94%EB%A1%9C%EA%B7%B8%EC%97%90-%EC%B2%AB-%ED%8F%AC%EC%8A%A4%ED%8C%85%ED%95%98%EA%B8%B0.html",
      "date"        : "2022-01-29 00:00:00 +0900",
      "description" : "",
      "content"     : "깃허브 블로그 제작기 시리즈의 세번째 글입니다.https://devinlife.com/howto%20github%20pages/first-post/위 링크를 참고하여 따라하였음을 밝힙니다.3. 생성된 블로그에 첫 포스팅하기 축약 3-1 markdown을 이용해서 블로그 글 작성하기 3-2 _posts폴더에 글 등록 후, github로 push하기 3-1. markdown을 이용해서 블로그 글 작성하기마크다운이란, 텍스트 기반의 마크업 언어로, 쉽게 쓰고 읽을 수 있습니다.문법은 매우 간단하며, 구글에 마크다운 문법을 검색하면 아주 많은 사이트들이 친절하게 알려줄 것입니다.문법도 간단하지만, 작성하는 것도 어렵지 않습니다.메모장에서 마크다운 문법에 맞추어 작성한 뒤 확장자를 .md로 맞추어주기만 하면 됩니다.하지만, 보통 이방법은 불편하죠^^저는 Typora 프로그램을 보통 사용하는데, Anaconda의 Jupyter notebook이나 웹 마크다운 에디터 등, 다양한 마크다운 에디터가 존재합니다. 다양한 것을 사용해보시고, 편하신 것으로 고르시길 바랍니다.3-2. _posts 폴더에 글 등록 후, github로 push하기다운받으신 j참고 사이트 https://devinlife.com/howto%20github%20pages/new-blog-from-template/"
    } ,
  
    {
      "title"       : "리눅스 커널에서 깃허브 사용하기",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%EB%A6%AC%EB%88%85%EC%8A%A4-%EC%BB%A4%EB%84%90%EC%97%90%EC%84%9C-%EA%B9%83%ED%97%88%EB%B8%8C-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0.html",
      "date"        : "2022-01-28 00:00:00 +0900",
      "description" : "",
      "content"     : "노드복사러에게 남기는 글.공용 git이기 때문에, 여러분은 (토큰을 생성하지 않았을 시) 토큰생성 먼저 하시고, 3-3의 git clone을 이용하여 작성된 노드들을 올려주시면 됩니다. 그 이후에 추가적으로 글을 올리실 때에는 3-1과 3-2를 참고하여 config 파일작성하고, fetch , pull 진행해주신 다음 add commit push 명령어를 실행하시면 됩니다.해당 글은 혹시, 개인적으로 repository를 생성하시고 싶으신 분들을 위해 레포지토리를 아예 생성하는것 부터시작합니다. 참고되셨으면 좋겠네요.이 이상 문제 생길시, 카톡으로 문의바랍니다.리눅스 커널에서 깃허브를 처음 사용해보는 분들을 위해 작성해 보았습니다.해당 포스팅은 깃허브에 가입을 하신 상태를 전제로 진행합니다.리눅스 커널에서 깃허브를 사용하는 방법을 알려드리지만, 깃허브의 add / commit / push등의 용어에 대해서는 설명하지 않습니다.목차 깃허브 토큰 만들기 깃허브 토큰 생성 페이지로 이동하기 토큰 생성하기 로컬 Git과 Github Repository 연결하기 Github repository 생성하기 로컬 Git 생성하기 Github repository와 로컬 Git 연결하기 git config 작성하기 Github repository에 파일 push하기 add와 commit하기 git branch 생성 push하기 이미 생성된 repository에 추가로 commit하고 싶다면? git fetch와 pull git config 작성 git clone하기 0. 깃허브 토큰 만들기깃허브 토큰은, 로컬 git에서 깃허브로 commit 혹은 pull등의 작업을 할 때 권한을 허가받기 위하여 필요합니다.생성 방법은 아래에 설명되어 있으며, 토큰 정보는 단 한번만 보이므로 잘 저장해놓아야 합니다.1. 깃허브 토큰 생성 페이지로 이동하기요약 : 깃허브 profile page &gt; Settings &gt; Developers settings &gt; Personal access tokens &gt; Generate new tokensprofile page &gt; settingssettings &gt; developer settingsdeveloper settings &gt; Personal access tokens &gt; Generate new tokens2. 토큰 생성하기요약 token 설명 작성 토큰 사용기한 설정 토큰 권한 설정 토큰 복사 ( 가장 중요 ) Note : token에 대한 설명을 작성합니다. (skip 가능) Expiration : 토큰 사용기한. 저는 약 6개월이상, 넉넉히 custom하여 2022-07-15일로 설정해주었습니다. 권한 설정 권한이 많을 수록 편리합니다. 깃허브 사용에 익숙하시지 않으시다면, delete 권한은 주지마세요.제가 설정한 권한은 다음과 같습니다. 토큰 복사 ( 가장 중요 ) 사진에 가려진 부분은 생성된 토큰입니다. 현재밖에 볼 수 없으므로, 메모장등에 꼭 복사해둡니다. pull / push 할 때 항상 사용합니다. 1. 로컬 Git과 Github repository 연결하기&lt; 요약 &gt; Github repository 생성하기 로컬 Git 생성하기 Github repository와 로컬 Git 연결하기 git config 작성하기 Git branch 생성하기1. Github repository 생성하기요약 : github profile page &gt; + 버튼 클릭 &gt; New Repository &gt; repository 정보 작성하기 &gt; create repository 버튼 클릭github profile page &gt; + 버튼 &gt; New Repository&lt; 아래 그림 설명 &gt;① : repository이름 작성​ 후에 로컬 Git 폴더명과 동일해야 합니다. ​ 저는 this_IS_test라는 repository를 생성했습니다.② : repository에 대한 설명③ : repository의 Access 설정④ : repository 초기화 파일⑤ : 1, 2, 3, 4 설정이 끝난 후, 눌러주면 repository가 생성됩니다.​ 깃허브 초보자가 리눅스 커널에서 git을 사용하려 할 때, 초기화 파일을 추가 하지 않는 것을 추천드립니다.​ 로컬 git에서 깃허브로 push할 때, 로컬 git의 상태가 깃허브의 repository 최신상태와 동일해야 합니다. (fetch &amp; pull한 상태 / 혹은 git clone한 직후.) 깃허브 repository 의 최신 상태와 동일하지 않으면 git push 명령어가 먹히지 않으므로, 초보자에게는 repository의 초기화파일을 만들지 않는 것을 추천드립니다. repository의 상태가 변경되기 때문입니다.초기화 파일을 선택하지 않았을 시, 완료 화면2. 로컬 Git 생성하기요약 리눅스에서 repository명과 동일한 폴더 만들기 git init 리눅스에서 repository명과 동일한 폴더 만들기 대소문자 구분해야합니다. 일부로 이름을 this_IS_test이렇게 한 것은, 대소문자 구분을 해주어야 함을 보여주기 위함입니다. 다음은 리눅스에서 this_IS_test 디렉토리를 만든 화면입니다. 실행화면 리눅스에서 사용한 명령어 $ mkdir 폴더명 ​ 이때 폴더명은 repository와 동일한 이름이어야 합니다.  git init 이 폴더를 Git으로 사용하겠다는 설정으로, 생성한 로컬 git 폴더로 이동하여 git init명령어를 사용합니다. 실행화면 리눅스에서 사용한 명령어 $ cd 폴더명 $ git init 3. Github repository와 로컬 Git 연결하기요약 git remote add origin https://github.com/Nega0619/this_IS_test.git git remote add origin https://github.com/Nega0619/this_IS_test.git 로컬 git 폴더 내에서 위 명령어를 실행해줍니다. 이때 깃허브 repository 주소는 사진에서 참고하시면 됩니다. ① : 깃허브의 repository로 이동한 상태에서 URL 복사+.git ② : 초기 생성화면에서 복사 버튼 클릭하면 됩니다. 실행화면 리눅스에서 사용한 명령어 (이미 로컬 깃내에 존재한다는 전제하에) $ git remote add origin https://github.com/Nega0619/this_IS_test.git 4. git config 작성하기요약 git config –global user.email 설정 git config –global user.name 설정 git config 파일 확인하기이미 config 작성을 하신적이 있다면 git config -l 로 설정이 되어있는지 확인하세요. git config –global user.email 설정 “여기안에 이메일” 써야합니다. $ git config –global user.email “h0riya0619@gmail.com” git config –global user.name 설정 “여기안에 name” 써야합니다. $ git config –global user.name “Nega0619” git config 파일 확인하기 $ git config -l 실행화면 2. repository에 파일 push하기&lt; 요약 &gt; add와 commit하기 git branch 생성 push하기 1. add와 commit하기 파일 생성하기로컬 깃에서 repository로 push하기 위해선 파일이 잇어야 합니다. 저는 다음 명령어로 파일을 만들어 주었습니다. ( 이미 올릴 파일이 있으면 skip ) $ cat &gt; test.txt this is test여기까지 작성하고 ctrl+z 입력하면 test.txt에 this is test 라는 파일이 생성됩니다.실행화면 add 깃에 존재하는 전체 파일을 github에 올리려면 *을 사용하면 됩니다. $ git add * commit commit 명령어에서 -m 옵션은 commit 메시지를 설정하는 옵션입니다. 아래 명령어는 깃으로 commit할때 first commit 이란 메시지로 commit하겠다는 뜻입니다. $ git commit -m “first commit” git commit 상태 확인하기 현재 add / commit할 파일들에 대한 정보를 보여줍니다. $ git status 실행화면2. git branch 생성main 이라는 이름의 branch를 생성합니다. $ git branch -M main실행화면3. push하기main이란 branch에 git을 push 합니다. $ git push -u origin main Username for ‘https://github.com’: 이메일넣기 Password for ‘https://h0riya0619@gmail.com@github.com’: 토큰넣기유의사항 이메일은 “” 등을 사용하지 않고 바로 작성합니다. 리눅스 환경의 경우, 비밀번호 입력에는 아무것도 보이지 않습니다. 당황하지 마시고 바로 입력하세요 리눅스 붙여넣기 명령어 : ctrl + shift + v 실행화면3. 이미 생성된 repository에 추가로 commit하고 싶다면?처음 repository를 생성하고 파일을 올리는 것과 다르게, 이미 사용하고 있는 repository에 로컬 git을 만들어 파일을 올리고 싶은 경우, add / commit / push가 안되는 경우가 있습니다.이런 경우, 2가지를 확인해 보면됩니다.1. git config 파일 확인하기리눅스 커널에서 git config 파일이 초기화 된경우가 있습니다.이럴 경우, git config -l명령어로 config 설정을 먼저 확인해 본 후, 1-4 처럼 user.email과 user.name을 설정을 해주면 됩니다.2. git fetch / git pull 명령어 사용하기git의 add / commit / push 오류가 나는 경우, 로컬 git의 상태가 git repository 상태와 다른 경우가 많습니다. 이럴때는 fetch와 pull을 해준 후, add / commit / push를 해주면 해결됩니다.명령어 : $ git fetch $ git pullfetch와 pull 후, push처럼 이메일과 토큰을 작성해주면 됩니다.실행화면### 3. git clone하기1, 2로도 해결이안된다 혹은 로컬 깃을 만들지 않은 상태라면, git clone 명령어를 이용해 이미 존재하는 repository를 최신 상태로 로컬에 가져올 수 있습니다.준비물 : repository URL명령어 $ git clone 깃허브RepositoryURL.git실행화면 : 기존에 있던 this_IS_test 로컬 깃을 삭제한 후, clone을 해보았습니다.만약 안된다면 config 설정은 해주었는지 확인해보세요~!아이펠 복사러들에게 git clone 명령어 선물 git clone https://github.com/Nega0619/Aiffel_nodes.git실행화면"
    } ,
  
    {
      "title"       : "머신러닝 ) 다변량회귀분석 multivariate regression",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-)-%EB%8B%A4%EB%B3%80%EB%9F%89%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D-Multivariate-regression.html",
      "date"        : "2022-01-24 00:00:00 +0900",
      "description" : "",
      "content"     : "fundamental 노드 19번을 공부할 때, 아이펠에서 나왔던 질문을 바탕으로 공부했던 것을 작성해보고자 합니다.출처 https://techblog-history-younghunjo1.tistory.com/118"
    } ,
  
    {
      "title"       : "깃 블로그 ) 2. jekyll 테마 적용하기",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%EA%B9%83-%EB%B8%94%EB%A1%9C%EA%B7%B8-)-2.-Jekyll-%ED%85%8C%EB%A7%88-%EC%A0%81%EC%9A%A9%ED%95%98%EA%B8%B0.html",
      "date"        : "2022-01-24 00:00:00 +0900",
      "description" : "",
      "content"     : "깃허브 블로그 제작기 시리즈의 두번째 글입니다.https://devinlife.com/howto%20github%20pages/new-blog-from-template/위 링크를 참고하여 따라하였음을 밝힙니다.이 링크를 따라하기 전에, github에 가입을 해주세요. 거기서 여유가 된다면 git의 add commit push pull의 동작방식에 관하여 공부하는 것을 추천드립니다.2. jekyll 테마 적용하기 축약 2-1. jekyll 테마 선택하기 2-2. minimal mistakes 다운로드 2-3. username.github.io 주소로 웹 호스팅하기 2-4. 확인하기 2-1. jekyll 테마 선택하기jekyll 테마는 다른 사람들이 이미 만들어 놓은 테마로, tistory처럼 원하는 템플릿을 골라서 적용하면, 바로 사용할 수 있습니다. 많은 테마가 무료로 오픈되어 있으며, 아래 사이트에서 고르실 수 있습니다. http://jekyllthemes.org/ http://themes.jekyllrc.org/ https://jekyllthemes.io/전 Adam Blog ver2.0를 선택했습니다.http://jekyllthemes.org/themes/adam-blog-2/2-2. Lokmont 다운로드Homepage를 클릭하여 테마의 github로 이동합니다.clone 혹은 download를 합니다.저는 clone을 택했습니다.1번 글에서, blog를 생성한 폴더 디렉토리에서 해당 테마를 clone 해줍니다.명령어는 다음과 같습니다. git clone 테마의깃허브URL주소.gitclone뒤의 주소를 어떻게 넣어야 하는지 모르겠다면, github 테마 블로그 페이지에서 URL을 복사한 후, .git만 붙여주면 됩니다! 실행화면 bundle 명령어 수행git clone으로 생겼을 레포지토리 폴더로 이동하여, bundle 명령어를 수행합니다.저의 경우 the-mvm.github.io입니다. cd the-mvm.github.io bundle 실행화면 기타 설명 만약, 어떤 디렉토리가 생겼는지 모르겠다면 dir명령어를 이용하여 해당 폴더에 어떤 파일들이 있는지 살펴보면 됩니다. 저의 경우는 이렇습니다. the-mvm.github.io가 보이네요!![image-20220204214814374](../images/pages/image-20220204214814374.png) 테마 적용되었는지 확인하기clone한 레포지토리 폴더에서 bundle exec jekyll serve를 실행하면 서버가 실행됩니다. 명령어는 다음과 같습니다. (cd the-mvm.github.io&amp; bundle 명령어만 사용한 경우) bundle exec jekyll serve 실행화면 (서버 실행화면) (localhost:4000 주소로 접속시) 오류화면 처음에 기존 블로그 생성 폴더에서 bundle exec jekyll serve 명령어를 실행해야 하는 줄 알고, 기존 폴더로 이동하여 실행했더니 다음 오류가 떴습니다. (원래 lokmont라는 테마로 진행했어서 파일경로가 다를 수있음) webrick 파일이 없다는 오류가 뜨면 bundle add webrick로 해결! 2-3. username.github.io 주소로 웹 호스팅하기github page용 repository 생성하기여기까지 완성한 블로그를 github pages에 연동시키기 위해서는 git에서 repository를 생성해야 합니다.repository명은 고정입니다 : 사용자ID.github.io여기서 사용자ID는 이메일 아이디가 아니라, github에서 사용하는 닉네임 즉, repository를 생성할 때, 옆에 뜨는 아이디를 똑같이 입력하시면 됩니다. 사진에는 user.name이 Nega0619이지만, url이니만큼 후에 대문자-&gt; 소문자로 바꾸어주었고, 정상동작하였습니다.유념사항 github 무료계정이라면 public repository만 생성가능 repository를 생성할때 선택하는 초기화 파일들은 전부 선택 해제 github repository와 생성한 블로그 연동시키기 테마 폴더 이름을 github repository이름과 일치시키기 방법 1. rename 기존폴더명 새로운폴더명 방법 2. 윈도우에서 폴더를 직접 켜서, 해당 폴더를 이름 변경하기 기존 origin 정보 삭제하기 이 명령어는 새로운 수정한 폴더에서 수정해야합니다. cd Nega0619.github.io git remote remove origin 자신의 origin 정보 추가 git remote add origin https://github.com/Nega0619/Nega0619.github.io.git ​ 자신이 생성한 깃허브 블로그 repository URL + .git 을 입력하면 됩니다. .git 잊지마세요! git push저는 commit &amp; push 전에 main이라는 branch를 생성해주었습니다. git branch -M main git push -u origin main 실행화면 도중에 다음과 같은 창이 뜨면 그냥 로그인 해주시면 됩니다. ( 안뜰수도있음 ) 실행 완료 화면 발생 했던 오류 사항 급한 성미의 필자의 경우, 만들고, 서버켜고 이것 저것 다해보았지만 첫날은 404 오류가 떴었습니다. 아마 블로그가 포스팅되는데 오래걸리는 듯 싶습니다. 여유를 가지시고 ^^/ 호스팅한지얼마안됫을경우 뜰수잇음 필자의 경우, user.name이 Nega0619라서, 대문자 -&gt; 소문자로 바꾸어주었습니다. 404가 뜨는 경우가 있었는데 대문자를 소문자로 바꿔주었습니다.2-4. 확인하기인터넷 창을 켜고 username.github.io로 들어가시면 자신이 적용한 블로그가 뜰 것입니다.저의 경우라면 nega0619.github.io겠죠?하지만, 만들고 직후에 들어가면 다음과 같이 404 페이지를 마주할 수도 있습니다.원래 github에 블로그를 만들고, 배포되기까지 빠르면 10분정도의 시간이 소요된다고 합니다. 빠르면 2분 :)조급하게 생각하지 마시고, 잘 만들어 졌는지 바로 확인하고 싶으시다면, github 홈페이지의 본인의 블로그 repository로 들어가 보세요.① : setting 메뉴를 누르면 다음과 같이 뜰겁니다.② : 다음과 같이 파란 메시지창이 뜨면, 설정이 잘 되었다는 뜻이니 조금 기다리시면 블로그가 배포될 것입니다.참고 사이트 https://devinlife.com/howto%20github%20pages/new-blog-from-template/. https://habitual-history.tistory.com/entry/github-%ED%99%88%ED%8E%98%EC%9D%B4%EC%A7%80-%EB%A7%8C%EB%93%A4%EA%B8%B0-github-pages-%EC%9B%B9-%ED%98%B8%EC%8A%A4%ED%8C%85"
    } ,
  
    {
      "title"       : "깃 블로그 ) 1. github pages 준비하기",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%EA%B9%83-%EB%B8%94%EB%A1%9C%EA%B7%B8-)-1.-github-pages-%EC%A4%80%EB%B9%84%ED%95%98%EA%B8%B0.html",
      "date"        : "2022-01-24 00:00:00 +0900",
      "description" : "",
      "content"     : "깃허브 블로그 제작기 시리즈의 첫번째 글입니다.https://devinlife.com/howto%20github%20pages/github-prepare/위 링크를 참고하여 따라하였음을 밝힙니다.1. github pages 준비하기 github pages 준비하기 축약 1-1 Ruby 설치하기 1-2 Jekyll과 bundler 설치하기 1-3 샘플 블로그 생성하기 1-1. Ruby 설치하기 Ruby를 먼저 설치해줍니다. 설치링크 - https://rubyinstaller.org/downloads/%E3%84%B4 저는 노란색으로 칠해진 버전을 다운받았고, with DEVKIT로 다운받았습니다. 설치화면 Accept license 원하는 파일 경로를 지정해줍니다. 저는 D드라이브에 설치해주었습니다. 제 경험상 Path 관련한 체크사항은 꼭! 체크를 해주고 넘어가야합니다. MSYS2 Development도 설치해줍니다. Ruby 설치 완료 MSYS2 Development toolchain설치 4번 설치를 완료하고 나면, 자동으로 뜹니다. Enter를 한번 눌러주면 설치가 진행됩니다. 처음화면 중간화면 완료화면 캡쳐를 못했는데, 처음화면과 같이 Enter를 입력하라는 칸이 더 뜹니다. 이때, 엔터를 한번 더 쳐주면, 끝나게 됩니다. 1-2. Jekyll과 bundler 설치하기윈도우 검색창에 Start Command Prompt with Ruby를 검색 후, 실행하면 다음과 같이 창이 뜹니다.다음 명령어를 사용하여 jekyll bundler를 설치합니다. gem install jekyll bundler 설치화면 완료화면 1-3. 샘플 블로그 생성하기 블로그 폴더 생성 새로운 블로그를 생성하려면 다음과 같이 명령어를 입력합니다. testBlog라는 블로그 폴더를 생성합니다. jekyll new testBlog 만약, D드라이브에 생성하고 싶다면, 명령어에 D: 를 입력한 후, cd 명령어로 원하는 디렉토리로 이동 한 후 똑같이 따라하시면 됩니다. 생성한 블로그 폴더로 이동 생성된 블로그 폴더로 이동합니다. cd testBlog 블로그 폴더에서 서버 실행 다음 명령어를 이용하여 jekyll서버를 내 컴퓨터에서 실행합니다. bundle exec jekyll serve 3-1. 정상 실행 화면 Server Running 인 상태로 창을 끄지않고 그대로 둔 후 인터넷 창에 localhost:4000으로 접속하면 다음과 같이 뜹니다. 빈 블로그 만들기가 성공했습니다! 3-2. 오류화면 ​ bundle exec jekyll serve로 로컬에서 jekyll 서버를 구동하려고 하면 webrick을 로드하지 못했다는 오류가 발생할 수 있습니다. ​ Error 명 : Cannot load such file – webrick (LoadError) Ruby 3.x 이상에서는 webrick파일이 자동으로 포함되지 않아 발생하는 문제입니다. 다음 명령어를 통해서 이를 해결할 수있습니다. bundle add webrick 참고사이트 https://devinlife.com/howto/ https://ogaeng.com/jekyll-blog-install/ https://junho85.pe.kr/1850"
    } ,
  
    {
      "title"       : "깃 블로그 ) 0. ruby와 jeykll을 이용한 깃허브 블로그 git pages 시작하기 ",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%EA%B9%83-%EB%B8%94%EB%A1%9C%EA%B7%B8-)-0.-Ruby%EC%99%80-Jeykll%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EA%B9%83%ED%97%88%EB%B8%8C-%EB%B8%94%EB%A1%9C%EA%B7%B8-git-pages-%EC%8B%9C%EC%9E%91%ED%95%98%EA%B8%B0.html",
      "date"        : "2022-01-24 00:00:00 +0900",
      "description" : "",
      "content"     : "개발자에게는 기술 블로그가 하나쯤은 있어야 한다고 생각합니다. 블로그 자체가 나라는 개발자를 알려주는 정체성이자, 자신이 한 프로젝트와 공부들 그리고 발전하고자 하는 방향을 알려주는 포트폴리오가 되어준다고 생각하기 때문입니다.처음에는 큰 꿈을 꿔서 스프링을 배우면서 Docker와 AWS를 이용하여 서버를 열고 블로그를 운영 해볼까도 생각했습니다. 단순히 온전한 웹 프로젝트를 해보고싶어서였습니다. 하지만 이 방법은 공부를 하는 와중에 블로그를 만들어야 하기도 하고, 블로그를 만드는 동안 포스팅이 불가능해서 일단 다른 방법을 택했습니다.Velog와 네이버 블로그, Tistory등을 무작정 사용해보면서 점차 제가 원하는 블로그는 무엇인지 요구사항이 명확해져갔습니다. 제 요구사항은 다음과 같습니다. 마크 다운으로 작성된 글 그 자체가 바로 올라가야한다. Typora라는 마크다운 프로그램을 사용하여 글을 작성하는데, 여기서 작성한 글 형태를 드대로 사용해야 두번 작업을 하지 않습니다. Typora에서 작성되는 문법이 대체로 다 적용되어야 한다. Velog는 줄바꿈 시 전부 &lt;br&gt;태그를 다 넣어줘야하고, Tistory는 그정도는 아닌데, 가끔 문법이 씹혀서, 예쁘게 문단을 나눠놓아도 글이 한 뭉탱이로 뭉쳐진 경우를 보았습니다. 그래도 Velog보다는 Tistory가 나아서, 더 오래 사용하였지만, 수식 적용이 안되는 점을 발견하였고, markdown 작성모드가 아직 안정화가 덜되서 그런지 이미지 첨부가 되지 않는 경우가 있었습니다. 네이버 / 구글 검색시 블로그가 노출되어야 한다. 이런 요구사항의 결과, github page로 블로그를 작성해보려 합니다.실은 한 3년 전에 시도를 했었지만, 당시는 실패했습니다. 그저 글을 보고 따라하는데도 Error가 많이 발생해서 그냥,, 바로 포기했던 기억이있습니다. 하지만 이젠 Error 대응에도 조금 자신감이 생겼고! 제 블로그 목표가 튜토리얼 단계일텐데도 발생하는 여러 문제들에 굴복하지 않고! 대응해나가며 성장하는 것이기 때문에 이번엔 포기하지 않을겁니다!&lt; 깃허브 블로그 제작기 &gt;깃허브 블로그가 뭔지 모르겠고, 동작 환경이 어떻게 되는지 궁금하신 분은, 다음 링크를 참고해주세요!https://devinlife.com/howto%20github%20pages/github-blog-intro/이 글은 윈도우 10환경에서 2022-01-24에 제작한 제작기입니다.해당 글은 이 링크를 참고하여 작성하였습니다.https://devinlife.com/howto/1. github pages 준비하기2. jekyll 테마 적용하기3. 생성된 블로그에 첫 포스팅하기"
    } ,
  
    {
      "title"       : "깃 블로그 ) ",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%EA%B9%83-%EB%B8%94%EB%A1%9C%EA%B7%B8-).html",
      "date"        : "2022-01-24 00:00:00 +0900",
      "description" : "",
      "content"     : "깃허브 블로그 제작기 시리즈의 두번째 글입니다.https://devinlife.com/howto%20github%20pages/new-blog-from-template/위 링크를 참고하여 따라하였음을 밝힙니다.2. jekyll 테마 적용하기 축약 2-1. jekyll 테마 선택하기 2-2. github에서 하는 준비사항 2-3. minimal mistakes 다운로드 2-4. 참고 사이트 https://devinlife.com/howto%20github%20pages/new-blog-from-template/"
    } ,
  
    {
      "title"       : "[fd_19] 선형회귀와 로지스틱 회귀 linear regression and logistic regression",
      "category"    : "",
      "tags"        : "",
      "url"         : "./fd_19-%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80%EC%99%80-%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1-%ED%9A%8C%EA%B7%80-Linear-Regression-and-Logistic-Regression.html",
      "date"        : "2022-01-24 00:00:00 +0900",
      "description" : "",
      "content"     : "이 포스팅은, 아이펠 Fundamental 19번 노드를 학습하고 작성한 기록물 입니다.1. 회귀 분석이란?통계학에서 전통적으로 많이 사용되는 분석 방법입니다. 관찰된 여러 데이터를 바탕으로 연속형 변수(실수와 같은) 간의 관계를 모델링하고, 그에 대하여 적합도를 측정하는 방법입니다.예를 들어, 부모의 신장과 자식 신장의 관계 / 1인당 국민 총 소득과 배기가스 배출량 사이의 관계 등을 분석할 때 사용합니다.이 예시는 모두 독립변수 independant variable와 종속변수 explanatory variable 사이의 상호 관련성을 규명합니다.- 독립 변수 = 설명 변수 explanatory variable- 종속 변수 = 반응 변수 response variable출처 AIFFEL LMS 문제시 연락 부탁드립니다. :) 5"
    } ,
  
    {
      "title"       : "Cs231 5강. cnn",
      "category"    : "",
      "tags"        : "",
      "url"         : "./CS231-5%EA%B0%95.-CNN.html",
      "date"        : "2022-01-24 00:00:00 +0900",
      "description" : "",
      "content"     : "Mirror paddinghttps://joungheekim.github.io/2020/09/28/paper-review/https://stackoverflow.com/questions/42317238/why-do-we-use-fully-connected-layer-at-the-end-of-cnnhttps://www.cs.ryerson.ca/~aharley/vis/conv/"
    } ,
  
    {
      "title"       : "동적 웹 페이지와 정적 웹 페이지",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%EB%8F%99%EC%A0%81-%EC%9B%B9-%ED%8E%98%EC%9D%B4%EC%A7%80%EC%99%80-%EC%A0%95%EC%A0%81-%EC%9B%B9-%ED%8E%98%EC%9D%B4%EC%A7%80.html",
      "date"        : "2022-01-23 00:00:00 +0900",
      "description" : "",
      "content"     : "https://titus94.tistory.com/4 예시https://hogni.tistory.com/75"
    } ,
  
    {
      "title"       : "Ai ) 그리드 탐색과 랜덤 탐색 greed search , random search",
      "category"    : "",
      "tags"        : "",
      "url"         : "./AI-)-%EA%B7%B8%EB%A6%AC%EB%93%9C-%ED%83%90%EC%83%89%EA%B3%BC-%EB%9E%9C%EB%8D%A4-%ED%83%90%EC%83%89-Greed-Search-,-Random-Search.html",
      "date"        : "2022-01-23 00:00:00 +0900",
      "description" : "",
      "content"     : ""
    } ,
  
    {
      "title"       : "Ai ) 결정계수란",
      "category"    : "",
      "tags"        : "",
      "url"         : "./AI-)-%EA%B2%B0%EC%A0%95%EA%B3%84%EC%88%98%EB%9E%80.html",
      "date"        : "2022-01-23 00:00:00 +0900",
      "description" : "",
      "content"     : "https://ltlkodae.tistory.com/19"
    } ,
  
    {
      "title"       : "Ai ) cnn과 dense에서 weight수",
      "category"    : "",
      "tags"        : "",
      "url"         : "./AI-)-CNN%EA%B3%BC-Dense%EC%97%90%EC%84%9C-weight%EC%88%98.html",
      "date"        : "2022-01-22 00:00:00 +0900",
      "description" : "",
      "content"     : "이번 포스팅에서는 keras 에서 Dense와 Conv2D메소드의 매개변수 Arguments를 살펴보고, 이에 따른 parameter의 개수는 어떻게 되는지 살펴보도록 하겠습니다. 아래 모델을 이용하여 parameter 개수를 셀 것입니다.tf.keras.layers.Conv2D1. Conv2D 층의 매개변수tf.keras.layers.Conv2D( filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), groups=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs) 인수   filters 정수, 출력 공간의 차원(즉, 컨볼루션의 출력 필터 수). kernel_size 2D 컨볼루션 창의 높이와 너비를 지정하는 정수 또는 2개의 정수로 구성된 튜플/목록입니다. 모든 공간 차원에 대해 동일한 값을 지정하는 단일 정수일 수 있습니다. strides 높이와 너비를 따라 컨볼루션의 보폭을 지정하는 정수 또는 2개의 정수로 구성된 튜플/목록입니다. 모든 공간 차원에 대해 동일한 값을 지정하는 단일 정수일 수 있습니다. 보폭 값 != 1을 지정하는 것은 값 != 1을 지정하는 것과 호환되지 않습니다. dilation_rate padding \"valid\"또는 \"same\"(대소문자 구분 안 함) 중 하나입니다 . \"valid\"패딩 없음을 의미합니다. \"same\"결과적으로 입력의 왼쪽/오른쪽 또는 위/아래에 균일하게 0이 채워집니다. padding=\"same\"및 strides=1인 경우 출력은 입력과 동일한 크기를 갖습니다 . data_format ( 기본값 ) 또는 . 입력에서 차원의 순서입니다. 모양이 있는 입력에 해당하고 모양이 있는 입력 에 해당합니다 . 기본적으로 Keras 구성 파일의 . 설정하지 않으면 입니다 . channels_lastchannels_firstchannels_last(batch_size, height, width, channels)channels_first(batch_size, channels,height, width)image_data_format~/.keras/keras.jsonchannels_last dilation_rate 확장된 컨볼루션에 사용할 확장률을 지정하는 정수 또는 튜플/2개의 정수 목록입니다. 모든 공간 차원에 대해 동일한 값을 지정하는 단일 정수일 수 있습니다. 현재 != 1 값을 지정하는 것은 stride 값 != 1을 지정하는 것과 호환되지 않습니다. dilation_rate groups 입력이 채널 축을 따라 분할되는 그룹 수를 지정하는 양의 정수입니다. 각 그룹은 필터로 개별적으로 컨볼루션됩니다. 출력은 채널 축을 따라 모든 결과를 연결한 것입니다. 입력 채널이며 둘 다 로 나눌 수 있어야 합니다 . filters / groupsgroupsfiltersgroups activation 사용할 활성화 기능입니다. 아무 것도 지정하지 않으면 활성화가 적용되지 않습니다( 참조 keras.activations). use_bias 부울, 레이어에서 바이어스 벡터를 사용하는지 여부. kernel_initializer kernel가중치 행렬 의 이니셜라이저 ( 참조 keras.initializers). 기본값은 ‘glorot_uniform’입니다. bias_initializer 바이어스 벡터에 대한 이니셜라이저( 참조 keras.initializers). 기본값은 ‘0’입니다. kernel_regularizer kernel가중치 행렬 에 적용된 정규화 함수 ( 참조 keras.regularizers). bias_regularizer 편향 벡터에 적용된 정규화 함수( 참조 keras.regularizers). activity_regularizer 레이어의 출력에 적용된 정규화 기능(“활성화”)( 참조 keras.regularizers). kernel_constraint 커널 행렬에 적용되는 제약 조건 함수( 참조 keras.constraints). bias_constraint 바이어스 벡터에 적용되는 제약 조건 함수입니다( 참조 keras.constraints). 2. Conv2D 층의 parameter 수W = 컨볼루션 층의 weight 수B = 컨볼루션 층의 bias 수N = 커널의 개수C = input img의 채널 개수P = 컨볼루션 층의 parameter 개수\\[W = K^2 * C * N \\\\B = N \\\\P = W + B\\]Example )model.add(keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28,28,3)))weight 수 = 커널 사이즈 * 채널 수 * 출력 차원 수 = (3*3) * 3 * 16bias 수 = 출력 차원 수 = 16총 parameter 수 = weight 수 + bias 수 = 448model.add(keras.layers.Conv2D(32, (3,3), activation='relu'))weight 수 = 커널 사이즈 * 채널 수 * 출력 차원 수 = (3*3) * 16 * 32bias 수 = 출력 차원 수 = 32총 parameter 수 = weight 수 + bias 수 = 4640tf.keras.layers.Dense1. Dense 층의 매개변수tf.keras.layers.Dense( units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs) 인수   units 양의 정수, 출력 공간의 차원입니다. activation 사용할 활성화 기능입니다. 아무 것도 지정하지 않으면 활성화가 적용되지 않습니다(예: “선형” 활성화: ). a(x) = x use_bias 부울, 레이어에서 바이어스 벡터를 사용하는지 여부. kernel_initializer kernel가중치 행렬 의 초기화 프로그램입니다 . bias_initializer 바이어스 벡터에 대한 이니셜라이저입니다. kernel_regularizer kernel가중치 행렬 에 적용된 정규화 함수 . bias_regularizer 편향 벡터에 적용된 정규화 함수. activity_regularizer 레이어의 출력에 적용된 레귤러라이저 기능(“활성화”). kernel_constraint kernel가중치 행렬 에 적용되는 제약 조건 함수 입니다. bias_constraint 바이어스 벡터에 적용되는 제약 조건 함수입니다. 2. Dense 층의 parameter수W = Dense 층의 weight 수B = Dense 층의 bias 수N = 노드의 개수I = input data 개수P = 컨볼루션 층의 parameter 개수\\(W = I * N\\\\B = N\\\\P = W + B\\)Example )model.add(keras.layers.Dense(32, activation='relu'))weight 수 = input data 수 * 노드의 개수 = 800 * 32bias 수 = 노드의 개수 = 32총 parameter 수 = weight 수 + bias 수 = 25632출처https://stackoverflow.com/questions/50212330/dense-layer-weights-shapehttps://seongkyun.github.io/study/2019/01/25/num_of_parameters/https://www.tensorflow.org/api_docs/python/tf/keras/layers/Densehttps://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D"
    } ,
  
    {
      "title"       : "파이썬 ) 지역변수, 전역변수",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%ED%8C%8C%EC%9D%B4%EC%8D%AC-)-%EC%A7%80%EC%97%AD%EB%B3%80%EC%88%98,-%EC%A0%84%EC%97%AD%EB%B3%80%EC%88%98.html",
      "date"        : "2022-01-21 00:00:00 +0900",
      "description" : "",
      "content"     : "https://i-never-stop-study.tistory.com/14오늘은 작사가 Exploration에서 코드가 이해되지 않았던 부분에 대해 작성하려고 한다.# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.# 지금은 동작 원리에 너무 빠져들지 마세요~for src_sample, tgt_sample in dataset.take(1):break# 한 배치만 불러온 데이터를 모델에 넣어봅니다model(src_sample) for문을 읽어오자 마자 break를 한다. 그냥 src_sample, tgt_sample = dataset.take(1)하면 안되나? 일단 두가지 시도를 해보았는데 안된다. + : (src_sample, tgt_sample) = dataset.take(1)도 해봤음. 위에거랑 같은 오류뜸. for문 안에 선언해준 변수 src_sample은 지역변수일텐데 왜 for 블록 바깥에서 쓸 수 있는가. 알고보니 python에서는 for에서 선언해준 함수가 지역변수가 아니라 전역변수였다.(!) 그래서 다음과 같은 식이 가능하다. for i in range(5): print(i) print(i+1) 결과 화면 1 2 3 4 5 자바나 c에서는 for (int i = i ; i &lt;5 ; i++) { } 이 코드블럭에서 i는 지역변수라서 파이썬도 당연히 지역변수일줄 알았는데, 아니었다."
    } ,
  
    {
      "title"       : "[fd_18]  딥러닝 들여다보기",
      "category"    : "",
      "tags"        : "",
      "url"         : "./fd_18-%EB%94%A5%EB%9F%AC%EB%8B%9D-%EB%93%A4%EC%97%AC%EB%8B%A4%EB%B3%B4%EA%B8%B0.html",
      "date"        : "2022-01-21 00:00:00 +0900",
      "description" : "",
      "content"     : "이 포스팅은 아이펠 fundamentals 18번 노드를 학습하고 적은 기록물 입니다.신경망이란?과학자들은 문제에 대한 해답을 종종 자연에서 찾아냅니다. 머신러닝 / 딥러닝 과학자들은 해당 문제를 해결하기 위하여 신경망의 뉴런 하나를 본 딴, 퍼셉트론의 개념을 도입하였고, 후에 이를 연결하여 다층 신경망 구조, 인공신경망을 구축해내게 됩니다.신경망은 모두가 가지고 있는 뇌의 뉴런들을 본딴 것입니다. 뉴런들이 모여 거대한 그물망과 같은 형태를 띄고 있는데, 이를 컴퓨터 프로그램으로 풀어낸 것입니다.다음은 다층 퍼셉트론 구조로 학습한 mnist 코드의 부분입니다.# 모델에 맞게 데이터 가공x_train_norm, x_test_norm = x_train / 255.0, x_test / 255.0x_train_reshaped = x_train_norm.reshape(-1, x_train_norm.shape[1]*x_train_norm.shape[2])x_test_reshaped = x_test_norm.reshape(-1, x_test_norm.shape[1]*x_test_norm.shape[2])# 딥러닝 모델 구성 - 2 Layer Perceptronmodel=keras.models.Sequential()model.add(keras.layers.Dense(50, activation='sigmoid', input_shape=(784,))) # 입력층 d=784, 은닉층 레이어 H=50model.add(keras.layers.Dense(10, activation='softmax')) # 출력층 레이어 K=10model.summary()궁금증 1. x_train_reshaped에서 왜 -1 값을 줄까?​ 찾아보니 reshape에서 (-1) 옵션을 주면, 주어진 옵션값에 맞는 값을 찾아준다고 한다.궁금증 2. 전에 Dense()는 뭐였을까 궁금했었다.​ Dense : 다층 퍼셉트론을 keras 코드로 사용하는 거였습니다.Bias란?bias에 관한 링크를 보다 궁금증이 생겼었습니다.참고 링크 : https://stackoverflow.com/questions/2480650/what-is-the-role-of-the-bias-in-neural-networks질문 1. 제가 알고 있는 출력값 y = w(가중치)*x(입력데이터) + b(bias)인데, 링크의 첫번째 답변에서 bias를 이용한 시그모이드를 sigmoid(w0*x + w1*1.0)이라 표현했어요. 제가 알기론 bias에 관해서는 가중치를 안쓴다고 알고있는데 쓰는경우도 있는건지, 원래 쓰는데 안쓰는걸로 제가 잘못알고있는건지 궁금해요. 이 질문에 가장 도움이 되었던 링크 두개를 먼저 첨부합니다. https://devlog.jwgo.kr/2018/04/16/sigmoid-graph-according-to-slope-change/ https://blog.naver.com/PostView.nhn?blogId=winddori2002&amp;logNo=221937861519 가장 먼저 이해가 안됐던 부분은 bias가 성공적인 학습에 매우 중요한 요소이며, bias가 바뀜에 따라 함수가 왼쪽 혹은 오른쪽으로 이동이 된다는 것이었습니다. 제가 알고있는 출력 함수의 식은 w*x + b인데, 결국 w가 0일때 활성화함수가 b인곳을 꼭 지나게 됩니다. w=0일때, bias값의 절편에 따라 출력함수값이 거기에 묶이게 되는데 어떻게 성공적인 학습에 어떻게 기여를 하는지가 이해가 안됐고 특히 bias값에 따라 그래프가 왼쪽 혹은 오른쪽으로 움직이는 것이 아니라 위 아래로 움직여야 한다고생각했습니다. 이 질문이 들었던 가장 큰 오해는 출력함수는 wx+b라는 가장 간단한 식으로 나타내는것이 아니라는 것입니다. 예를 들어, sigmoid함수에서 wx+b함수를 적용해서 그리면 아래 링크와 같습니다. https://devlog.jwgo.kr/2018/04/16/sigmoid-graph-according-to-slope-change/ bias를 -10에서 10으로 변화시킬때 시그모이드 함수의 형태를 보여주고 있습니다. 세로가 아니라 가로로 움직이게 된다는 것을 시각적으로 확인하였고, wx+b에서 학습결과가 꼭 0, b를 지나게되지 않는다는 것도 이해를 하게되었습니다. 두번째로 이해가 안되었던, 시그모이드에서 bias 1.0을 sigmoid(w0*x + w1*1.0)과 같이 가중치가 있는것 처럼 표현하였습니다. 이부분은 아래 질문에서 답하겠습니다. 질문2. bias에 관해서 그렇게 깊게 생각해보지 않았었는데, (그냥 주는구나 싶은정도?) bias 값은 어떻게 결정해서 주게되나용? 이거도 그냥 많이 쓰는숫자에서 시행착오로 돌려보고 결정하나요? https://brunch.co.kr/@coolmindory/32 여기 링크를 보고 이해가 되었습니다. 여기서 보면 backpropagation에서 업데이트 대상은 가중치와 bias 둘 다입니다. 그랬기 때문에, 바이어스에도 가중치가 있다는 듯이 표현을 한것이라 이해했습니다. 그리고 최근에 backpropagation에 대해서 발표를 한 적이 있는데, 그때 이런 질문을 받은 적이 있습니다. 그렇다면 Backpropagation에서 bias는 어떻게 업데이트되냐 라는 질문이었습니다. 저는 그때 출력함수 wx+b라는 매우 간단한 식만을 이해하고 있었기 때문에, backpropagation에서 b는 상수항이다. 그래서, 미분하면 0이 된다. 라고 답변을 드렸었는데, 제가 잘못된 답변을 드린것을 알게되었습니다. 가중치를 갱신할 때 w=w-α*dL/dw 인것처럼 bias도 b= b-α*dL/db로 갱신된다는 것을 알게 되었습니다. 활성화 함수신경망은 수 많은 뉴런으로 이루어져 있고,손실함수경사하강법오늘 배운 메소드 np.dot(X,Y) : np.array 객체를 곱할때 사용합니다. 1차원 행렬(Vector) 이라면 : 각 자리 숫자끼리 곱하여 더합니다. 2차원 행렬(matrix) 라면 : 일반적인 행렬 곱을 수행합니다. 출처 : https://ko.wikipedia.org/wiki/%ED%96%89%EB%A0%AC_%EA%B3%B1%EC%85%88 X나 Y 중 하나가 상수라면 : 상수곱 연산을 합니다. np.exp(x) : numpy.exp() 함수는 밑이 자연상수 e인 지수함수(e^x)로 변환해줍니다. 출처 AIFFEL LMS 문제시 연락 부탁드립니다. :) https://m.blog.naver.com/shwotjd14/221435180635 https://reniew.github.io/12/"
    } ,
  
    {
      "title"       : "[fd_17] 딥러닝과 신경망의 본질",
      "category"    : "",
      "tags"        : "",
      "url"         : "./fd_17-%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B3%BC-%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%98-%EB%B3%B8%EC%A7%88.html",
      "date"        : "2022-01-21 00:00:00 +0900",
      "description" : "",
      "content"     : "이 포스팅은, 아이펠 Fundamental 17번 노드를 학습하고 작성한 기록물 입니다.출처 AIFFEL LMS 문제시 연락 부탁드립니다. :)"
    } ,
  
    {
      "title"       : "머신러닝 ) 앙상블 ensemble",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-)-%EC%95%99%EC%83%81%EB%B8%94-Ensemble.html",
      "date"        : "2022-01-18 00:00:00 +0900",
      "description" : "",
      "content"     : "Kaggle 사이트에서출처 Introduction to Ensemble Learning https://subinium.github.io/introduction-to-ensemble-1/#:~:text=%EC%95%99%EC%83%81%EB%B8%94(Ensemble)%20%ED%95%99%EC%8A%B5%EC%9D%80%20%EC%97%AC%EB%9F%AC,%EB%A5%BC%20%EA%B0%80%EC%A7%80%EA%B3%A0%20%EC%9D%B4%ED%95%B4%ED%95%98%EB%A9%B4%20%EC%A2%8B%EC%8A%B5%EB%8B%88%EB%8B%A4. Kaggle Ensembling Guide 한글 번역 https://jamm-notnull.tistory.com/16 ㅎ vcvvvvvvvvvvvvvvvvvvvv"
    } ,
  
    {
      "title"       : "Cv ) 파이썬으로 이미지 다루기",
      "category"    : "",
      "tags"        : "",
      "url"         : "./CV-)-%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%9C%BC%EB%A1%9C-%EC%9D%B4%EB%AF%B8%EC%A7%80-%EB%8B%A4%EB%A3%A8%EA%B8%B0.html",
      "date"        : "2022-01-17 00:00:00 +0900",
      "description" : "",
      "content"     : "오늘은 파이썬에서 이미지 파일 다루는 법에 대하여 공부해 보았습니다. AIFFEL의 FUNDMANTAL 16번 노드의 학습노트입니다.디지털 이미지란?컴퓨터에서 이미지 데이터를 방식에는 대표적으로 두가지가 있습니다. 래스터 Raster 방식 / 비트맵 Bitmap 방식 벡터 Vector 방식래스터 Raster 방식 / 비트맵 Bitmap 방식사람의 시세포는 대부분 3가지 색상에 반응합니다. 많이 들어본 RGB값으로 Red, Green, Blue 색상입니다. 이와 같은 방식으로 픽셀 하나당의 RGB값을 저장하는 방식이 래스터 혹은 비트맵 방식입니다.한 픽셀당 0-255값을 가지며 0은 검정, 255는 흰색입니다.픽셀당 값이 정해져있기 때문에, 확대와 축소시 해당 픽셀값을 넓히거나 축소시켜 모자이크된 것 처럼 깨지는 특징이 있습니다.벡터 Vector 방식상대적인 점과 선의 위치를 방정식에 기록해두었다가, 확대 및 축소에 따라 해당 픽셀값을 계산해내는 방식입니다. 그렇기 때문에 확대와 축소에 자유로운 특징이 있습니다.그 밖의 방식들 YUV 방식 예전에 흑백 TV에서 컬러 TV로 이용되던 방법으로 흑백 이미지에다가 1/4 해상도를 가진 두 색상채널을 덧붙여서 송출하는 방식입니다. HSV 방식 Hue 색상 Saturation 채도 Value 명도 CMYK 인쇄매체에서 자주 사용되는 이미지의 색상 표현방법이며, Cyan, Magenta, Yellow, Black 네가지 색상을 이용합니다. 컬러 스페이스 Color space : 색을 표현하는 다양한 방식채널 Channel : 각 컬러 스페이스를 구성하는 단일 축Pillow 사용법파이썬의 이미지 라이브러리는 PIL이라는 라이브러리가 유명했으나 현재는 개발이 진행되고 있지 않습니다. 대신 Pillow라는 라이브러리가 대신 개발이 되고 있으며, 요즘 이미지처리에는 OpenCV를 많이 사용합니다.1. PIL을 이용해 간단한 이미지 생성하기이미지는 배열 형태의 데이터로 이루어져 있습니다. 그렇기 때문에, 배열형태로 데이터를 선언하기만 하면 이미지를 생성할수있습니다.import numpy as npfrom PIL import Imagedata = np.zeros([32, 32, 3], dtype=np.uint8)# np.zeros : 생성되는 행렬 값을 전부 0으로 채운다.image = Image.fromarray(data, 'RGB')#fromarray : np 배열을 이미지 객체로 변환해주는 코드imagecf. 흰색 : data = np.zeros([32, 32, 3], dtype=np.uint8) +255cf. 파란색 : data[:, :] = [0, 0, 255]2. 이미지 저장하기레퍼런스 보는 것은 항상 어려운것 같습니다.봐도봐도 모르겠고, 어떻게 써야할지는 모르겠으나,, 일단 레퍼런스에서 저장하는 메소드를 찾아서 코드를 작성해보았습니다.img.save(\"test.jpg\")위코드는 안되며, 다음 오류를 내뱉습니다.뭔가 RGBA를 RGB로 바꾸어 줘야 하나 봅니다.어디선가 줏어들은 지식으로 JPG는 RGB값저장하는 형태라고 했던듯img = img.convert('RGB')img.save(\"test.jpg\")위와 같이 수정했더니 오류는 안나지만, jpg파일이 저장되진 않았습니다.이 파이썬 파일이 존재하는 상대경로에 생길줄알았는데ㅠㅠ해결된 코드는 다음과 같습니다.new_image_path = os.getenv('HOME')+'/aiffel/python_image_proc/data/jpg_pillow_practice.jpg'img = img.convert('RGB')img.save(new_image_path)저장될 경로를 선언해주는 것이 중요합니다.3. cifar-100 데이터 전처리cifar-100 데이터 셋은 10000*3072의 numpy array of uint8s 로 이루어져 있습니다. 각 행은 32*32 컬러이미지를 저장하고 있고, 첫번째 1024 픽셀은 Red, Green, Blue 데이터를 저장하고 있습니다.다음 코드는 cifar 데이터 중 train 데이터를 가저오는 코드입니다.import osimport picklefrom PIL import Imagedir_path = os.getenv('HOME')+'/aiffel/python_image_proc/data/cifar-100-python'train_file_path = os.path.join(dir_path, 'train')with open(train_file_path, 'rb') as f: train = pickle.load(f, encoding='bytes')print(type(train))해당 행을 이미지로 변환하려면, reshape를 해주면 되지만, 1024를 각각 32*32에 채워주고 이를 3번 반복해야합니다.그러기 위에서는 reshape에 order옵션을 사용하면 됩니다. 앞선 차원부터 데이터를 채우는 방식입니다. 코드는 아래와 같습니다.image_data = train[b'data'][0].reshape([32, 32, 3], order='F') image = Image.fromarray(image_data) # fromarray : 배열을 이미지 객체로 변환아무 처리를 하지 않은 상태로 위의 image를 출력하면 사진이 누워있는 형태로, X축과 Y축이 바뀌어있습니다. 그렇기 때문에 축을 바꿔주는 과정이 필요합니다.image_data = image_data.swapaxes(0, 1)image = Image.fromarray(image_data)OpenCVopenCV는 오픈소스로 제공되는 컴퓨터 비전용 라이브러리입니다.보통 이미지는 RGB 순서를 사용하지만 openCV는 BGR순서를 사용합니다.이미지는 [너비, 높이, 채널]의 형태를 가집니다.openCV를 이용해서, 사진에서 파란색 부분만을 추출하는 토이 프로젝트를 작성했었고, 해당 코드 중 기억해야할 메서드를 정리해보았습니다.hsv = cv.cvtColor(img, cv.COLOR_BGR2HSV) : BGR인 이미지를 HSV이미지로 변경하는 코드 그밖의 많은 옵션들 : https://docs.opencv.org/4.x/d8/d01/group__imgproc__color__conversions.htmlmask = cv.inRange(hsv, lower_blue, upper_blue) lower_blue = np.array([100,100,100]) upper_blue = np.array([130,255,255]) H : 색상, S : 채도, V 명도 파란색상은 100-130, 채도와 명도를 100-255 범위로 상정 lowerblue와 upper_blue 사이에 있는 값은 True로 1로(픽셀값으론 255로 설정), 나머지는 False 0으로 칠하는 코드 여기서 확인 : https://docs.opencv.org/4.x/d2/de8/group__core__array.html#ga48af0ab51e36436c5d04340e036ce981 res = cv.bitwise_and(img, img, mask=mask) img에서 mask값이 0인 부분은 색상을 날려버리고, 1인 부분은 기존 이미지의 색상값으로 연산하는 메소드출처 AIFFEL LMS 문제시 연락 부탁드립니다. :) https://pillow.readthedocs.io/en/stable/reference/Image.html"
    } ,
  
    {
      "title"       : "Aiffel) 2 3주차의 회고, 4주차 시작의 마음가짐",
      "category"    : "",
      "tags"        : "",
      "url"         : "./AIFFEL)-2-3%EC%A3%BC%EC%B0%A8%EC%9D%98-%ED%9A%8C%EA%B3%A0,-4%EC%A3%BC%EC%B0%A8-%EC%8B%9C%EC%9E%91%EC%9D%98-%EB%A7%88%EC%9D%8C%EA%B0%80%EC%A7%90.html",
      "date"        : "2022-01-17 00:00:00 +0900",
      "description" : "",
      "content"     : "설렘과 열정 가득한 첫 OT는 이제 까마득한 옛날이 되었고,정말 바빴던 2-3주차를 보내고, 4주차의 마음가짐을 정리하려 한다.2-3주차의 회고설레는 첫, OT는 이제 까마득한 옛날이 되었고, 개인적으로 정말 바쁜 2-3주를 보냈다.서울로 올라오겠다는 결심을 하고, 이사와 아이펠, 야자 공부와 틈틈히 코테도 병행하다보니 쉬지 않았는데도,, 한번도 아이펠 끝나자마자 맘편히 쉬지 않았는데도, 하고싶은일 &amp; 해야할 일이 밀리는 일이 발생했다. 2주차에는 아이펠 중간중간 쉬는시간에 짐을 싸고, 아이펠 끝나고도 짐을싸고, 그렇게 싼 짐을 먼저 서울로 보냈다. 그리고 서울의 보금자리에 수요일에 올라가서 일요일까지 지낸 후, 필요한 리스트를 파악했다. 그다음 일요일 아침에 부산으로 내려와 필요했던 짐들을 챙기고 다음주(3주차)수요일에 서울로 올라갔다. 그리고 또 짐 정리와 청소의 연속,,,처음 아이펠을 시작할 때 최대한 많은 것을 배우고, 그 모든 것을 블로그에 남겨야겠다라는 결심을 했었다.그런 나에게 아이펠에만 집중할 수 있는 시간이 확 줄어든 것은 치명타였다. 블로그 글을 쓰는 것 자체가 느렸고, 저작권에 걸리지 않으려고 내용을 재생산하는데도 시간이 꽤나 걸렸다. 그런 와중에 욕심은 많아서 강화학습 스터디, 수학스터디도 가입하고 이젠 스탠포드의 CS231 강의도 매주 봐야 했다. 이젠 파이썬 문법을 떼는 시기였기에, 내가 진정으로 원했던 인공지능과 머신러닝에 대해 알려주는 노드들이 속속 등장했다. 그리고 난이도도 급상승과 상승을 반복.아, 방금 생각났는데, 매주 오프라인 모임까지 나갔다.ㅎㅎㅎㅎ 너무 즐겁고 좋지만 시간잡아먹는 귀신솔직히 하고싶은것과 벌여놓은일이 많았고, 해야하는 일이 많은것은 인정한다. 바빴던것도 인정하고. 하지만 그러면서도 그 목표와 일정을 전부 소화해내지 못하는 나 자신을 내가 너무 괴로워했다. 일이 많았다는것은 인정하지만, 못하는것을 이해할 수 있는 관용을 허락할 수 없었던것 같다. 그런다고 할일이 줄어드는 것은 아니니까.4주차의 마음가짐3주차의 끝 무렵.이 힘듦을, 아이펠을 그만둠으로써 해결해야하나 고민했다. 이 스케줄을 해결못하는 것이, 내가 인공지능을 공부하는데 능력이 없는게 아닐까, 라는 의구심과 내가 이 공부를 앞으로도 계속 할 수있을까 라는 물음이 머리와 마음속에 가득했다. 차라리 백엔드 개발자 준비를 본격적으로 하고, 딥러닝은 취미나 흥미정도로 남겨야 하는지에 대한 고민이 가득이었다. 결국 그 생각들은 바깥으로 삐져나왔고, 아이펠 사람들에게 이 고민을 토로했다.모두 같은 입장이었다.모두 스케줄이 빡빡하다고 느끼고 있었고, 이 내용을 소화하기엔 난이도가 높으며, 아이펠식의 약간의 중구난방적인 학습방식은 자신이 뭘 하고있는지 (비전공자, AI / 딥러닝 공부가 처음인 사람에게는 특히) 어느 분야의 어느 파트를 공부하고 있는지 이해하기 어려운게 맞았다.모두 같은 고민과 힘듦을 토로하고 나서 그 후로 좀 리프레쉬 된것 같다.혼자 뒤쳐진것 같은 기분이었는데, 전부 비슷하다는 것을 느끼고 나니, 조급함이 좀 사라졌다.그래서 이제는 마음가짐을 좀 달리먹으려 한다. 모든 노드를 private 깃허브에 기록하되, 부가적인 생각과 필수 메소드는 블로그에 작성하려한다. 블로그에서 글을 작성할때, 저작권이 매우 마음에 걸렸었다. 전체 프로젝트를 내가 알고는 있어야, 나중에 축약된 블로그 글을 봐도 이해를 할텐데, 전체 프로젝트를 저작권에 걸리지 않게 재생산 해내는 것은 시간이 매우 오래걸렸다. 하지만, 오늘 퍼실님께 여쭤본 결과 노드 내용을 공표하는게 문제고, 개인이 볼려고 저장하는것은 괜찮다고 하셔서, 전체 내용을 private에 작성하고, 액기스만 블로그에 작성하는걸로 결론을 냈다. 앞으로 글쓰기 시간을 줄이는데 많은 도움이 될 것이다. 미뤄진 것은 이미 미뤄진것. 2-3주간 미뤄진 것은 미뤄진 것이다. 지금 그걸 다하면서 4주차 + 새로운 스터디 + 새로운 내용 전부 공부하려니 과부하가 걸렸다. 일단 첫째 주 처럼 현재 노드에 충실하려한다. 즉, 미뤄진 노드를 더 생산하지 않기! 노드는 밀리더라도, CS231, 강화학습 강의를 본 것은 꼭 블로그에 남기기! 영어 강의가 이해하는데 생각보다 많은 시간을 잡아먹는다. 그렇기 때문에 진짜 최소한 강의를 봤던 요약본은 꼭!! 블로그에 남겨서 두번 세번 보는 일을 줄이기로 하였다. 너무 조급해하지 않기. 모든 내용을 다 챙기고 지금 다 기억하는 것은 무리인 것을 인정하고, 취할 것위주로 먼저 끝내기로 했다. 그래도 오늘 알게 된 희소식은 곧 설날이라는 것! 설날 = 아이펠 쉬는날 = 밀린 노드 / 공부 처리할 수있는 날!일단, 현재에 충실하고, 이미 미뤄져버린것에 집착은 하지 않도록 할것이다."
    } ,
  
    {
      "title"       : "머신러닝) 탐색적 데이터 분석을 이용한 포켓몬",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D)-%ED%83%90%EC%83%89%EC%A0%81-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B6%84%EC%84%9D%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-%ED%8F%AC%EC%BC%93%EB%AA%AC.html",
      "date"        : "2022-01-14 00:00:00 +0900",
      "description" : "",
      "content"     : "오늘은, kaggle의 ( ) 데이터셋을 사용하여서 머신러닝을 진행해보았습니다.출처 AIFFEL LMS 문제시 연락 부탁드립니다. :)"
    } ,
  
    {
      "title"       : "데이터 전처리 ) 원 핫 인코딩 one Hot encoding",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%84%EC%B2%98%EB%A6%AC-)-%EC%9B%90-%ED%95%AB-%EC%9D%B8%EC%BD%94%EB%94%A9-One-Hot-Encoding.html",
      "date"        : "2022-01-14 00:00:00 +0900",
      "description" : "",
      "content"     : "원 핫 인코딩이란, 범주형 데이터 일 때, 많이 사용하는 데이터 전처리 방법이며, 카테고리별로 이진특성을 만들어 해당하는 특성만 1, 나머지는 0으로 주는 방법입니다.출처 AIFFEL LMS 문제시 연락 부탁드립니다. :)"
    } ,
  
    {
      "title"       : "데이터 전처리의 다양한 기법",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%84%EC%B2%98%EB%A6%AC%EC%9D%98-%EB%8B%A4%EC%96%91%ED%95%9C-%EA%B8%B0%EB%B2%95.html",
      "date"        : "2022-01-13 00:00:00 +0900",
      "description" : "",
      "content"     : "인공지능에는 다양한 데이터 처리의 다양한 기법이 있습니다.오늘은 다양한 데이터 전처리의 기법에 대해 알아보았고, 각각의 내용을 정리해 보았습니다. 그리고 그에 대한 내용을 링크에 남겼습니다.데이터 전처리 기법결측치결측치란, 입력이 누락된 값입니다. 보통 제거하거나 대체하여 처리를 합니다.중복된 데이터말 그대로, 중복된 데이터입니다. 혹시 데이터의 특성상 같은 행에서 모든 데이터가 유일해야 한다면, 중복된 데이터를 제거하여 처리합니다.이상치 outlier대부분의 값의 범위에서 벗어나 극단적으로 크거나 작은 값. 이상치를 처리하는 방법은 z-score, modified z-score method, iqr method가 있다.정규화 Normalization[데이터 전처리 ) 정규화 처리 방법]:원-핫-인코딩 One-Hot-Encoding구간화 Binning그 밖의 기법로그변환2019년 kaggle에서 진행한 캐글 코리아와 함께하는 2nd ML대회 - House Price Prediction에서 train데이터의 분포를 살펴보면 다음과 같습니다.…생략위 그래프 중에서 bedrooms, sqft_living, sqft_lot, sqft_above, sqft_basement, sqft_living15, sqft_lot15 변수가 한쪽으로 치우친 경향을 보입니다.이렇게 한쪽으로 치우친 분포의 경우, 로그 변환(log_scaling)을 통해 데이터 분포를 정규분포에 가깝게 만들어 주면 좋습니다.skew_columns = ['bedrooms', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'sqft_lot15', 'sqft_living15']for c in skew_columns: data[c] = np.log1p(data[c].values)위 코드는 로그 변환을 한 코드입니다. numpy.log1p() 함수는 입력 배열의 각 요소에 자연로그 log(1 + x)처리해 주는 함수입니다.이전보다 치우침이 줄어든 분포를 확인할 수있습니다.로그 변환시 치우침이 줄어드는 이유는 다음과 같습니다. 0&lt;x&lt;1 범위에서 기울기는 매우 가파릅니다. 그래서 x의 범위는 0-1로 매우 작은 반면 y는 (−∞,0)로 매우 발산하는 형태입니다. 따라서, 0에 치우친 값들을 로그 변환하면 값이 넓은 범위로 펼칠 수 있는 특징을 갖게 됩니다. 반면 x값이 1을 넘어가면 y의 기울기는 급격히 작아집니다. 즉, 큰 x 값들에 대해 y값이 크게 차이나지 않게되며 넓은 범위의 x값은 작은 y값에 모이게 됩니다."
    } ,
  
    {
      "title"       : "데이터 전처리 ) 중복된 데이터 처리 방법",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%84%EC%B2%98%EB%A6%AC-)-%EC%A4%91%EB%B3%B5%EB%90%9C-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%B2%98%EB%A6%AC-%EB%B0%A9%EB%B2%95.html",
      "date"        : "2022-01-13 00:00:00 +0900",
      "description" : "",
      "content"     : "말 그대로, 중복된 데이터입니다. 혹시 데이터의 특성상 같은 행에서 모든 데이터가 유일해야 한다면, 중복된 데이터를 제거하여 처리합니다.1. 중복된 데이터 여부dataframe_object.duplicated()true / false로 결과값이 나옵니다.2. 중복된 데이터 행 출력dataframe_object[dataframe_object.duplicated()]3. 중복된 데이터 행 모두 출력)dataframe_object[(dataframe_object[중복데이터 열이름]==\"중복데이터\")&amp;(dataframe_objct[중복데이터 열이름]==\"중복데이터\")]이를 이용하면 중복된 데이터가 같이 출력되며, 인덱스도 확인할 수 있게 됩니다.4. 중복된 데이터 삭제dataframe_object.drop_duplicates(inplace=True)DB에서 삽입이상 문제에서 데이터삭제1-5까지의 인덱스가 있고 모든 인덱스값은 유일해야한다는 가정에서 3번 인덱스를 삭제하고 데이터를 추가해야하는데, 삭제하지 않고 추가하는 경우 인덱스가 12345 3 이렇게 추가되는 경우가 있습니다.이렇게 인덱스값이 중복된 경우, 맨 나중에 들어온 값만 남겨야 합니다. 이때는 dataframe.drop_duplicates()메서드에서 subset, keep 옵션을 통해 중복을 제거할 수 있습니다.dataframe_object.drop_duplicates(subset=[중복이 발생한 열], keep='last')출처 AIFFEL LMS 문제시 연락 부탁드립니다. :)"
    } ,
  
    {
      "title"       : "머신러닝 ) 의사결정 트리와 랜덤 포레스트 decision tree random forest",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-)-%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95-%ED%8A%B8%EB%A6%AC%EC%99%80-%EB%9E%9C%EB%8D%A4-%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8-Decision-tree-Random-forest.html",
      "date"        : "2022-01-12 00:00:00 +0900",
      "description" : "",
      "content"     : "Aiffel Exploration 2에서 배운 내용 중, Decision Tree와 Random Tree에 대해서 작성해보려 합니다.1. Decision Tree2. Random forestDecision Tree의 단점을 극복하기 위해 여러개 모아 놓은 것이 Random Forest입니다.이러한 기법을 앙상블 Ensemble 기법이라고 합니다. 데이터 사이언스에서 앙상블이란, 의견을 통합하거나 여러가지 결과를 합치는 방식을 의미힙니다. 출처 : 텐서플로우 허브위 그림은 Random Forest를 나타냅니다. 여기서 랜덤은 의사 결정트리를 만드는 데 있어 사용되는 feature값을 무작위적으로 선정합니다. 조금 더 풀어서 설명하자면 다음과 같습니다. 30개의 feature가 존재한다면, 30개의 feature에 대한 의사결정 트리를 만듭니다. 그 중 무작위적으로 일부만 선택하여 하나의 의사결정 트리를 생성합니다. 이를 여러번 반복합니다. 여러 결정 트리들이 내린 예측값 중 가장 많이 나온 값을 최종 예측값으로 지정합니다.이렇게 의견을 통합하거나, 여러 가지 결과를 합치는 방식을 앙상블이라고 합니다. 정리하자면, Random forest는 하나의 거대한 결정 트리를 만드는 것이 아니라, 여러 개의 작은 결정 트리를 만드는 것이며 앙상블 기법을이용하여 예측값을 냅니다.Random foreset는 상위 모델들이 예측하는 편향된 결과보다 다양한 모델들의 결과를 반영하여 더 다양한 데이터에 대한 의사결정을 내리게 됩니다.해당 코드는 다음 링크에 설명되어 있습니다.출처 AIFFEL LMS Exploration 2 문제시 연락부탁드립니다 :) 질문 다음 사진에서 8번이 이해안가요"
    } ,
  
    {
      "title"       : "머신러닝 ) svm",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-)-SVM.html",
      "date"        : "2022-01-12 00:00:00 +0900",
      "description" : "",
      "content"     : "Support Vector Machine (SVM)SVM은 Support Vector와 Hyperplane(초평면)을 이용해서 분류를 수행하게 되는 대표적인 선형 분류 알고리즘입니다.2 차원 공간에서, 즉 데이터에 2개의 클래스만 존재할 때, Decision Boundary(결정 경계): 두 개의 클래스를 구분해 주는 선 Support Vector: Decision Boundary에 가까이 있는 데이터 Margin: Decision Boundary와 Support Vector 사이의 거리Margin이 넓을수록 새로운 데이터를 잘 구분할 수 있다. (Margin 최대화 -&gt; robustness 최대화) Kernel Trick: 저차원의 공간을 고차원의 공간으로 매핑해주는 작업. 데이터의 분포가 Linearly separable 하지 않을 경우 데이터를 고차원으로 이동시켜 Linearly separable하도록 만든다. cost: Decision Boundary와 Margin의 간격 결정. cost가 높으면 Margin이 좁아지고 train error가 작아진다. 그러나 새로운 데이터에서는 분류를 잘 할 수 있다. cost가 낮으면 Margin이 넓어지고, train error는 커진다. γ: 한 train data당 영향을 미치는 범위 결정. γ가 커지면 영향을 미치는 범위가 줄어들고, Decision Boundary에 가까이 있는 데이터만이 선의 굴곡에 영향을 준다. 따라서 Decision Boundary는 구불구불하게 그어진다. (오버피팅 초래 가능) 작아지면 데이터가 영향을 미치는 범위가 커지고, 대부분의 데이터가 Decision Boundary에 영향을 준다. 따라서 Decision Boundary를 직선에 가까워진다.많은 선형 분류 모델은 대부분 이진 분류 모델입니다. 그런데 이진 분류 알고리즘을 일대다(one-vs.-rest 또는 one-vs.-all) 방법을 사용해 다중 클래스 분류 알고리즘으로 사용할 수 있습니다. 일대다 방식은 각 클래스를 다른 모든 클래스와 구분하도록 이진 분류 모델을 학습시킵니다. 클래스의 수만큼 이진 분류 모델이 만들어지고 예측할 때는 만들어진 모든 이진 분류기가 작동하여 가장 높은 점수를 내는 분류기의 클래스를 예측값으로 선택합니다. 그리고 SVM 모델은 다음과 같이 사용합니다.출처 AIFFEL LMS 문제시 연락 부탁드립니다. :)"
    } ,
  
    {
      "title"       : "강화학습 ) 1강. introdution to reinforcement learning",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-)-1%EA%B0%95.-Introdution-to-Reinforcement-Learning.html",
      "date"        : "2022-01-11 00:00:00 +0900",
      "description" : "",
      "content"     : "현재 저는 강화학습 스터디를 참여해 강화학습에 대한 강의를 보고있습니다.오늘의 포스팅은 그 강의에 대한 회고록 및 요약 정리에 관한 것입니다.강의정보 링크 https://www.youtube.com/watch?v=TCCjZe0y4Qc 강의자 Hado van Hasselt, Senior Staff Reasearch Scientist, Deepmind 강화학습 스터디에서 진행하는 강의 같이보기 도전중! 1. what is reinforcement learning?Industrial revolution과 Digital Revolution으로 이야기를 시작고, 본격적인 AI에 관한 이야기가 진행되는데 이 말이 너무 멋있어서 메모했다. Next thing that you can think of which is already ongoing.지금 현재, AI 기술이 얼마나 빠르게 발전하고 있는지에 대해 알 수 있는 문장이다.Artificial Intelligence란?현재는 데이터를 학습하여 그저 계산을 하는 단계는 넘어섰고, 스스로 학습하여 솔루션을 찾을 수 있는 기계의 단계로 넘어가고 있다. 즉 자율적으로 결정을 하고, 학습을 하는 것이다.우리가 정의 내리는 인공지능은 다음에 따라 결정될 것이다 : To be able to learn to make dicisions to achieve goals.Reinforcement learning이란?사람이 환경과 상호작용을 하며 학습을 한다는 아이디어를 시작으로 개발된 이론강화학습의 특징 수동적이기보단 적극적이며, (2강에서 자세히) 독립적이지 않고 연속적이다. 향후 상호작용은 이전 상호작용에 의해 달라질 수 있다. 목표 지향적이다. we can learn without examples of optimal behavior 모든 하위 행동을 하나하나 지정해주지 않아도, 명확한 목적을 통해 하위 행동이 자동적으로 행동이 정해진다.(이 부분이 잘 이해가 안갑니다. 약 10:00분쯤) 보상을 최적화 한다.(학습한다.) 즉, 목표 : Agent is going to try to optimize sum of rewards, through repeated interaction 그렇기 때문에 강화학습은, environment와 agent의 상호작용(observation / action)에서 상호작용 하나하나에 관심을 갖기보다, 그 상호작용의 결과물, 즉 보상이 최대가 되도록 하는데에 관심을 갖는다. The reward hypothesis16:14Examples RL problems실제로 강화학습을 이용해 해결한 문제들Fly a helicopter보상 : air time(비행시간), inverse distance, ..Manage an investment porfolio보상 : gains, gains minus risk, ..Control a power station보상 : efficienct, …Make a robot walk보상 : distance, speed, …play video or board games보상 : win, maximise score, …강화학습을 꼭 이용했어야 하는 distinct한 이유들 find solutions it can adapt online, deal with unforeseen circumstances강화학습은, 해결책을 찾아야 하거나, 전혀 모르는 상황에 대해서도 적응할 수 있어, 위 두개의 문제에 대한 알고리즘을 제공해 줄 수 있다.하지만, 강화학습 can adapt online에서, 일반화를 하는것이 아닌, online 환경에서 효과적으로 계속 배울수 있다는 의미.-&gt; online의 데이터를 이용하여 학습을 하고 추상화 한다는게 아니라, 온라인 환경에서 학습을 할 수 있다는 의미인가요? (22”00)2. About Reinforcement LearningAgent and Environment - Interaction Loop 매 t단계마다 관찰과 보상을 받음. Agent 관찰받음 O_t 혹은 보상 R_t 행동 A 실행 A_t Environment 행동 A 수령 O_t+1 방출 혹은 보상 R_t+1 Reward reward R_t 는 스칼라값 Agent의 할일은 보상을 최대화 하는 것.G는 Goal이라고 생각하는게 보통이지만, 이건 return 반환이라고 생각. 즉, return은 accumulative reward or the sum of rewards into the future의 약어Valuesstate s에 대한 cumulative reward를 의미 agent가 취하는 행동에 따라 다름 우리가 원하는 것은, agent가 value가 커지는 액션을 취하도록 만드는 것. so, we want to pick actions such that this value becomes large. reward와 value는 해당 state에 대한 action의 유용함(타당함?)을 의미한다. reward와 value는 재귀적으로 정의될 수 있다. 32:30 return값이 R_t-1+G_t-1이 아니라, t+1인 이유가 이해가 잘안됩니다. Maximising value by taking actions 강화학습의 목표는, value를 최대화하는 액션들을 고르는것. 액션들은 장기간의 결과를 가질수있음. 보상이 지연되어 발생할 수 있음 즉시보상을 안받는 것이, 장기간의 보상을 받는게 더 좋을 수있음. Action valuesq = 어떤 state일때의 action값. 즉 historical 한 의미 가짐.q는 상태 및 행동의 가치 기능을 나타냄3. Core conceptsEnvironmentreward signalAgent Agent component agent state policy value function model agent의 내부 Agent state36분쯤 여길 좀 중점적으로 같이봣으면 좋겟어여ㅠ 예측 정책을 정의하고 선택해야함. agent 내부 사진에 정책에서 작업으로 action으로 향하는 화살표 추가 가능 어떻게 어디로 나눠야할지 모르겟음 ;Environment stateEnvironment’s internal stateIt is usually invisible to the agent​ even if it visible it may contains lots of irrelevant information additional memory가 필요함Fully Observable Environments state흔한 경우는 아니지만, agent가 모든 environments state를 다 아는경우가 있다. 이때는 observation = environment state4. Markov Decision process이 이론은 강화학습의 기반이 된 수학적 프레임워크 이론이다.보상확률과 이후의 연속적인 state 에 대한것.마르코브 의사결정은 state가 우리가 알아야하는 history를 전부 알고있다. 즉 그 state값만 알아도 의사결정을 내리는데 아무 무리가없다.Partially Observable EnvironmentsThese observations are not Markovian ex 아래 그림 내용 설명할때 잘 이해가 안가요. 45:40Policy에이전트의 행동을 정의단지, 에이전트에서 작업으로의 맵핑입니다.pi : 상태가 주어진 행동의 확률value function &amp; value Estimatesvalue functionvalue 함수는 return에 대한것을 예측할 수있습니다.value func는 정책에 의존하기 때문에 앞전에 나온 정의를 명시적으로 수정하였습니다.위의 식에 새로운 요소discount factor는 즉시보상과 장기보상의 중요성을 상쇄시킨다? value func는 상태의 만족도를 평가하는 데 사용할 수 있습니다. value func는 action들을 선택하는데 사용할 수 있습니다. return value : ~pi는 (pi가 결정론적인 상태일지라도) 정책 pi에 의해 선택된다는 것을 의미한다. (벨만 방정식, bellman equation) model모델은 다음 상태를 예측하는 모델 p를 가질 수있습니다.a state, an action, 그리고 다음상태의 state를 input으로 관찰 한후, 다음 상태를 예상하는 확률에 대한 근사치"
    } ,
  
    {
      "title"       : "Ai ) 언젠가 읽어보고 싶은 논문들",
      "category"    : "",
      "tags"        : "",
      "url"         : "./AI-)-%EC%96%B8%EC%A0%A0%EA%B0%80-%EC%9D%BD%EC%96%B4%EB%B3%B4%EA%B3%A0-%EC%8B%B6%EC%9D%80-%EB%85%BC%EB%AC%B8%EB%93%A4.html",
      "date"        : "2022-01-11 00:00:00 +0900",
      "description" : "",
      "content"     : "COMPUTING MACHINERY AND INTELLIGENCE https://www.youtube.com/watch?v=TCCjZe0y4Qc 강의의 5:50초에 나온 추천 논문 2014년 CVPR (Computer Vision and Pattern Recognition) chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/viewer.html?pdfurl=https%3A%2F%2Fwww.cv-foundation.org%2Fopenaccess%2Fcontent_cvpr_2014%2Fpapers%2FKazemi_One_Millisecond_Face_2014_CVPR_paper.pdf&amp;clen=5174385&amp;chunk=true"
    } ,
  
    {
      "title"       : "파이썬 ) 객체",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%ED%8C%8C%EC%9D%B4%EC%8D%AC-)-%EA%B0%9D%EC%B2%B4.html",
      "date"        : "2022-01-10 00:00:00 +0900",
      "description" : "",
      "content"     : "파이썬에는 다음과 같은 유명한 말이 있습니다. Everything in Python is an object, and almost everything has attributes and methods.여기 링크(클릭) 에서출처 AIFFEL LMS"
    } ,
  
    {
      "title"       : "Git) 커널에서 깃허브 사용하기",
      "category"    : "",
      "tags"        : "",
      "url"         : "./Git)-%EC%BB%A4%EB%84%90%EC%97%90%EC%84%9C-%EA%B9%83%ED%97%88%EB%B8%8C-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0.html",
      "date"        : "2022-01-07 00:00:00 +0900",
      "description" : "",
      "content"     : "많은 프로젝트를 진행하면서 Git을 이용해봤지만, 부끄럽게도 항상 GUI프로그램이 잘 되어있는 Git을 사용해왔다. Xshell이나 리눅스에서 cmd라인으로 깃에 로그인하고 레포지토리를 만들고 commit과 pull 등의 기본 동작을 하는 것을 배웠었으나, 당시에는 너무 어렵게만 느껴졌었고, 오류라도 뜨면 해결할 생각보다는 모르겠다며 넘기기 바빴다. 그런 이유로 smartGit이나, Github Desktop과 같은 프로그램들을 좋아했다.언제나 한켠에 짐짝처럼 커맨드라인으로 깃을 사용할 줄 알아야 한다고 느꼈었지만 미루기 일수였지만, (깃에 올리기만 하면 되잖아..! 라는 마인드였다.) 오늘, 그 짐을 털어버리려한다.커맨드라인 / 리눅스에서 git사용하기 START!Git과 Github는 뭘까?Git소스 코드가 업데이트 되는 버전을 기록해두고 관리할 수 있는 소스 코드 버전 관리 시스템이다.GithubGit으로 관리하는 프로젝트를 호스팅하고, 시간과 공간의 제약없이 협업할 수 있는 온라인 서비스를 제공하는 웹 사이트이다.즉, git은 버전 기록을 저장하고, github는 git의 소스코드들을 다른 사람과 공유하여 여러사람과 협업할 수있게 도와준다.Git 사용하기1. 로컬 git에 github 계정 정보 등록하기가장 먼저 할 일은, 로컬의 git과 원격에 있는 github를 연결하는 것이다.$ git config --global user.email \"yourEmailAddress@gmail.com\"$ git config --global user.name \"Nickname\"위 과정이 끝나고 git의 config 정보를 출력하면 다음과 같이 나올것이다.$ git config -l실행결과 user.email=yourEmailAddress@gmail.com user.name=Nickname2. 내 컴퓨터에 로컬 저장소 만들기git으로 관리할 로컬 저장소를 만들어봅니다.$ cd ~$ mkdir localGit cd ~ : ~는 홈디렉토리를 의미합니다. mkdir localGit 홈디렉토리에 localGit이라는 디렉토리(폴더)를 하나 만들었습니다.주의사항 및 수정사항git만드는 것을 cmd창에서 실행할 때, git의 repository이름과 로컬저장소의 폴더 이름이 달라도 되는 줄 알았다.repository의 이름과 로컬 저장소 폴더 이름이 다르면 git push할때 오류가 난다. 아마 다르게 설정을 더 해주어야 하거나(추측) 맘편하려면 git과 repository이름을 같게하는 것이 나을 것이다.그래서, 처음에는 글을 작성할 때, 로컬 git을 localGit이라는 폴더로 만들었으나, 후에 test_git으로 수정하였다.아래 글에서는 전부 test_git으로 전부 수정하였다.$ mkdir localGit –&gt; $ mkdir test_git으로 만들기3. 로컬 저장소를 git으로 만들기$ cd test_git$ git init실행 결과 Initialized empty Git repository in /aiffel/test_git/.git/ cd localGit : git저장소로 만들고 싶은 폴더내부로 이동합니다. git init : git이 이제 localGit폴더에서 발생하는 변화를 모두 기록할 것입니다. ls명령어를 사용하면 .git디렉토리가 생긴것을 볼 수 있습니다. 4. git 변화주기 - README.md 작성유명한 github의 레포지토리를 들어가면 readme.md파일이 존재하는 것을 볼 수 있습니다. 이는 해당 레포지토리가 담은 오픈 소스 코드들에 대하여 혹은 해당 레포지토리 프로젝트에 대하여 소개하는 역할을 합니다..md파일은 markdown이라는 파일을 지칭하며, 개발자들이 많이 사용하는 문서작업용 언어입니다. 필자의 블로그 카테고리 중 개인공부 &gt; 기타 에서 markdown 문법에 대한 글을 몇몇 볼 수 있으며, 구글에 markdown 문법이라 검색하면 많이 나옵니다.간단하게 readme.md를 작성해보았습니다. 이 파일은 로컬 git 저장소에서 만들어야 합니다.$ cd ~/test_git$ echo \"# This is my First command line git directory\" &gt;&gt; README.mdecho는 출력을 하는 명령어로 “쌍따옴표 안의 문자열”을 » 뒤에 나오는 파일을 타겟으로 출력합니다.즉, README.md파일을 만들고, # This is my First command line git directory을 입력해줍니다. 만들어진 파일은 다음과 같이 확인할 수 있습니다.$ ls README.md$ cat README.md # This is my First command line git directory5. git 변경사항 저장하기 - stage4번째 단계에서 git에 README.md파일을 작성함으로써 깃에 변화를 주었습니다. git의 변화는 다음과 같이 확인할 수 있습니다.$ git status On branch master No commits yet Untracked files: (use “git add ...\" to include in what will be committed) README.md nothing added to commit but untracked files present (use “git add” to track) 기본 브랜치로 master를 사용하고 있어 master로 표기되고 있습니다. 현재는 인종차별이슈로 메인 브랜치가 main으로 표기되기도 한다네요! untracked files: 새로운 파일에 대한 정보를 읽어냈습니다. 이력관리 대상에 포함 되지 않은 README.md파일을 감지한 것을 알 수 있습니다.add 와 commitgit의 repository 구조는 다음과 같이 3단계로 구성되어 있습니다.작업폴더(working directory / 로컬 git) -&gt; 인덱스 (staging area) -&gt; 저장소 (Head-repository / 실제 github내의 repository라 이해하심이 편합니다)저장소에 commit 하기 위해서는 추가(untracked files) 및 변경(Modified files)하고자 하는 파일을 먼저 인덱스에 기록(stage)하고 스테이징된 목록을 commit 명령어를 통해 저장소에 기록하게 됩니다.즉, stage(현재 git 변화가 있는지 확인용도) -&gt; add (현재 변화가 있는 파일을 후에 저장소에 저장하겠다고 표시함) -&gt; commit(저장소에 add된 요소들을 기록) 과 같은 단계로 진행됩니다.add 명령어status에서 새로 추가된 파일에 대하여 add명령어를 수행할것입니다.$ git add README.md$ git status실행 결과 On branch master No commits yet Changes to be committed: (use “git rm –cached ...\" to unstage) new file: README.md Changes to be committed : 목록에 있는 파일은 stage 완료된 파일입니다. 여러 파일을 add하기 모든 파일 : git add . 인덱스에 추가된 파일을 제외하려면 : git rm --cached commit 명령어로컬에서 stage된 변화들을 저장소에 기록하는 명령어입니다. 이를 위해서는 저장소에 repository를 만들어야합니다. 로컬에서 git을 만들었지만, 저장소에 해당 git에 대한 repository를 만들지 않았으므로, repository 생성을 한 후, commit 해보겠습니다.6. github 저장소에 repository 만들기 github에서 repository 만들기 오른쪽 위에 프로필에 + 버튼을 입력하면 New repository 버튼을 통해 새로운 repository를 만들 수 있다. ​ 저는 test_repository라는 이름으로 새로운 repository를 만들었습니다. test_git이라는 이름으로 repository를 만들었습니다.사진은 수정하지 않았으므로 혼동하지마세요! cmd 명령어로 repository 만들기 todo 7. 로컬 저장소와 원격 저장소 연결하기이 명령어는 로컬 저장소 디렉토리 안에서 실행해야 합니다.$ cd ~/localGit$ git remote add origin https://github.com/username/test_git.git위 실행에서 url은 아까 repository를 만들고 나면 나오는 url을 넣으면 됩니다. test_repository —&gt; test_git.사진은 수정하지 않았으므로 혼동하지마세요!8. 원격 저장소 연결을 위한 토큰 생성하기user profile &gt; setting &gt; Developer settings &gt; Personal access tokens &gt; generate tokenhttps://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token주의점은 토큰은, 지금 현재만 보이므로, 메모 잘해두어야합니다.9. 로컬 저장소의 기록을 원격 저장소로 전송하기9.1 commit$ git commit -m \"first commit\"add 했던 파일들이 commit 됩니다.9.2 branch 생성$ git branch -M main9.3 push$ git push -u origin mainUsername for 'https://github.com': [계정에 사용된 이메일을 입력하세요]Password for 'https://[위에 입력한 이메일]@github.com': [비밀번호(토큰)를 입력하세요]Enumerating objects: 3, done.Counting objects: 100% (3/3), done.Writing objects: 100% (3/3), 230 bytes | 230.00 KiB/s, done.Total 3 (delta 0), reused 0 (delta 0)To https://github.com/jeina7/first-repository.git유념사항 [계정에 사용된 이메일을 입력하세요] : 제가 임의로 작성한 것이며, 이메일을 작성할 때는 대괄호를 빼고 작성하시면 됩니다. 리눅스 환경의 경우, 비밀번호 입력에는 커서가 안움직이고 아무것도 안보이는게 맞습니다. 당황하지 말고 그냥 입력하세요.다음과 같이 뜨면 commit 성공!참고하면 좋을 링크 지옥에서 온 Git - git 관련 강의 https://opentutorials.org/module/2676 출처 AIFFEL LMS https://ifuwanna.tistory.com/193 - add와 commit 비교"
    } ,
  
    {
      "title"       : "통계) 파이썬으로 평균, 표준편차, 중간값, 분산 계산하기",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%ED%86%B5%EA%B3%84)-%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%9C%BC%EB%A1%9C-%ED%8F%89%EA%B7%A0,-%ED%91%9C%EC%A4%80%ED%8E%B8%EC%B0%A8,-%EC%A4%91%EA%B0%84%EA%B0%92,-%EB%B6%84%EC%82%B0-%EA%B3%84%EC%82%B0%ED%95%98%EA%B8%B0.html",
      "date"        : "2022-01-07 00:00:00 +0900",
      "description" : "",
      "content"     : "오늘은 인공지능에 대한 배경지식을 위하여 통"
    } ,
  
    {
      "title"       : "Python 제너레이터 ",
      "category"    : "",
      "tags"        : "",
      "url"         : "./Python-%EC%A0%9C%EB%84%88%EB%A0%88%EC%9D%B4%ED%84%B0.html",
      "date"        : "2022-01-07 00:00:00 +0900",
      "description" : "",
      "content"     : ""
    } ,
  
    {
      "title"       : "Python 이터레이터 iterator",
      "category"    : "",
      "tags"        : "",
      "url"         : "./Python-%EC%9D%B4%ED%84%B0%EB%A0%88%EC%9D%B4%ED%84%B0-iterator.html",
      "date"        : "2022-01-07 00:00:00 +0900",
      "description" : "",
      "content"     : ""
    } ,
  
    {
      "title"       : "Ai) 머신러닝 알고리즘의 유형과 예시",
      "category"    : "",
      "tags"        : "",
      "url"         : "./AI)-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%EC%9D%98-%EC%9C%A0%ED%98%95%EA%B3%BC-%EC%98%88%EC%8B%9C.html",
      "date"        : "2022-01-07 00:00:00 +0900",
      "description" : "",
      "content"     : "인공지능은 다음과 같이 구분할 수 있습니다. ![1. 딥러닝이란 무엇인가? 텐서 플로우 블로그 (Tensor ≈ Blog)](https://tensorflowkorea.files.wordpress.com/2018/12/028.jpg?w=625) 인공지능이라는 범주 아래에, 머신러닝, 딥러닝의 개념이 존재합니다.오늘은 이 중에서 머신러닝 알고리즘의 유형과 예시에 대하여 공부한 것을 정리하였습니다.머신러닝의 유형크게 지도학습과 비지도 학습으로 나눌 수 있습니다.지도학습지도학습이란, 학습데이터와 정답데이터가 모두 주어지며, 이를 기반으로 학습이 이루어 집니다.예를 들어, 고양이사진과 강아지 사진을 섞어놓고, 각각의 이미지마다 정답을 라벨링을 합니다. 머신러닝 모델은 이미지와 정답 라벨을 같이 학습하게 되며, 후에 새 이미지가 들어왔을 경우, 고양이인지 강아지인지를 판별하게 되는 형식으로 진행됩니다.즉, 지도학습이 학습하게 될 데이터는 이미 정답이 분류된 학습용 데이터로 구성된 입력변수와 원하는 출력변수로 이루어져 있습니다. 그리고 알고리즘을 통해 이 데이터를 분석함으로써 입력변수와 출력변수의 관계를 맵핑시키게 됩니다. 이렇게 학습된 머신러닝 모델은 새로운 데이터를 일반화시켜 새로운 사례에 대한 판단을 내리거나 눈에 보이지 않는 상황속에서 결과를 예측할 수 있습니다.분류 classification범주형 데이터를 예측할때 사용되는 기법입니다회귀 regression예측 forecasting비지도학습출처 https://blogs.sas.com/content/saskorea/2017/08/22/%EC%B5%9C%EC%A0%81%EC%9D%98-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%EC%9D%84-%EA%B3%A0%EB%A5%B4%EA%B8%B0-%EC%9C%84%ED%95%9C-%EC%B9%98%ED%8A%B8/"
    } ,
  
    {
      "title"       : "Ai) 강화학습의 개념",
      "category"    : "",
      "tags"        : "",
      "url"         : "./AI)-%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5%EC%9D%98-%EA%B0%9C%EB%85%90.html",
      "date"        : "2022-01-07 00:00:00 +0900",
      "description" : "",
      "content"     : "강화학습에 관련한 유명커뮤니티 https://github.com/reinforcement-learning-kr https://github.com/aikorea/awesome-rl"
    } ,
  
    {
      "title"       : "Python) 파이썬의 디렉터리 관련 표준 라이브러리",
      "category"    : "",
      "tags"        : "",
      "url"         : "./Python)-%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%9D%98-%EB%94%94%EB%A0%89%ED%84%B0%EB%A6%AC-%EA%B4%80%EB%A0%A8-%ED%91%9C%EC%A4%80-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC.html",
      "date"        : "2022-01-06 00:00:00 +0900",
      "description" : "",
      "content"     : "파이썬 디렉터리 관련 표준 라이브러리 sys os globsys definition : 시스템 특정 파라미터와 함수 reference : https://docs.python.org/ko/3/library/sys.htmlos definition : 기타 운영 체제 인터페이스 reference : https://docs.python.org/ko/3.8/library/os.htmlglob definition : 유닉스 스타일 경로명 패턴 확장 reference : https://docs.python.org/ko/3/library/glob.html자주쓰는 함수 sys.path : 현재 폴더와 파이썬 모듈들이 저장되는 위치를 리스트 형태로 반환 sys.path.append() : 자신이 만든 모듈의 경로를 append 함수를 이용해서 추가함. 그 후 추가한 디렉터리에 있는 파이썬 모듈을 불러와 사용할 수 있다. os.chdir() : 디렉터리 위치 변경 os.getcwd() : 현재 자신의 디렉터리 위치를 반환 os.mkdir() : 디렉터리 생성 os.rmdir() : 디렉터리 삭제 (단, 디렉터리가 비어 있을 경우) glob.glob() : 해당 경로 안의 디렉터리나 파일들을 리스트 형태로 반환 os.path.join() : 경로(path)를 병합하여 새 경로 생성 os.listdir() : 디렉터리 안의 파일 및 서브 디렉터리를 리스트 형태로 반환 os.path.exists() : 파일 혹은 디렉터리의 경로 존재 여부 확인 os.path.isfile() : 파일 경로의 존재 여부 확인 os.path.isdir() : 디렉터리 경로의 존재 여부 확인 os.path.getsize() : 파일의 크기 확인출처 AIFFEL LMS"
    } ,
  
    {
      "title"       : "Ai) 붓꽃 분류문제 by scikit Learn",
      "category"    : "",
      "tags"        : "",
      "url"         : "./AI)-%EB%B6%93%EA%BD%83-%EB%B6%84%EB%A5%98%EB%AC%B8%EC%A0%9C-By-scikit-learn.html",
      "date"        : "2022-01-06 00:00:00 +0900",
      "description" : "",
      "content"     : ""
    } ,
  
    {
      "title"       : "통계) 신뢰도, 신뢰구간, 오차범위",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%ED%86%B5%EA%B3%84)-%EC%8B%A0%EB%A2%B0%EB%8F%84,-%EC%8B%A0%EB%A2%B0%EA%B5%AC%EA%B0%84,-%EC%98%A4%EC%B0%A8%EB%B2%94%EC%9C%84.html",
      "date"        : "2022-01-05 00:00:00 +0900",
      "description" : "",
      "content"     : "신뢰도랑 신뢰구간, 표준오차 이런 개념을 읽고왔는데,tip by day에서 friday의 신뢰구간이 높은 이유가뭘까요?제가 고등학생때 통계배울때는 신뢰구간이 90%인 혹은 95%인 이런 조건이 바로 주어졌어서, 신뢰구간을 직접 구해본적이 없는데, tips.csv파일에서https://wikidocs.net/86290https://8888-wl9t7m68w14d404q7nop1mvye.e.prod.connect.ainize.ai/edit/seaborn-data/tips.csvhttps://ko.wikihow.com/%EC%8B%A0%EB%A2%B0-%EA%B5%AC%EA%B0%84-%EA%B3%84%EC%82%B0%EB%B2%95 부트스트랩 신뢰구간 https://learnshare.tistory.com/17"
    } ,
  
    {
      "title"       : "추천링크",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%EC%B6%94%EC%B2%9C%EB%A7%81%ED%81%AC.html",
      "date"        : "2022-01-05 00:00:00 +0900",
      "description" : "",
      "content"     : "numpyhttp://jalammar.github.io/visual-numpy/"
    } ,
  
    {
      "title"       : "Ai) visualization에 사용할 수 있는 패키지와 메소드들",
      "category"    : "",
      "tags"        : "",
      "url"         : "./AI)-Visualization%EC%97%90-%EC%82%AC%EC%9A%A9%ED%95%A0-%EC%88%98-%EC%9E%88%EB%8A%94-%ED%8C%A8%ED%82%A4%EC%A7%80%EC%99%80-%EB%A9%94%EC%86%8C%EB%93%9C%EB%93%A4.html",
      "date"        : "2022-01-05 00:00:00 +0900",
      "description" : "",
      "content"     : "출처 AIFFEL LMS"
    } ,
  
    {
      "title"       : "Anaconda) 가상화 환경 만들기",
      "category"    : "",
      "tags"        : "",
      "url"         : "./Anaconda)-%EA%B0%80%EC%83%81%ED%99%94-%ED%99%98%EA%B2%BD-%EB%A7%8C%EB%93%A4%EA%B8%B0.html",
      "date"        : "2022-01-04 00:00:00 +0900",
      "description" : "",
      "content"     : "https://uoa-eresearch.github.io/eresearch-cookbook/recipe/2014/11/20/conda/가상화 환경의 중요성 ai학습시 모델마다 필요하거나 사용할 수있는 패키지가 다를수있다. 이때 각 환경을 계속 바꿔주는걶 ㅣㅁ드므로 가상화환경을 생서아여서 다른패키지를 ㅏㅅ요앟도록하룻잇다."
    } ,
  
    {
      "title"       : "Ai) 정규화가 필요한 이유",
      "category"    : "",
      "tags"        : "",
      "url"         : "./AI)-%EC%A0%95%EA%B7%9C%ED%99%94%EA%B0%80-%ED%95%84%EC%9A%94%ED%95%9C-%EC%9D%B4%EC%9C%A0.html",
      "date"        : "2022-01-04 00:00:00 +0900",
      "description" : "",
      "content"     : "https://dryjelly.tistory.com/145"
    } ,
  
    {
      "title"       : "Ai) 교차검증 cross validation이란",
      "category"    : "",
      "tags"        : "",
      "url"         : "./AI)-%EA%B5%90%EC%B0%A8%EA%B2%80%EC%A6%9D-Cross-validation%EC%9D%B4%EB%9E%80.html",
      "date"        : "2022-01-04 00:00:00 +0900",
      "description" : "",
      "content"     : ""
    } ,
  
    {
      "title"       : "Ai) train data, test data, validation data",
      "category"    : "",
      "tags"        : "",
      "url"         : "./AI)-Train-data,-Test-data,-Validation-Data.html",
      "date"        : "2022-01-04 00:00:00 +0900",
      "description" : "",
      "content"     : "AI 딥러닝에서, 신경망 모델을 학습하고 평가하기 위해 dataset이 필요합니다.이 dataset은 아래 3가지로 나눌 수 있습니다. Train data Validation data Test dataTrain data딥러닝 모델을 학습하기 위한 dataset입니다. 이때, 모델을 학습하는데에는 오직 train data를 이용합니다.보통, 이 데이터 set을 이용하여 각기 다른 모델을 서로 다른 epoch로 학습을 시키며, 여기서 다른 모델은 hidden layer혹은 hyper parameter에 변화를 준 모델들입니다.Validation data와 Test data는Validation dataTest data출처 https://ganghee-lee.tistory.com/38"
    } ,
  
    {
      "title"       : "Ai) mnist를 사용한 손글씨 판별기 성능개선",
      "category"    : "",
      "tags"        : "",
      "url"         : "./AI)-Mnist%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%9C-%EC%86%90%EA%B8%80%EC%94%A8-%ED%8C%90%EB%B3%84%EA%B8%B0-%EC%84%B1%EB%8A%A5%EA%B0%9C%EC%84%A0.html",
      "date"        : "2022-01-04 00:00:00 +0900",
      "description" : "",
      "content"     : "일반적인 딥러닝 과정데이터 준비 → 딥러닝 네트워크 설계 → 학습 → 테스트(평가)Mnist란?http://yann.lecun.com/exdb/mnist/데이터 준비데이터의 종류https://ganghee-lee.tistory.com/38Train Data모델을 학습시키기 위한 data setTest Data학습과 검증이 완료된 모델 성능을 평가하기 위한 data setValidation Data학습이 이미 완료된 모델을 검증하기 위한 data set즉, 각각 데이터의 사용 순서는 Train data기존 모델모델 layer 16이미지 특징을 학습하는 2D CNN, activation=’relu’ 2D Maxpooling층 32 이미지 특징을 학습하는 2D CNN, activation=’relu’ 2D Maxpooling층 Flatten Dense(32), activation = ‘relu’ Dense(10), activation = ‘softmax’ 개선시킨 모델출처 AIFFEL LMS"
    } ,
  
    {
      "title"       : "Ai) mnist를 사용한 손글씨 판별기 성능개선 오류일기",
      "category"    : "",
      "tags"        : "",
      "url"         : "./AI)-Mnist%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%9C-%EC%86%90%EA%B8%80%EC%94%A8-%ED%8C%90%EB%B3%84%EA%B8%B0-%EC%84%B1%EB%8A%A5%EA%B0%9C%EC%84%A0-%EC%98%A4%EB%A5%98%EC%9D%BC%EA%B8%B0.html",
      "date"        : "2022-01-04 00:00:00 +0900",
      "description" : "",
      "content"     : "컬러이미지 -&gt; 흑백 이미지 사용을 위한 코드 수정아이펠 기존 이미지는 컬러이미지인데, 244*244로 이미지가 너무많이 날라갈것을고려하였따.그래서 나는 흑백으로 96?**으로 찍었고, 2828로 reshape해주었다.기존 코드import numpy as npdef load_data(img_path, number_of_data=300): # 가위바위보 이미지 개수 총합에 주의하세요. # 가위 : 0, 바위 : 1, 보 : 2 img_size=28 color=3 #이미지 데이터와 라벨(가위 : 0, 바위 : 1, 보 : 2) 데이터를 담을 행렬(matrix) 영역을 생성합니다. imgs=np.zeros(number_of_dataimg_sizeimg_size*color,dtype=np.int32).reshape(number_of_data,img_size,img_size,color) labels=np.zeros(number_of_data,dtype=np.int32)idx=0for file in glob.iglob(img_path+'/scissor/*.jpg'): img = np.array(Image.open(file),dtype=np.int32) imgs[idx,:,:,:]=img # 데이터 영역에 이미지 행렬을 복사 labels[idx]=0 # 가위 : 0 idx=idx+1for file in glob.iglob(img_path+'/rock/*.jpg'): img = np.array(Image.open(file),dtype=np.int32) imgs[idx,:,:,:]=img # 데이터 영역에 이미지 행렬을 복사 labels[idx]=1 # 바위 : 1 idx=idx+1 for file in glob.iglob(img_path+'/paper/*.jpg'): img = np.array(Image.open(file),dtype=np.int32) imgs[idx,:,:,:]=img # 데이터 영역에 이미지 행렬을 복사 labels[idx]=2 # 보 : 2 idx=idx+1 print(\"학습데이터(x_train)의 이미지 개수는\", idx,\"입니다.\")return imgs, labelsimage_dir_path = os.getenv(“HOME”) + “/aiffel/rock_scissor_paper”(x_train, y_train)=load_data(image_dir_path)x_train_norm = x_train/255.0 # 입력은 0~1 사이의 값으로 정규화print(“x_train shape: {}”.format(x_train.shape))print(“y_train shape: {}”.format(y_train.shape))##a =3 ;"
    } ,
  
    {
      "title"       : "Ai) keras 인공지능 모델 기본 메서드",
      "category"    : "",
      "tags"        : "",
      "url"         : "./AI)-Keras-%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%AA%A8%EB%8D%B8-%EA%B8%B0%EB%B3%B8-%EB%A9%94%EC%84%9C%EB%93%9C.html",
      "date"        : "2022-01-04 00:00:00 +0900",
      "description" : "",
      "content"     : "모델 생성 관련model=keras.models.Sequential()model.add(keras.layers.Conv2D(16, (3,3), activation=’relu’, input_shape=(28,28,1)))model.add(keras.layers.MaxPool2D(2,2))model.add(keras.layers.Conv2D(32, (3,3), activation=’relu’))model.add(keras.layers.MaxPooling2D((2,2)))model.add(keras.layers.Flatten())model.add(keras.layers.Dense(32, activation=’relu’))model.add(keras.layers.Dense(10, activation=’softmax’))print(‘Model에 추가된 Layer 개수: ‘, len(model.layers))모델```출처 AIFFEL LMS"
    } ,
  
    {
      "title"       : "마크다운 ) 수식 작성하는 법",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%EB%A7%88%ED%81%AC%EB%8B%A4%EC%9A%B4-)-%EC%88%98%EC%8B%9D-%EC%9E%91%EC%84%B1%ED%95%98%EB%8A%94-%EB%B2%95.html",
      "date"        : "2022-01-03 00:00:00 +0900",
      "description" : "",
      "content"     : "기본 수식 사용법$$ 수식 $$로 사용합니다.1. 띄어쓰기마크다운 수식 사이에 띄어쓰기를 하고 싶다면 한칸 : \\,\\(start \\, end\\) $$start\\,end$$ \\; 두칸 \\quad 네칸 \\qquad 여덟칸 이라는데 마크다운에선 안먹히네요? 2. 분수 작성하기\\[z\\, score(\\frac{X-μ}{σ})\\]$$z\\, score(\\frac{X-μ}{σ})$$인라인 수식 작성법$ 수식 $로 사용합니다.$ w_1,\\cdots, w_{n-1} $는 다음과 같이 작성합니다.$ w_1,\\cdots, w_{n-1} $"
    } ,
  
    {
      "title"       : "데이터 전처리) 이상치 outlier",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%84%EC%B2%98%EB%A6%AC)-%EC%9D%B4%EC%83%81%EC%B9%98-Outlier.html",
      "date"        : "2022-01-03 00:00:00 +0900",
      "description" : "",
      "content"     : "이상치 Outlier란?대부분의 값의 범위에서 벗어나 극단적으로 크거나 작은 값. 즉, 노이즈 같은것!이상치를 찾는 방법1. z score가장 많이 쓰는 방법으로 평균과 표준 편차를 이용한다.-&gt; z score : 평균을 빼주고 표준 편차로 나누는 방법\\(z\\, score(\\frac{X-μ}{σ})\\) μ : 평균 σ : 표준 편차코드상의 구현 abs(df[col] - np.mean(df[col])) : 데이터 - 평균 의 절대값 abs(df[col] - np.mean(df[col]))/np.std(df[col]) : 표준편차로 나눠줌 df[abs(df[col] - np.mean(df[col]))/np.std(df[col])&gt;z].index : 값이 z보다 큰 데이터 인덱스 추출2. Modified Z-score methodhttp://colingorrie.github.io/outlier-detection.html3. 사분위 범위수 IQR method\\[IQR=Q_3-Q_1\\]과 같이 제 3사분위 수에서 제 1사분위 값을 뺀 값으로 데이터의 중간 50% 범위라고 생각하면 된다.Q1 - 1.5 * IQR보다 왼쪽에 있거나 Q3 + 1.5 * IQR보다 오른쪽에 있는 경우 이상치라고 판단한다.다음은, 이해를 돕기 위한 그림이다.z-score 방법이 가지는 단점 2가지1) Robust하지 못하다 - 왜나하면 평균과 표준편차 자체가 이상치의 존재에 크게 영향을 받기 때문이다.2) 작은 데이터셋의 경우 z-score의 방법으로 이상치를 알아내기 어렵다. 특히 item이 12개 이하인 데이터셋에서는 불가능하다.출처 AIFFEL LMS"
    } ,
  
    {
      "title"       : "데이터 전처리) 결측치 처리 방법 missing data",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%84%EC%B2%98%EB%A6%AC)-%EA%B2%B0%EC%B8%A1%EC%B9%98-%EC%B2%98%EB%A6%AC-%EB%B0%A9%EB%B2%95-Missing-Data.html",
      "date"        : "2022-01-03 00:00:00 +0900",
      "description" : "",
      "content"     : "결측치 Missing Data란?결측값은 입력이 누락된 값을 의미합니다. 즉, 누락된 값을 의미합니다. = 입력칸이 있는데 null인 값.결측치 처리하는 방법 결측치 데이터를 제거 데이터가 너무 없어서 무쓸모일것 같다 -&gt; 제거 결측치를 어떤 값으로 대체 데이터가 대부분 유효한 값이 있는데 한 두가지만 없다 -&gt; 대체 결측치 데이터를 제거drop 메소드를 이용.결측치 데이터를 대체 수치형 데이터의 경우 특정 값으로 설정 결측치가 많은 경우, 모두 특정값으로 대체하면 분산이 작아지는 경우가 발생할 수 있음 평균 / 중앙 값으로 설정 결측치가 많은 경우, 분산이 작아지는 경우가 발생 가능 다른 데이터를 이용한 예측값으로 설정 Q. Gan을 이용하나? 시계열 데이터의 경우 앞 뒤 데이터를 이용하여 데이터 설정 ex. 전 후 데이터의 평균으로 보완 범주형 데이터인 경우 특정 값으로 설정 기타 / 결측 과 같이 새로운 범주를 만들어 결측치를 채움 최빈 값 등으로 대체 결측치가 많은 경우 적합하지 않음 다른 데이터를 이용한 예측값사용 시계열 데이터의 경우 앞 뒤 데이터를 이용하여 데이터 설정 출처 AIFFEL LMS"
    } ,
  
    {
      "title"       : "Window) 윈도우 커맨드창 명령어 정리",
      "category"    : "",
      "tags"        : "",
      "url"         : "./Window)-%EC%9C%88%EB%8F%84%EC%9A%B0-%EC%BB%A4%EB%A7%A8%EB%93%9C%EC%B0%BD-%EB%AA%85%EB%A0%B9%EC%96%B4-%EC%A0%95%EB%A6%AC.html",
      "date"        : "2022-01-03 00:00:00 +0900",
      "description" : "",
      "content"     : "윈도우 커맨드 라인 명령어켜는 법 : 작업표시줄에서 cmd 검색cd : 현재 디렉토리 출력cd 경로 : 디렉토리 변경copy : 파일 복사del : 디렉토리 혹은 파일 지우기dir : 파일과 디렉토리 목록 출력move : 파일을 이동mkdir : 새 디렉터리 생성rmdir : 디렉토리 삭제"
    } ,
  
    {
      "title"       : "숏코딩) 백준 알고리즘 풀이에 사용되는 숏코딩 관련 링크",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%EC%88%8F%EC%BD%94%EB%94%A9)-%EB%B0%B1%EC%A4%80-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%ED%92%80%EC%9D%B4%EC%97%90-%EC%82%AC%EC%9A%A9%EB%90%98%EB%8A%94-%EC%88%8F%EC%BD%94%EB%94%A9-%EA%B4%80%EB%A0%A8-%EB%A7%81%ED%81%AC.html",
      "date"        : "2022-01-03 00:00:00 +0900",
      "description" : "",
      "content"     : "https://computer-choco.tistory.com/397"
    } ,
  
    {
      "title"       : "Todo_pandas) 기본 메서드",
      "category"    : "",
      "tags"        : "",
      "url"         : "./TODO_pandas)-%EA%B8%B0%EB%B3%B8-%EB%A9%94%EC%84%9C%EB%93%9C.html",
      "date"        : "2022-01-03 00:00:00 +0900",
      "description" : "",
      "content"     : "todo : pandas.head() 메서드 검색해야함 pandas.drop(blah, axis??) axis 뭔지 검색 ㄱ pandas.any(blah, axis??) axis 뭔지 검색 ㄱ cf. pandas 객체는 DataFrame 형식입니다.Dataframe은 0번지 인덱스부터 시작합니다.1. 판다스 csv파일 읽어오기import os# csv파일 경로 읽어오기csv_file_path = os.getenv('HOME')+'/aiffel/data_preprocess/data/trade.csv'# 판다스 객체생성trade = pd.read_csv(csv_file_path)trade.head()todo : pandas.head() 메서드 검색해야함2. csv파일에서 필요없는 컬럼 삭제하기# 위 코드의 연장선trade.drop('삭제할 컬럼', axis=1)3. csv파일에서 결측치 확인하기3-1. null값 확인하기null 데이터가 들어간거 출력trade(pandas객체).isnull() csv파일에서 null로 처리된 값이 있다 = true csv파일에서 유효한 값이 있다 = false3-2. csv행에서 null값 존재 여부 확인하기해당 행에서 null이 존재하는지 확인trade(pandas객체).any(axis=1) 행에 null값 존재 : true 행에 null값 없음 : false4. 결측치 제거하기trade(pandas객체).dropna(how='all(모든행을지운다.)', subset=[특정 컬럼 리스트], inplace=True) how ‘all’ : subset에서 선택한 컬럼 전부가 결측치인 행을 삭제 ‘any’ : subset에서 선택한 컬럼 일부에 결측치가 존재하면 삭제 subset 컬럼 선택하는 옵션 inplace True : DataFrame 파일(pandas객체)을 직접적으로 수정한다. False : DataFrame 파일(pandas객체)을 수정 X 5. 원하는 행의 값 불러오기trade(DataFrame 객체).loc[행 라벨, 열 라벨] trade.loc[[행라벨1, 행라벨2]] : 행 리스트를 이용하여 행여러개를 출력 가능 trade.loc[행라벨] : 행 하나만 출력할 때 trade.loc[[행라벨]] : 행 하나만 출력할 때 두개 출력방법이 다른데 왜다름-_-???????? 6. 원하는 행의 열 값 수정하기trade(DataFrame객체).loc[행값, '컬럼이름(열)']=원하는 대입식 NaN 값 주고싶다면 null아닌 None 대입7. 중복 데이터7-1. 중복 데이터 확인하기trade(DataFrame객체).duplicated()7-2 중복 데이터 삭제하기trade(DataFrame객체).drop_duplicates출처 AIFFEL LMS"
    } ,
  
    {
      "title"       : "Todo_ai) 다양한 데이터 전처리 방법",
      "category"    : "",
      "tags"        : "",
      "url"         : "./TODO_AI)-%EB%8B%A4%EC%96%91%ED%95%9C-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%84%EC%B2%98%EB%A6%AC-%EB%B0%A9%EB%B2%95.html",
      "date"        : "2022-01-03 00:00:00 +0900",
      "description" : "",
      "content"     : "링크 붙이기다양한 데이터 전처리 방법결측치 처리중복된 데이터 제거이상치 처리정규화원-핫 인코딩구간화"
    } ,
  
    {
      "title"       : "함수형 프로그래밍이란",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%ED%95%A8%EC%88%98%ED%98%95-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D%EC%9D%B4%EB%9E%80.html",
      "date"        : "2022-01-02 00:00:00 +0900",
      "description" : "",
      "content"     : "함수형 프로그래밍이란? 함수형 프로그래밍의 특징은 문제를 함수로 분해한다는 것이다. 각각의 함수는 입력을 받아 출력을 만들어 내기만 한다. 즉, 주어진 입력값이 함수를 통과 하여 연산을 한 후, 생성된 결과 값은 해당 함수외의 다른 변수나 함수에 의해 변하지 않는다. == 다른 변수나 함수의 영향을 받지 않는다. 효율성, 버그 없는 코드, 병렬 프로그래밍과 같은 장점을 갖는다.함수형 프로그래밍의 특징순수성함수형 프로그램에서 함수는 입력으로 부터 출력을 만들어 내며, 내부의 상태를 수정하거나 함수 내부의 보이지 않는 특정 값을 이용하여 출력을 만들어 내지 않는다.다른 변경사항들이 만드는 부작용 있는 함수를 사용하지 않으며, 이러한 부작용이 없도록 만든 함수를 순수함수라고 한다. 코드를 통한 예시 순수성이 없는 코드 y = 30 def impure_add(x): return x+y print(impure_add(100)) 문제점 1. y=30 이라는 입력을 받지 않고도 결정된 값이 있음 문제점 2. 함수 안에 있는 값 뿐만 아니라 함수 밖에있는 y값을 사용함. 순수성이 지켜진 코드 def pure_add(x,y): return x+y print(pure_add(100,30)) 모듈성모든 입력으로만 출력을 내는 형식으로 프로그래밍을 하면, 문제를 작은 조각으로 분해하도록 강제하기 때문에 코딩하기는 더 쉬워진다.작은 함수 하나 하나는, 가독성도 좋고 오류 확인하기도 쉬워지며 프로그램의 모듈성이 높아진다.디버깅과 테스트 용이성함수형 프로그래밍의 모듈성 덕분에, 이 방법으로 개발된 프로그램은 각각의 함수가 작고 명확하여 디버깅이 쉽다.프로그램이 에러나 오류를 뱉는 경우, 각 함수는 데이터가 올바르게 출력되고 있는지 확인하는 breakpoint들이 된다. 그렇기 때문에 각 함수에 입출력을 확인하며 값이 올바르게 나오고 있는지 확인할 수 있어 디버깅이 쉽다.각 함수는 단위 테스트 unit test 대상이므로 테스팅도 쉽다.출처 AIFFEL LMS"
    } ,
  
    {
      "title"       : "런타임과 컴파일타임",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%EB%9F%B0%ED%83%80%EC%9E%84%EA%B3%BC-%EC%BB%B4%ED%8C%8C%EC%9D%BC%ED%83%80%EC%9E%84.html",
      "date"        : "2022-01-02 00:00:00 +0900",
      "description" : "",
      "content"     : "컴파일 타임 Compile time과 런타임 Run time은 소프트웨어 개발의 서로 다른 계층의 차이를 설명하기 위한 용어이다.컴파일타임 Compile time개발자가 작성한 소스 코드를 컴파일러가 컴파일이라는 과정을 통해 기계어 코드로 번역한다. 이러한 편집 과정을 컴파일타임이라고 한다.런타임 Run time컴파일 과정을 마친 프로그램이 사용자에게 실행 되며, 이 응용 프로그램이 동작되어 지는 때를 런타임이라고 한다.컴파일 타임과 런타임서로 다른 타입의 에러를 나타내기 위하여 사용되어지곤 한다. 컴파일 타임에 일어나는 에러 syntax error type error 런타임에 일어나는 에러 zero division error null point error 출처 https://spaghetti-code.tistory.com/35"
    } ,
  
    {
      "title"       : "Python) 파이썬 람다식 표현식 lambda expression",
      "category"    : "",
      "tags"        : "",
      "url"         : "./Python)-%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EB%9E%8C%EB%8B%A4%EC%8B%9D-%ED%91%9C%ED%98%84%EC%8B%9D-lambda-expression.html",
      "date"        : "2022-01-02 00:00:00 +0900",
      "description" : "",
      "content"     : "람다 표현식 lambda expression 이란?람다란, 런타임에 생성해서 사용할 수 있는 익명 함수입니다. 자바에서 사용하는 익명함수와 비슷한 역할을 하는데, 람다 표현식은 사용법이 좀 더 간결합니다.사용 예시print( (lambda a, b : a * b)(5,6)) 실행 결과 : 30즉, 사용방법은 다음과 같습니다.(lambda 사용할 변수 : 변수를 이용한 수식)(변수에 들어갈 값)map 함수map(함수 f, iterable객체) iterable객체의 요소를 하나씩 함수 f에 넣어줌. 이때 f()가 아니라 f만 (함수 이름만!) 매개변수로 넣어줌.list(map(lambda a,b : a*b ,(5,6),(10,100))) 실행 결과 : [50, 600] print(map~~~)안 한 이유 : map의 주소값을 뱉어내기 때문. 결과값을 뱉어내게 하기 위해 list함수에 map객체를 넣어줬음. list뿐만 아니라 tuple도 가능함 tuple(map(lambda a : a *100,[5])) 실행결과 : (500,) [5] -&gt; (5) 변경은 안됨. tuple로 들어갈 줄 알았는데 int형으로 들어가서 TypeError: int object is not iterable 뜸. [5] -&gt; (5,)로 넣어야 tuple로 들어감. reducefilter더공부할 사이트https://wikidocs.net/64위키독스 람다.출처 AIFFEL LMS"
    } ,
  
    {
      "title"       : "마크다운 ) 토글 만들기   접기, 펼치기 expander control",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%EB%A7%88%ED%81%AC%EB%8B%A4%EC%9A%B4-)-%ED%86%A0%EA%B8%80-%EB%A7%8C%EB%93%A4%EA%B8%B0-%EC%A0%91%EA%B8%B0,-%ED%8E%BC%EC%B9%98%EA%B8%B0-Expander-control.html",
      "date"        : "2021-12-31 00:00:00 +0900",
      "description" : "",
      "content"     : "아래 글을 클릭하세요. 예시 접기 버튼 &lt;pre&gt; 안에 내용은 pre 태그 쓰세요. 이렇게 띄어쓰기도 인식해주고 줄바꿈도 읜식해줘요 ㅠㅠ &lt;/pre&gt;&lt;/div&gt; &lt;/details&gt; &lt;details&gt; &lt;summary&gt;예시 접기 버튼&lt;/summary&gt; &lt;div markdown=\"1\"&gt; &lt;pre&gt; 안에 내용은 pre 태그 쓰세요. 이렇게 띄어쓰기도 인식해주고 줄바꿈도 읜식해줘요 ㅠㅠ &lt;/pre&gt; &lt;/div&gt; &lt;/details&gt;"
    } ,
  
    {
      "title"       : "Python) 파이썬에서 list와 array의 차이",
      "category"    : "",
      "tags"        : "",
      "url"         : "./Python)-%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%97%90%EC%84%9C-list%EC%99%80-array%EC%9D%98-%EC%B0%A8%EC%9D%B4.html",
      "date"        : "2021-12-31 00:00:00 +0900",
      "description" : "",
      "content"     : "List 란?동적 배열(Dynamic Array)의 데이터 자료 구조 데이터들이 떨어진 위치에 저장되며 pointer로 연결된다. pointer : 해당 데이터의 다음 데이터 주소값을 가리키고 있다. 임의 접근 불가 순차 접근 / 시퀀셜 액세스 Sequential Access를 이용해야한다. Array 란?연속(Sequence)형 데이터의 자료구조 데이터들이 연속된 메모리 영역에 순차적으로 저장 임의 접근 가능 : 인덱스 번호를 이용해서 빠르게 접근List와 Array의 차이 시간 계산 Type Read Write/Update/Delete Array O(1) O(n) List O(n) O(1) 데이터를 읽을때는 arraylist가 빠르지만 데이터를 입력/삭제/수정할때는 리스트 자료형이 더 빠르다. 담을 수 있는 자료형 list element사이에 여러가지 자료형을 넣을 수 있다. array 단 한가지의 자료형만 허락하며 선언시 어떤 자료형을 사용할지 옵션으로 준다. arr.array('i', [1, 2, 3]) ‘i’ : integer 자료형으로만 추가 가능 다른 옵션 참고 시 클릭 [1,2,3] : 실제 array의 데이터들.. 사용 방법 list 따로 모듈 추가 없이 바로 사용이 가능하다. array import array 로 모듈을 import 해주어야 사용이 가능하다. 출처 AIFFEL LMS https://blog.martinwork.co.kr/theory/2018/09/22/what-is-difference-between-list-and-array.html"
    } ,
  
    {
      "title"       : "Python) format()",
      "category"    : "",
      "tags"        : "",
      "url"         : "./Python)-format().html",
      "date"        : "2021-12-31 00:00:00 +0900",
      "description" : "",
      "content"     : "문자열 포맷팅 text formatting 기본적인 사용 방법 &gt;&gt;&gt; print(\"hi, my name is {}\".format(\"호이얏\"))hi, my name is 호이얏&gt;&gt;&gt; print(\"{} is a horse in korean\".format(\"말\"))말 is a horse in korean 인덱스 사용 &gt;&gt;&gt; print(\"책 제목 : {1}, 출판사 : {0}, 가격 : {2}\".format(\"길벗\", \"이 책은 누구의 책이냐?\", 9500))책 제목 : 이 책은 누구의 책이냐?, 출판사 : 길벗, 가격 : 9500&gt;&gt;&gt; print(\"a: {3}, b: {1}, c:{0}\".format(3,2,\"-\",1))a: 1, b: 2, c:3 변수 지정 &gt;&gt;&gt; print(\"책 제목 : {a}, 출판사 : {b}, 가격 : {c}\".format(b=\"길벗\", a=\"이 책은 누구의 책이냐?\", c=9500))책 제목 : 이 책은 누구의 책이냐?, 출판사 : 길벗, 가격 : 9500&gt;&gt;&gt; print(\"a: {a}, b: {c}, c:{d}\".format(d=3,c=2,b=\"-\",a=1))a: 1, b: 2, c:3 형식 변경 &gt;&gt;&gt; format(10000,\",\")'10,000' 진수 변환 10진수 -&gt; 16진수 \"{0:x}\".format(10진수) &gt;&gt;&gt; \"{0:x}\".format(1669)'685'&gt;&gt;&gt; \"{0:x}\".format(9999)'270f'&gt;&gt;&gt; \"{0:x}\".format(1111)'457'&gt;&gt;&gt; \"{0:x}\".format(1000)'3e8' 10진수 -&gt; 8진수 \"{0:o}\".format(10진수) &gt;&gt;&gt; \"{0:o}\".format(1669)'3205'&gt;&gt;&gt; \"{0:o}\".format(9999)'23417'&gt;&gt;&gt; \"{0:o}\".format(1111)'2127'&gt;&gt;&gt; \"{0:o}\".format(1000)'1750' 10진수 -&gt; 2진수 \"{0:b}\".format(10진수) &gt;&gt;&gt; \"{0:b}\".format(1669)'11010000101'&gt;&gt;&gt; \"{0:b}\".format(9999)'10011100001111'&gt;&gt;&gt; \"{0:b}\".format(1111)'10001010111'&gt;&gt;&gt; \"{0:b}\".format(1000)'1111101000'"
    } ,
  
    {
      "title"       : "Linux) 리눅스 단축키",
      "category"    : "",
      "tags"        : "",
      "url"         : "./Linux)-%EB%A6%AC%EB%88%85%EC%8A%A4-%EB%8B%A8%EC%B6%95%ED%82%A4.html",
      "date"        : "2021-12-31 00:00:00 +0900",
      "description" : "",
      "content"     : "복사Ctrl + Insert 또는 Ctrl + Shift + C붙여넣기Shift + Insert 또는 Ctrl + Shift + V"
    } ,
  
    {
      "title"       : "데이터 전처리) 정규화 normalization",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%84%EC%B2%98%EB%A6%AC)-%EC%A0%95%EA%B7%9C%ED%99%94-Normalization.html",
      "date"        : "2021-12-31 00:00:00 +0900",
      "description" : "",
      "content"     : "정규화모델을 학습시킬 때 입력 데이터를 통일화 시켜주는 것은 중요합니다. 예를 들어, 데이터의 범위가 0 - 1까지인 데이터와 10000-100000인데이터가 있을 경우, 이런 데이터를 클러스터링 한다고 할 경우, 데이터간의 범위를 잴 때 범위가 큰 후자의 데이터에만 영향을 크게 받을 것입니다.이렇게 데이터간의 범위가 크게 다를 경우, 전처리 과정에서 데이터를 정규화 합니다. 보통 많이 알려진 방법은 표준화와 Min-Max Scaling이 있습니다.정규화 방법Standardization데이터의 평균을 0, 분산을 1로 변환합니다.\\(\\frac{X-μ}{σ}\\)# x를 dataframe이라 할때x_standardization = (x - x.mean())/x.std()x_standardizationMin-Max Scaling데이터의 최솟값은 0, 최댓값은 1로 변환\\(\\frac{X-X_{min}}{X_{max}-X_{min}}\\)# 데이터 x를 min-max scaling 기법으로 정규화합니다. x_min_max = (x-x.min())/(x.max()-x.min())x_min_max그밖 : 로그변환https://www.youtube.com/watch?v=FDCfw-YqWTE&amp;feature=emb_logo여기 더 공부하자.참고자료 : https://www.youtube.com/channel/UCcIXc5mJsHVYTZR1maL5l9w출처 AIFFEL LMS"
    } ,
  
    {
      "title"       : "프로그래밍 기법) 메모이제이션 memoization",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D-%EA%B8%B0%EB%B2%95)-%EB%A9%94%EB%AA%A8%EC%9D%B4%EC%A0%9C%EC%9D%B4%EC%85%98-memoization.html",
      "date"        : "2021-12-30 00:00:00 +0900",
      "description" : "",
      "content"     : "메모이제이션 memoization프로그래밍에서 중간 계산 값을 변수에 저장해놓는다. 반복되는 계산을 하게 되는 부분은 코딩상으로 계속 계산하게 하는 것 대신에 중간 계산 값을 읽어서 사용하여 계산 시간을 줄이는 기법이다.예시. 나쁜 예시def fibonacci(n): if n &lt;= 2: return 1 else: return fibonacci(n-2) + fibonacci(n-1) for i in range(1,20): print(fibonacci(i))예시. 좋은 예시memory = {1: 1, 2: 1}def fibonacci(n): if n in memory: number = memory[n] else: number = fibonacci(n-1) + fibonacci(n-2) memory[n] = number return number print(fibonacci(20)) print(memory)나쁜 예시에서는 fibonacci(1), fibonacci(2)가 들어갈때는 바로 if문에서 끝나게 된다.하지만, fibonacci(2+n)의 단계에서 부터는 1, 1, 이라는 값이 고정인데도 불구하고 else문에서 재귀함수로 fibonacci를 두번이나 불러주게 된다. 이렇게 재귀로 함수를 두번 부르는 것 보다는 차라리 중간 계산 값(좋은 예시에서 memory)을 지정해놓고 fibonacci 계산을 할때 변수 값을 사용(읽어서)하는게 처리 속도가 더 빠르다.출처 AIFFEL LMS"
    } ,
  
    {
      "title"       : "Python) 프로그램 실행시간 재기",
      "category"    : "",
      "tags"        : "",
      "url"         : "./Python)-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%A8-%EC%8B%A4%ED%96%89%EC%8B%9C%EA%B0%84-%EC%9E%AC%EA%B8%B0.html",
      "date"        : "2021-12-30 00:00:00 +0900",
      "description" : "",
      "content"     : "프로그램의 실행 시간을 재려면 time모듈을 사용하면 됩니다.import timestart = time.time()for i in range(10000): print(\" hi \", end=' ')print(\"\\nend time =\", time.time()-start)실행 결과 hi hi hi hi hi hi hi hi hi hi hi hi hi …. end time = 0.10024499893188477import timestart = time.time()for i in range(5000): print(\" hi \", end=' ')print(\"\\nend time =\", time.time()-start)실행 결과 hi hi hi hi hi hi hi hi hi hi hi hi hi …. end time = 0.05303597450256348출처 AIFFEL LMS"
    } ,
  
    {
      "title"       : "Python) 파이썬으로 사용하는 json파일",
      "category"    : "",
      "tags"        : "",
      "url"         : "./Python)-%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%9C%BC%EB%A1%9C-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94-JSON%ED%8C%8C%EC%9D%BC.html",
      "date"        : "2021-12-30 00:00:00 +0900",
      "description" : "",
      "content"     : "JSON 이란?JSON이란 JavaScript Object Notation의 약자로, JavaScript의 데이터 객체 표현 방식입니다. 웹과 다른 애플리케이션 사이에서 HTTP 요청으로 데이터를 보낼 때 사용하는 표준 파일 포맷 중 하나입니다.XML과 JSON 형식을 많이 쓰는데 특히 웹 API나 config 데이터를 전송할 때 많이 사용됩니다. JSON 형식의 예시 book = { \"title\" : \"홍길동전\", \"author\" : \"허균\", \"price\" : 8500, \"publisher\" : [{\"name\":\"나라\", \"telephone\" : \"02-123-4567\"}]} json 파싱파이썬에서 Dictionary를 사용한다.저장import jsonbook = { \"title\" : \"홍길동전\", \"author\" : \"허균\", \"price\" : 8500, \"publisher\" : [{\"name\":\"나라\", \"telephone\" : \"02-123-4567\"}]}with open(\"book.json\", \"w\") as f: json.dump(book , f)읽기import jsonwith open(\"book.json\", \"r\", encoding=\"utf-8\") as f: contents = json.load(f) print(contents[\"title\"]) print(contents[\"publisher\"])실행결과 홍길동전 [{“name”:”나라”, “telephone” : “02-123-4567”}]용어정리 config 데이터란? 컴피그레이션(configuration)이란 설정을 의미합니다. 컴퓨터 서버 네트워크 기기 또는 OS 소프트웨어 등의 설정을 가리킵니다. 출처 AIFFEL LMS"
    } ,
  
    {
      "title"       : "Python) 파이썬 모듈 및 패키지, 라이브러리의 개념",
      "category"    : "",
      "tags"        : "",
      "url"         : "./Python)-%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EB%AA%A8%EB%93%88-%EB%B0%8F-%ED%8C%A8%ED%82%A4%EC%A7%80,-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC%EC%9D%98-%EA%B0%9C%EB%85%90.html",
      "date"        : "2021-12-30 00:00:00 +0900",
      "description" : "",
      "content"     : "모듈특정 기능들(함수, 변수, 클래스 등)이 구현되어 있는 파일을 의미합니다.즉, 파이썬으로 만든 코드가 들어간 파일 = .py패키지특정 기능과 관련된 여러 모듈들을 하나의 상위 폴더에 넣어놓은 것다르게 말하면 기능적으로 동일하거나 동일한 결과를 만드는 모듈들의 집합 또는 폴더.거의 모든 패키지는 pip 명령어를 이용하여 설치할 수 있다.pip install 패키지이름==버전 혹은 pip install 패키지이름라이브러리여러 모듈과 패키지를 묶어 라이브러리라고 합니다.즉, 모듈들과 패키지의 집합.특정 기능을 위한 여러 함수나 클래스를 담고 있는 보따리라고 할 수 있다.Ex. tensorflow, pytorch, keras, numpy 등출처 AIFFEL LMS"
    } ,
  
    {
      "title"       : "Python) 세트 set란",
      "category"    : "",
      "tags"        : "",
      "url"         : "./Python)-%EC%84%B8%ED%8A%B8-set%EB%9E%80.html",
      "date"        : "2021-12-30 00:00:00 +0900",
      "description" : "",
      "content"     : "1. 세트란?파이썬 자료형 중에는 수학적 의미를 가진 ‘집합’의 개념이 존재합니다. 이를 파이썬에서 세트라고 합니다.자료형이란?Data Type이라고도 하며, 프로그래밍 언어에서 정수(integer), 실수(float), 문자(character), 문자열(string), 부울(Boolean : True, False 값을 가지는 데이터) 와 같은 여러 종류의 데이터를 식별하는 분류입니다.자료형은 컴퓨터와 프로그래머에게 어떤 종류의 자료를 다루고 있는지 알려줍니다. 자료형에 따라 사용할 수 있는 연산이나 값이 제한됩니다. (ex. string - string은 안됨 : “abc”-“a” 불가 / integer-integer는 가능 : 456-3 은가능)1.1 세트 자료형의 선언변수 = {값1, 값2, 값3}{ } 중괄호 안에 값을 저장하며 콤마,로 구분해 줍니다.set()set( 반복 가능한 iterable 객체 ) : return 세트 자료형의 데이터&gt;&gt;&gt; result = set('banana')&gt;&gt;&gt; result 실행결과 {‘a’, ‘b’, ‘n’}&gt;&gt;&gt; result = set(range(3))&gt;&gt;&gt; result 실행결과 {0, 1, 2}&gt;&gt;&gt; result = set() # 빈 세트 만드는 법.&gt;&gt;&gt; result 실행결과 set()result = set()  는 result={}와 같지 않음 XXXXXX dictionary로 선언되기 때문1.2 세트 자료형의 특징 순서가 정해져 있지 않음. 데이터 값 중복 불가. -&gt; 중복 데이터를 넣으면, 중복 데이터는 무시됨. 특정 요소만 출력 불가. 전체만 출력 가능 세트안에 세트 넣기 불가. 내용을 변경할 수 없는 세트도 있음 형식 : a = frozenset(range(10)) set안에 frozenset 넣는것은 가능 frozenset 안에 frozenset 넣는것도 가능 ex 1. result={frozenset(\"123\"), frozenset(\"456\")} ex 2. result = frozenset({frozenset(\"123\"), frozenset(\"456\"), \"789\"}) 1.3 세트 연산 = 집합 연산1.3.1 합집합or 연산자를 사용하거나 set.union(세트1, 세트2)메서드 사용| 연산자 사용&gt;&gt;&gt; a = {1, 2, 3}&gt;&gt;&gt; b = {4, 5, 6}&gt;&gt;&gt; a | b # 혹은 a |= b a = a|b 의 축약형실행 결과 {1, 2, 3, 4, 5, 6}set.union()사용&gt;&gt;&gt; a = {1, 2, 3}&gt;&gt;&gt; b = {4, 5, 6}&gt;&gt;&gt; set.union(a,b)실행 결과 {1, 2, 3, 4, 5, 6}세트1.update(세트2)사용&gt;&gt;&gt; a = {1, 2, 3}&gt;&gt;&gt; b = {4, 5, 6}&gt;&gt;&gt; a.update(b) # a에 결과가 저장됨.실행 결과 {1, 2, 3, 4, 5, 6}1.3.2 교집합&amp; 연산자 사용&gt;&gt;&gt; a = {1, 2, 3}&gt;&gt;&gt; b = {4, 5, 6}&gt;&gt;&gt; a &amp; b # 혹은 a &amp;= b a = a &amp; b의 축약형실행 결과 set()set.intersection(a,b) 사용&gt;&gt;&gt; a = {1, 2, 3}&gt;&gt;&gt; b = {4, 5, 6}&gt;&gt;&gt; set.intersection(a, b)실행 결과 set()세트1.intersection_update(세트2) 사용&gt;&gt;&gt; x = {1, 2, 3}&gt;&gt;&gt; y = {2, 3, 4}&gt;&gt;&gt; x.intersection_update(y) # x에 결과가 저장됨.실행 결과 {2, 3}1.3.3 차집합-연산자 사용&gt;&gt;&gt; x = {1, 2, 3}&gt;&gt;&gt; y = {2, 3, 4}&gt;&gt;&gt; x - y # 혹은 x -= y실행 결과 {1}set.difference(a,b) 사용&gt;&gt;&gt; x = {1, 2, 3}&gt;&gt;&gt; y = {2, 3, 4}&gt;&gt;&gt; set.difference(x,y)실행 결과 {1}세트1.difference_update(세트2)&gt;&gt;&gt; x = {1, 2, 3}&gt;&gt;&gt; y = {2, 3, 4}&gt;&gt;&gt; x.difference_update(y) #x에 결과가 저장됨실행 결과 {1}1.3.4 대칭차집합 a b^ 연산자 사용&gt;&gt;&gt; x = {1, 2, 3}&gt;&gt;&gt; y = {2, 3, 4}&gt;&gt;&gt; x ^ y # 혹은 x ^= y x = x^y축약형실행 결과 {1, 4}set.symmetric_difference() 메서드 사용&gt;&gt;&gt; x = {1, 2, 3}&gt;&gt;&gt; y = {2, 3, 4}&gt;&gt;&gt; set.symmetric_difference()실행 결과 {1, 4}세트1.symmetric_difference_update(세트2)&gt;&gt;&gt; x = {1, 2, 3}&gt;&gt;&gt; y = {2, 3, 4}&gt;&gt;&gt; x.symmetric_difference_update(y) # x에 결과가 저장됨실행 결과 {1, 4}1.3.5 부분집합&lt;= 사용&gt;&gt;&gt; i = { 1, 2, 3, 4}&gt;&gt;&gt; j = {2, 3}&gt;&gt;&gt; i &lt;= jFalse&gt;&gt;&gt; j &lt;= iTrue⊂세트1.issubset(세트2)&gt;&gt;&gt; i = { 1, 2, 3, 4}&gt;&gt;&gt; j = {2, 3}&gt;&gt;&gt; i.issubset(j)False&gt;&gt;&gt; j.issubset(i)True1.3.6 진부분집합&lt; 사용&gt;&gt;&gt; a= {1,2,3}&gt;&gt;&gt; b={2}&gt;&gt;&gt; a&lt;bFalse&gt;&gt;&gt; b&lt;aTrue1.3.7 상위 집합&gt;= 사용&gt;&gt;&gt; i = {1, 2, 3, 4}&gt;&gt;&gt; j = {2, 3}&gt;&gt;&gt; i&gt;=jTrue&gt;&gt;&gt; j&gt;=iFalse&gt;&gt;&gt; 세트1.issuperset (세트2)&gt;&gt;&gt; i = {1, 2, 3, 4}&gt;&gt;&gt; j = {2, 3}&gt;&gt;&gt; i.issuperset(j)True&gt;&gt;&gt; j.issuperset(i)False1.4 세트 자료형의 활용1.4.1 데이터 검색값 in 세트변수&gt;&gt;&gt; alphabet={'a', 'b', 'c', 'd', 'e'}&gt;&gt;&gt; 'c' in alphabet실행결과 True&gt;&gt;&gt; alphabet={'a', 'b', 'c', 'd', 'e'}&gt;&gt;&gt; '5' in alphabet실행결과 False값 not in 세트&gt;&gt;&gt; alphabet={'a', 'b', 'c', 'd', 'e'}&gt;&gt;&gt; 'c' not in alphabet실행결과 False&gt;&gt;&gt; alphabet={'a', 'b', 'c', 'd', 'e'}&gt;&gt;&gt; '5' not in alphabet실행결과 True1.4.2 데이터 추가add()&gt;&gt;&gt; alphabet={'a', 'b', 'c', 'd', 'e'}&gt;&gt;&gt; alphabet.add(5)&gt;&gt;&gt; alphabet실행결과 {‘b’, 5, ‘a’, ‘e’, ‘d’, ‘c’}1.4.3 데이터 삭제remove()특정 요소 삭제 시, 요소가 없으면 에러를 발생시킴.discard()특정 요소 삭제 시, 요소가 없어도 에러 발생 Xpop()임의 요소를 삭제한다.clear()모든 요소를 삭제한다.1.4.4 데이터 갯수 세기len()함수를 사용합니다.1.5 세트 자료형의 할당과 복사RAM : Random Access Memorya 세트를 b 변수에 복사하고 싶다면 어떻게 하면 될까요? a를 선언해주고, b = a로 할당해주면 되는 걸까요?실상은 아닙니다.id(매개변수) 메서드는 매개변수의 고유 주소 값( 객체가 메모리에 위치해 있는 주소 )를 나타냅니다.a 세트 변수를 b에 복사 하고 싶다는 것은, a 세트 객체와 b세트 객체가 따로 존재해야 한다는 것을 의미합니다. 그렇게 되면, a의 주소 값과 b의 주소 값이 달라야 하는데 위를 보면 같은 메모리 번지를 참고 하고 있습니다. 하나의 객체를 가리키는 방법이 하나 더 늘어난 것 뿐이라는 의미입니다.즉, a와 b는 같은 객체를 가리키고 있으므로 a가 수정되면 b도 수정되게 됩니다. 다음과 같이요.a 와 b를 독립적인 다른 객체로 만들려면 copy()메서드를 사용하여 모든 요소를 복사해야 합니다. 이를 사용하여 객체를 변경하게 되면 a와 b는 독립적인 객체이므로, a의 변화가 b에 영향을 주지 않습니다.1.6 세트 표현식for을 이용한 세트 생성if를 이용한 세트 생성퀴즈답 : b답 : B, E답 : c, d답 : a,c 출처 https://dojang.io/mod/page/view.php?id=2314 - 주 내용 https://ko.wikipedia.org/wiki/%EC%9E%90%EB%A3%8C%ED%98%95 - 자료형"
    } ,
  
    {
      "title"       : "Python) 리스트 컴프리헨션(list comprehension)",
      "category"    : "",
      "tags"        : "",
      "url"         : "./Python)-%EB%A6%AC%EC%8A%A4%ED%8A%B8-%EC%BB%B4%ED%94%84%EB%A6%AC%ED%97%A8%EC%85%98(list-comprehension).html",
      "date"        : "2021-12-30 00:00:00 +0900",
      "description" : "",
      "content"     : "리스트 컴프리헨션 List Comprehension이란?리스트, 셋(set), 딕셔너리에 사용할 수 있는 기능으로, 기존의 이중 포문을 이용해서 선언해주어야 했던 변수를 한줄로 끝낼 수 있다.list_1 = [1,2]list_2 = [i for i in range(100,500,100)]result_list = []for i in list_1: for j in list_2: result_list.append((i,j)) print(result_list)실행 결과 [(1, 100), (1, 200), (1, 300), (1, 400), (2, 100), (2, 200), (2, 300), (2, 400)]위와 같은 코드가 아래와 같이 짧아진다.list_1 = [1,2]list_2 = [i for i in range(100,500,100)]result_list = [(i,j) for i in list_1 for j in list_2]print(result_list)실행 결과 [(1, 100), (1, 200), (1, 300), (1, 400), (2, 100), (2, 200), (2, 300), (2, 400)]"
    } ,
  
    {
      "title"       : "Python) for문 심화   enumerate",
      "category"    : "",
      "tags"        : "",
      "url"         : "./Python)-for%EB%AC%B8-%EC%8B%AC%ED%99%94-enumerate.html",
      "date"        : "2021-12-30 00:00:00 +0900",
      "description" : "",
      "content"     : "enumerate란?리스트, 문자열, 튜플 등이 있는 경우 순서와 리스트의 값을 함께 반환해주는 기능입니다.&gt;&gt;&gt; list = [i for i in range(5)]&gt;&gt;&gt; list[0, 1, 2, 3, 4]&gt;&gt;&gt; for i, value in enumerate(list): print(i,\"번째 값 : \", value) 0 번째 값 : 01 번째 값 : 12 번째 값 : 23 번째 값 : 34 번째 값 : 4사용법 핵 간편!"
    } ,
  
    {
      "title"       : "Linux) 리눅스 명령어 정리",
      "category"    : "",
      "tags"        : "",
      "url"         : "./Linux)-%EB%A6%AC%EB%88%85%EC%8A%A4-%EB%AA%85%EB%A0%B9%EC%96%B4-%EC%A0%95%EB%A6%AC.html",
      "date"        : "2021-12-30 00:00:00 +0900",
      "description" : "",
      "content"     : "리눅스 명령어abc순으로 정리되어있으며 ctrl + f해서 찾으세요.cd : change directory경로 이동cp : copy cp -r 복제할폴더이름 : 디렉토리 복사 할 때 -r 옵션이 필수이다.grep : 특정 문자열을 찾고자할 때특정 문자열을 찾고자할 때 사용하는 명령어 정규표현식을 사용하여 보통 많이 서칭합니다. 정규 표현식 메타 문자 및 설명&lt;/summay&gt; &lt;table border = 1&gt; &lt;th&gt;메타 문자 (Meta Character&gt;&lt;/th&gt; &lt;th&gt;설명&lt;/th&gt; &lt;tr&gt; &lt;td&gt;.&lt;/td&gt; &lt;td&gt;1개의 문자 매치 (정확히 1개의 문자와 매치)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;*&lt;/td&gt; &lt;td&gt;앞 문자가 0회 이상 매치&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;{n}&lt;/td&gt; &lt;td&gt;앞 문자가 정확히 n회 매치&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;{n,m}&lt;/td&gt; &lt;td&gt;앞 문자가 n회 이상 m회 이하 매치&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;[ ]&lt;/td&gt; &lt;td&gt;대괄호에 포함된 문자 중 한개와 매치&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;[^ ]&lt;/td&gt; &lt;td&gt;대괄호 안에서 ^뒤에 있는 문자들을 제외&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;[ - ]&lt;/td&gt; &lt;td&gt;대괄호 안 문자 범위에 있는 문자들 매치&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;()&lt;/td&gt; &lt;td&gt;표현식을 그룹화&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;^&lt;/td&gt; &lt;td&gt;문자열 라인의 처음&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;$&lt;/td&gt; &lt;td&gt;문자열 라인의 마지막&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;?&lt;/td&gt; &lt;td&gt;앞 문자가 0 또는 1회 매치 (확장 정규 표현식)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;+&lt;/td&gt; &lt;td&gt;앞 문자가 1회 이상 매치 (확장 정규 표현식)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;|&lt;/td&gt; &lt;td&gt;표현식 논리 OR (확장 정규 표현식)&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/details&gt; grep 명령어 옵션 &lt;pre&gt; grep [OPTION...] PATTERN [FILE...] -E : PATTERN을 확장 정규 표현식(Extended RegEx)으로 해석. -F : PATTERN을 정규 표현식(RegEx)이 아닌 일반 문자열로 해석. -G : PATTERN을 기본 정규 표현식(Basic RegEx)으로 해석. -P : PATTERN을 Perl 정규 표현식(Perl RegEx)으로 해석. -e : 매칭을 위한 PATTERN 전달. -f : 파일에 기록된 내용을 PATTERN으로 사용. -i : 대/소문자 무시. -v : 매칭되는 PATTERN이 존재하지 않는 라인 선택. -w : 단어(word) 단위로 매칭. -x : 라인(line) 단위로 매칭. -z : 라인을 newline(\\n)이 아닌 NULL(\\0)로 구분. -m : 최대 검색 결과 갯수 제한. -b : 패턴이 매치된 각 라인(-o 사용 시 문자열)의 바이트 옵셋 출력. -n : 검색 결과 출력 라인 앞에 라인 번호 출력. -H : 검색 결과 출력 라인 앞에 파일 이름 표시. -h : 검색 결과 출력 시, 파일 이름 무시. -o : 매치되는 문자열만 표시. -q : 검색 결과 출력하지 않음. -a : 바이너리 파일을 텍스트 파일처럼 처리. -I : 바이너리 파일은 검사하지 않음. -d : 디렉토리 처리 방식 지정. (read, recurse, skip) -D : 장치 파일 처리 방식 지정. (read, skip) -r : 하위 디렉토리 탐색. -R : 심볼릭 링크를 따라가며 모든 하위 디렉토리 탐색. -L : PATTERN이 존재하지 않는 파일 이름만 표시. -l : 패턴이 존재하는 파일 이름만 표시. -c : 파일 당 패턴이 일치하는 라인의 갯수 출력. &lt;/pre&gt;&lt;/div&gt; &lt;/details&gt; ls : list 디렉토리 목록 확인 -R 옵션 : 하위 디렉토리까지 출력한다. https://akdl911215.tistory.com/201 ln : link 한 파일을 다른 파일 이름으로도 사용하고자 할 때 사용 -s : 심볼릭 링크 생성 ln -s &lt;TARGET&gt; &lt;LINK_NAME&gt; 윈도우에서 바로가기 같은 느낌 mkdir : make directory 디렉토리 생성 -p 옵션 : 하위 디렉토리까지 한번에 생성 가능 $ mkdir -p a/b/c $ ls -R a/ a/: b/ a/b: c/ a/b/c: mv : move mv 옮길파일경로 옮겨질경로pwd : print working directory현재 작업중인 디렉토리 정보 출력rm : remove 디렉토리 삭제 : rm -r -r : 하위 디렉토리와 파일까지 모두 삭제하라는 것을 의미한다. 일반 파일 삭제 : rm"
    } ,
  
    {
      "title"       : "Sercret_왜 파이썬인가",
      "category"    : "",
      "tags"        : "",
      "url"         : "./SERCRET_%EC%99%9C-%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%9D%B8%EA%B0%80.html",
      "date"        : "2021-12-30 00:00:00 +0900",
      "description" : "",
      "content"     : "프로그래밍에서 가장 중요하게 고려해야 할 것 - 퍼포먼스, 생산성퍼포먼스먼저 퍼포먼스(성능) 란 어떤 언어든 코드를 짜서 실행을 시켰을 때 얼마나 빨리 처리가 되는가를 말합니다.자전거와 자동차를 이용해 좀 더 자세히 설명해 보겠습니다. 자전거는 사람이 엔진이 되어 움직이는 이동 수단입니다. 그리고 자동차는 기계가 엔진으로 있는 성능이 매우 좋은 이동 수단입니다. 성능이 좋고 나쁘고는 이동수단이 얼마나 빨리 움직이는지에 따라 정해집니다. 자전거를 타고 가는 것보다는 자동차를 타고 움직이는 것이 훨씬 빠르겠죠?언어도 똑같습니다. 퍼포먼스가 좋은 언어는 특정한 연산을 빠르게 수행하고 퍼포먼스가 안 좋은 언어는 연산이 느리게 수행되게 되죠.아래는 언어별 특정 연산에 대한 수행 속도 결과입니다.위를 보시면 C 언어가 퍼포먼스 상위를 차지하고 있고 파이썬은 상대적으로 하위에 위치에 있는 것을 보실 수 있습니다.생산성그렇다면 생산성은 무엇을 의미할까요?답은 시간에 있습니다. 시간은 항상 우리에게 충분히 주어지지 않을 때가 더 많습니다. 생산성이란 한 마디로 똑같은 기능을 하는 프로그램을 얼마나 빨리 작성할 수 있는가 입니다.빠른 시간 안에 기능을 구현해야 할 때, 높은 생산성은 큰 의미가 있습니다.예를 들어, 저희가 사진을 분류하는 프로그램을 만든다고 해봅시다. 같은 기능을 가진 프로그램이라도 파이썬으로는 일주일 만에 짤 수도 있지만, C++로는 한 달이 걸릴 수도 있습니다. 훨씬 많은 코드를 쳐서 만들어야 하죠. 퍼포먼스 VS 생산성 ?C와 파이썬만 보더라도 이렇게 생산성이 올라가면 퍼포먼스가 떨어지고, 퍼포먼스가 올라가면 생산성이 떨어지는 trade-off가 상당한 걸 보실 수 있습니다. 그렇다면 생산성과 퍼포먼스 중에서는 어떤 것을 우선적으로 선택해야 할까요?생산성과 퍼포먼스가 모두 뛰어난 언어를 선택하면 좋겠지만 사실 생산성이 좋은 언어들은 퍼포먼스가 떨어지고 퍼포먼스가 좋은 언어는 생산성이 떨어집니다. 이런 상황에서 우리는 선택을 해야 합니다. 그래서 어떤 언어를 써야 돼?목적에 맞게, 상황에 맞게 언어를 선택하고 사용하는 방법을 배우면 됩니다.실무에서는 먼저 회사의 각 프로그램들에서 기존에 사용하고 있는 언어를 가장 먼저 고려하고, 그다음에는 개발하고자 하는 프로젝트의 성능과 개발 기간을 고려해서 언어를 정하게 됩니다.결국 성능도 고려하고 시간도 고려해야 한다는 말이네요. trade-off에 있는 두 가지를 잘 배합하여 상황에 따라 적절한 언어를 골라야 합니다."
    } ,
  
    {
      "title"       : "Sercret_generator",
      "category"    : "",
      "tags"        : "",
      "url"         : "./SERCRET_GENERATOR.html",
      "date"        : "2021-12-30 00:00:00 +0900",
      "description" : "",
      "content"     : "제너레이터(Generator)https://dojang.io/mod/page/view.php?id=2412머신러닝을 하면 매우 많은 데이터를 다루게 됩니다. 데이터는 1건만 존재하는 법은 없기 때문에 우리는 데이터를 처리하는 반복 구조를 위해 for문을 떠올리게 될 것입니다. 위에서 살펴본 코드를 다시 한번 가져와 봅시다.우리는 my_list에 있는 데이터셋을 하나씩 가져와서 공급해 주는 제너레이터를 만들 것입니다. my_list를 총 2번 반복하여 8개의 데이터를 공급할 계획이죠단순히 for문을 사용해서 데이터를 공급해 보도록 하는 첫 번째 코드와, Generator의 개념을 이용하여 데이터를 공급하는 두 번째 코드를 비교해 봅시다.```my_list = [‘a’,’b’,’c’,’d’]my_list = [‘a’,’b’,’c’,’d’]인자로 받은 리스트를 가공해서 만든 데이터셋 리스트를 리턴하는 함수def get_dataset_list(my_list): result_list = [] for i in range(2): for j in my_list: result_list.append((i, j)) print(‘» {} data loaded..’.format(len(result_list))) return result_listfor X, y in get_dataset_list(my_list): print(X, y)실행 완료 8 data loaded..0 a0 b0 c0 d1 a1 b1 c1 d``` 첫 번째 코드의 문제가 뭘까요? 이중 for 문이 다 돌아가는 걸 기다린 후, 반환된 result_list 값에 대해 또 for 문을 돌려야 한다는 것입니다. 뭔가 중복되는 느낌도 들고, 확실히 느리겠죠? 지금은 my_list 에 데이터 4개밖에 없지만 만약 1억 개가 담겨 있다면 또 어떤 문제가 있을까요? 바로, get_dataset_list(my_list) 를 위해 엄청난 양의 데이터를 전부 메모리에 올려놔야 한다는 것입니다.my_list = ['a','b','c','d']# 인자로 받은 리스트로부터 데이터를 하나씩 가져오는 제너레이터를 리턴하는 함수def get_dataset_generator(my_list): result_list = [] for i in range(2): for j in my_list: yield (i, j) # 이 줄이 이전의 append 코드를 대체했습니다 print('&gt;&gt; 1 data loaded..')dataset_generator = get_dataset_generator(my_list)for X, y in dataset_generator: 0 1 print(X, y)실행 완료0 a&gt;&gt; 1 data loaded..0 b&gt;&gt; 1 data loaded..0 c&gt;&gt; 1 data loaded..0 d&gt;&gt; 1 data loaded..1 a&gt;&gt; 1 data loaded..1 b&gt;&gt; 1 data loaded..1 c&gt;&gt; 1 data loaded..1 d&gt;&gt; 1 data loaded..두 번째 코드가 이 문제를 해결합니다. 8번째 줄에 yield라는 새로운 키워드가 보이네요.영어로 “Yield”라는 단어는 “양보하다”라는 뜻을 갖고 있죠. 파이썬에서도 마찬가지로 yield는 코드 실행의 순서를 밖으로 “양보”합니다. 즉, dataset_generator = get_dataset_generator(my_list) 을 실행해도 “generator object” 만 반환할 뿐, 저희가 원하는 값을 바로 반환하고 있지 않습니다. 실질적으로 데이터를 반환하는 건 for 문에서 값을 하나씩 불러올 때죠.위 코드 블록의 실행 결과를 보면 값을 한 번 반환 후 “1 data loaded..” 를 출력하는 걸 반복합니다.이처럼 제너레이터가 없다면 우리는 길이 1억짜리 리스트를 리턴 받아 메모리에 전부 올려놓고 처리를 시작해야 합니다. 그러나 제너레이터를 활용할 때는 1억 개의 데이터를 전부 메모리에 올려놓을 필요가 없이 현재 처리해야 할 데이터를 1개씩 로드해서 사용할 수 있게 됩니다. 이것은 빅데이터를 처리해야 할 머신러닝 상황에서 매우 요긴합니다."
    } ,
  
    {
      "title"       : "Python 병렬처리 multiprocessing",
      "category"    : "",
      "tags"        : "",
      "url"         : "./Python-%EB%B3%91%EB%A0%AC%EC%B2%98%EB%A6%AC-Multiprocessing.html",
      "date"        : "2021-12-30 00:00:00 +0900",
      "description" : "",
      "content"     : "순차 처리와 병렬 처리순차 처리 serial processingimport timenum_list = ['p1','p2', 'p3', 'p4']start = time.time()def count(name): for i in range(0, 100000000): pass print(\"finish:\"+name+\"\\n\")for num in num_list: count(num)print(\"time :\", time.time() - start)병렬 처리 parallel processingimport multiprocessingimport timenum_list = ['p1','p2', 'p3', 'p4']start = time.time()def count(name): for i in range(0, 100000000): a = 1+2 print(\"finish:\"+name+\"\\n\") if __name__ == '__main__': pool = multiprocessing.Pool(processes = 4) pool.map(count, num_list) pool.close() pool.join()print(\"time :\", time.time() - start)프로세세서,,?를 여러개사용하는것.병렬처리인데사 왜 시간은 비슷하지? pool = multiprocessing.Pool(processes = 4) : 병렬 처리 시, 4개의 프로세스를 사용하도록 합니다. CPU 코어의 개수만큼 입력해 주면 최대의 효과를 볼 수 있습니다. pool.map(count, num_list) : 병렬화를 시키는 함수로, count 함수에 num_list의 원소들을 하나씩 넣어 놓습니다. 여기서 num_list의 원소는 4개이므로 4개의 count 함수에 각각 하나씩 원소가 들어가게 됩니다.즉, count('p1'), count('p2'), count('p3'), count('p4')가 만들어집니다. pool.close() : 일반적으로 병렬화 부분이 끝나면 나옵니다. 더 이상 pool을 통해서 새로운 작업을 추가하지 않을 때 사용합니다. pool.join() : 프로세스가 종료될 때까지 대기하도록 지시하는 구문으로써 병렬처리 작업이 끝날 때까지 기다리도록 합니다."
    } ,
  
    {
      "title"       : "텍스트 데이터의 처리과정",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%ED%85%8D%EC%8A%A4%ED%8A%B8-%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%9D%98-%EC%B2%98%EB%A6%AC%EA%B3%BC%EC%A0%95.html",
      "date"        : "2021-12-29 00:00:00 +0900",
      "description" : "",
      "content"     : "텍스트 데이터의 처리 과정사람이 이해하는 데이터는 문자열 데이터라고 합니다. 이를 컴퓨터가 이해하도록 번역 해주는 과정을 인코딩이라고 합니다. 컴퓨터는 이진수를 이해하기 때문에 인코딩과정은 문자/숫자 등의 데이터를 이진수로 변환하는 것이라고 할 수 있습니다.만약 이 인코딩의 과정에서 세계 각국마다 문자 인코딩하는 방법이 다르다면 어떻게 될까요? 알파벳 a를 한국은 97, 미국은 65으로 표현한다면 나라별로 데이터를 다시 인코딩을 해주는 과정이 추가되어야 하거나, 이 과정을 거치지 않으면 데이터가 다 깨져버릴 것입니다. 그렇기 때문에 변환하는 과정은 전 세계적으로 통일되어 있으며, 국제 표준 기구인 ISO(International Standards Organization)에서 전 세계 문자를 모두 표시할 수 있는 표준 코드를 제공하였습니다. 바로 유니코드 Unicode입니다.그렇다면 html등의 웹 개발을 해보신 분이라면, UTF-8, UTF-16이라는 것을 본 적이 있으실텐데요, 이것은 무엇일까요?사실 저는 이 UTF-8, UTF-16이 유니코드라고 착각했었습니다. 웹 개발을 배울 때, 인코딩하는 방식을 설정해주는 것이라고 배웠고, 이 과정을 텍스트를 유니코드로 바꿔주는 것이라고 이해했습니다. 여기서 제가 착각을 했었습니다.UTF-8은 유니코드를 인코딩하는 방식입니다. 즉, 유니코드를 10101110101010등과 같이 컴퓨터가 이해할 수있는 기계어로 바꾸어 주는 것입니다. 유니코드는 글자와 코드가 1:1 매핑되어 있는 (코드표)입니다. 전세계적으로 공통되어야 하기 때문에 유니코드는 오직 한 가지 버전만 존재합니다.다른 말로 다시 정리하면, 유니코드는 각 문자열에 대하여 1:1형식으로 대응되는 데이터이고, UTF-8 / UTF-16등은 유니코드로 정의된 텍스트를 메모리에 인코딩하는 방식 입니다.참고사항 아스키코드 https://ko.wikipedia.org/wiki/ASCII UTF-8과 UTF-16의 비교 https://pickykang.tistory.com/13 용어설명 바이트 byte 컴퓨터의 기본 저장 단위 인코딩 encoding 문자열을 바이트로 변환하는 과정 디코딩 decoding 바이트를 문자열로 변환하는 과정"
    } ,
  
    {
      "title"       : "이스케이프 문자 escape string",
      "category"    : "",
      "tags"        : "",
      "url"         : "./%EC%9D%B4%EC%8A%A4%EC%BC%80%EC%9D%B4%ED%94%84-%EB%AC%B8%EC%9E%90-Escape-String.html",
      "date"        : "2021-12-29 00:00:00 +0900",
      "description" : "",
      "content"     : "이스케이프 문자 Escape 문자란?이스케이프 문자는 이스케이프 시퀀스를 따르는 문자들로서, 다음 문자가 특수문자임을 알리는 \\ 백슬래시를 사용한다.이를 풀어쓰면 다음과 같다.프로그래밍에서 문자열에 작은 따옴표''나, 큰 따옴표\"\" 혹은 슬래쉬 /\\ 등의 특수 문자를 사용하고 싶을 때, 즉 그 문자가 특수 문자가 아닌 일반 문자열 인 것처럼 처리하고 싶을 때 이스케이프 문자를 사용한다.참고역슬래시\\는 키보드에서 원화 돈표시￦로 표기되어 있습니다.파이썬 python\\ 백슬레시를 이용하여 이스케이프 처리하기원시 문자열을 사용하여 이스케이프 처리원시 문자열을 사용. 문자열 시작 앞 따옴표에 r을 붙이면 됩니다.print('Please don\\'t touch it')print(r'Please don\\'t touch it')실행 결과 Please don't touch itPlease don\\'t touch it 자바 javac++태그#이스케이프문자 #EscapeString #특수문자무시 #특수문자포함문자열 #기호무시"
    } ,
  
    {
      "title"       : "Python) print()",
      "category"    : "",
      "tags"        : "",
      "url"         : "./python)-print().html",
      "date"        : "2021-12-29 00:00:00 +0900",
      "description" : "",
      "content"     : "파이썬에서 가장 자주 쓰는 print() 메서드.여러가지 옵션을 주어서 다르게 사용하도록 해보았다.print(“문자열”, “123”,sep=’ㅁ’)sep은 print함수에서 문자열 여러개를 출력할 때 문자열 사이에 sep다음 문자를 끼워준다.&gt;&gt;&gt; print(\"문자열\", \"123\",sep='ㅁ')문자열ㅁ123&gt;&gt;&gt; print(\"문자열\", \"123\",\"abc\",sep='y')문자열y123yabcprint(“문자열”, end=’a’)&gt;&gt;&gt; for i in range(10): print(i,end=' ') 0 1 2 3 4 5 6 7 8 9 &gt;&gt;&gt; for i in range(10): print(i, end='%%') 0%%1%%2%%3%%4%%5%%6%%7%%8%%9%%"
    } ,
  
    {
      "title"       : "Python) int() 형변환, 진수 변환",
      "category"    : "",
      "tags"        : "",
      "url"         : "./python)-int()-%ED%98%95%EB%B3%80%ED%99%98,-%EC%A7%84%EC%88%98-%EB%B3%80%ED%99%98.html",
      "date"        : "2021-12-29 00:00:00 +0900",
      "description" : "",
      "content"     : "int()int(매개변수) : return int형 매개변수int(integer) -&gt; Integerstring to intint(“string”) -&gt; Integern진수 형변환2/8/16진수를 10진수로 변환int(“number”, base=10) -&gt; 10진수를 Integer 변환int(“binary number 2진수 문자열”, 2) -&gt; 2진수를 Integer로 변환int(“octal number 8진수 문자열”, 8) -&gt; 8진수를 Integer로 변환int(“decimal number 10진수 문자열” , 10) -&gt; 10진수를 Integer로 변환int(“hexadecimal 16진수 문자열”, 16) -&gt; 16진수를 Integer로 변환int(\"ffff\",16)의 실행 결과 : 65535int(\"0xffff\",16)의 실행 결과 : 65535int(58)의 실행 결과 : 58int(\"11\", base =2)의 실행 결과 : 3"
    } ,
  
    {
      "title"       : "Python) continue, pass 명령어",
      "category"    : "",
      "tags"        : "",
      "url"         : "./python)-continue,-pass-%EB%AA%85%EB%A0%B9%EC%96%B4.html",
      "date"        : "2021-12-29 00:00:00 +0900",
      "description" : "",
      "content"     : "continue반복문에서 continue를 만나면 해당 단계의 반복문은 종료되고 다음 단계로 넘어간다.pass코드 실행 중 pass를 만나면 해당 코드는 아무것도 진행하지 않고 다음을 실행한다.즉, 실행할 것이 아무것도 없을 때 적는 코드이다.파이썬은 들여쓰기가 중요한 언어이다.아무 동작은 하지만, if나 for, while등과 같은 조건문에서 소스코드 블록이 있다는 표시로 자주 쓴다. 다음은 그 예시이다.if i != 0: pass이렇게 들여쓰기가 필요한 문법 설명할때 자주 사용한다continue와 pass의 비교다음 코드를 돌려보면 바로 이해가 갈 것이다.for i in range(10): if(i%2==0): #continue or pass 넣어보기 print(i*100, end=\" \") # a 코드 #continue or pass 넣어보기 else: print(i, end=\" \")\"\"\"경우 1. continue -&gt; 코드 a : a 코드 실행 안됨경우 2. pass -&gt; 코드 a : a 코드 실행됨경우 3. 코드 a -&gt; continue : a 코드 실행됨경우 4. 코드 a -&gt; pass : a 코드 실행됨\"\"\"경우 1의 경우, 짝수일 때 가장 먼저 만나는 코드가 continue이다. 이때는 해당 반복문을 skip하기 때문에 결과화면은 다음과 같다. 1 3 5 7 9경우 2의 경우, 짝수일 때 가장 먼저 만나는 코드가 pass이다. 이때는 해당 pass명령어가 실행되지만, 아무 동작을 하지않는다. 아무 동작을 하지 않는 이 코드가 끝나면 다음 코드를 실행하게된다. 때문에 결과화면은 다음과 같다. 0 1 200 3 400 5 600 7 800 9경우 3, 4의 경우 print(i*100)이 먼저 실행되고 pass나 continue가 실행되므로 결과는 다음과 같다. 0 1 200 3 400 5 600 7 800 9"
    } ,
  
    {
      "title"       : "Python) 주석 문자열 변수",
      "category"    : "",
      "tags"        : "",
      "url"         : "./Python)-%EC%A3%BC%EC%84%9D-%EB%AC%B8%EC%9E%90%EC%97%B4-%EB%B3%80%EC%88%98.html",
      "date"        : "2021-12-29 00:00:00 +0900",
      "description" : "",
      "content"     : "파이썬 주석 문자열\"\"\"내용\"\"\"의 여러 쓰임진짜 여러줄을 문자열로 처리할 때파이썬에서 여러줄을 주석처리할 때 다음과 같이 사용합니다.a = 5\"\"\"여기서 주석 시작a= 7여기서 주석 끝\"\"\"print(a)실행결과 5위와 같은 코드가 있을 때, \"\"\" 내용 \"\"\"에서 내용부분은 주석으로 처리됩니다.그렇기 때문에, a=7로 초기화해도 위과 같이 실행결과가 5가 나오게 됩니다.문자열을 변수로 사용할때알고나서 너무 신기했는데, 파이썬은 여러줄로 표현된 문자열도 변수로 받을 수 있으며 이때 해당 문자열을 다음과 같이 \"\"\" 여러줄의 문자열 \"\"\"로 감싸줍니다.import re # 정규표현식을 사용하기 위한 라이브러리 추가#- 연도(숫자)text = \"\"\"The first season of America Premiere League was played in 1993. The second season was played in 1995 in South Africa. Last season was played in 2019 and won by Chennai Super Kings (CSK).CSK won the title in 2000 and 2002 as well.Mumbai Indians (MI) has also won the title 3 times in 2013, 2015 and 2017.\"\"\"pattern = re.compile(\"[1-2]\\d\\d\\d\")print(pattern.findall(text)) [‘1993’, ‘1995’, ‘2019’, ‘2000’, ‘2002’, ‘2013’, ‘2015’, ‘2017’]같이 공부하면 좋을 것 (링크첨부필요) 원시 문자열 str = r”원시문자열” 포맷 문자열 리터럴 str=f”포맷 문자열 리터럴”"
    } ,
  
    {
      "title"       : "Python) 메소드 사전",
      "category"    : "",
      "tags"        : "",
      "url"         : "./Python)-%EB%A9%94%EC%86%8C%EB%93%9C-%EC%82%AC%EC%A0%84.html",
      "date"        : "2021-12-29 00:00:00 +0900",
      "description" : "",
      "content"     : "todoformat정리여기는 자주쓰는 함수 input int 이런거만 남겨놓고 하이퍼링크하고각각의 input/int페이지 작성하기ctrl + f 해서 찾기!이것 정돈 알아야 한다!input()백준 알고리즘을 풀게되면 가장 많이 쓰게 될 함수.사용자로부터 입력을 받온다. return형은 string인듯?print()int()int(매개변수) : return int형 매개변수이거 알면 핵편함enumerate()리스트, 문자열, 튜플 등이 있는 경우 순서와 리스트의 값을 함께 반환해주는 메서드map(function, iterable)두 번째 인자로 들어온 반복 가능한 자료형 (리스트나 튜플)을 첫 번째 인자로 들어온 함수에 하나씩 집어넣어서 함수를 수행하는 함수입니다.출처: https://blockdmask.tistory.com/531 [개발자 지망생]문자열 처리검색startswith()endswith()검사isX시리즈이며, “string”.isX() 형식으로 사용한다.isupper()문자열이 모두 대문자로만 되어 있으면 True, 그렇지 않으면 False를 반환islower()문자열이 모두 소문자로만 되어 있으면 True, 그렇지 않으면 False를 반환istitle()문자열의 첫 글자만 대문자로 되어 있으면 True, 그렇지 않으면 False를 반환isalpha()문자열이 모두 알파벳 문자로만 되어 있으면 True, 그렇지 않으면 False를 반환isalnum()문자열이 모두 알파벳 문자와 숫자로만 되어 있으면 True, 그렇지 않으면 False를 반환isdecimal()문자열이 모두 숫자로만 되어 있으면 True, 그렇지 않으면 False를 반환연결 혹은 자르기join()split()문구 대체replace()“target string”.replace(“old”, “new”)공백문자 제거strip()“string”.strip()공백 문자 제거하는 함수정규표현식search()match()findall()split()sub()람다 표현식 lambda expressionlambda(lambda 사용할 변수 : 변수를 이용한 수식)(변수에 들어갈 값)mapmap(함수 f, iterable객체) iterable객체의 요소를 하나씩 함수 f에 넣어줌. 이때 f()가 아니라 f만 (함수 이름만!) 매개변수로 넣어줌.대소문자upper()모든 문자를 대문자로 변환합니다.lower()모든 문자를 소문자로 변환 합니다.capitalize()첫 글자만 대문자로 변호나 합니다.순서 정리reversed()매개변수에 시퀀스 객체를 넣으면 시퀀스 객체를 뒤집어 줍니다. 이때, 원본 객체 자체는 바뀌지 않으며 뒤집어서 꺼내줍니다.진수 변환int()2/8/16진수를 10진수로 변환format()2/8/16진수를 10진수로 변환유니코드ord()해당 문자에 대응하는 유니코드 숫자를 반환 합니다.ord('a') 의 실행 결과 : 97ord('A') 의 실행 결과 : 65ord(5) 의 실행 결과 : TypeError : ord() expected string of length 1, but int found-&gt; ord의 매개변수로는 1자리의 string 문자 데이터만 들어갈 수 있습니다.ord('5') 의 실행 결과 : 53chr()해당 유니 코드 숫자에 대응하는 문자를 반환 합니다.chr(97)의 실행 결과 : achr(56)의 실행 결과 : 8chr(0xAC00)의 실행 결과 : 가type()"
    } ,
  
    {
      "title"       : "Python) startswith(), endswith()",
      "category"    : "",
      "tags"        : "",
      "url"         : "./Python)-startswith(),-endswith().html",
      "date"        : "2021-12-29 00:00:00 +0900",
      "description" : "",
      "content"     : "언제 사용?여러 문자열들을 가지고와서 원하는 문자로 시작하거나 끝나는 문자열을 검색할때 사용합니다.startswithEmployeeID = ['OB94382', 'OW34723', 'OB32308', 'OB83461', 'OB74830', 'OW37402', 'OW11235', 'OB82345'] Production_Employee = [P for P in EmployeeID if P.startswith('OB')] # 'OB'로 시작하는 직원 ID를 다 찾아봅니다Production_Employee실행결과 [‘OB94382’, ‘OB32308’, ‘OB83461’, ‘OB74830’, ‘OB82345’]endswithimport osimage_dir_path = os.getenv(\"HOME\") + \"/data/pictures\" photo = os.listdir(image_dir_path )png = [png for png in photo if png.endswith('.png')]print(png)#해당디렉토리에 실행결과의 이미지.PNG들이 존재해야 아래와 같이 뜸.실행결과 ['image5.png', 'image2.png', 'image12.png', 'image3.png', 'image8.png', 'image13.png', 'image6.png', 'image4.png', 'image14.png', 'image9.png', 'image10.png', 'image7.png', 'image11.png']"
    } ,
  
    {
      "title"       : "Python) join(), split()",
      "category"    : "",
      "tags"        : "",
      "url"         : "./Python)-join(),-split().html",
      "date"        : "2021-12-29 00:00:00 +0900",
      "description" : "",
      "content"     : "join()튜플, 리스트, stirng 등 반복 가능한 iterable 객체를 받는 메서드입니다.각각의 원소를 모아 하나의 문자열로 합쳐줍니다.“요소 연결시 추가할 문자”.join(iterable객체)#- join()alphabet = ['a', 'b', 'c']\",\".join(alphabet)실행 결과 ‘a,b,c’split()#- split()&gt;&gt;&gt; \"hi this is me\".split()['hi', 'this', 'is', 'me']&gt;&gt;&gt; \"this,is,me!\".split(',')['this', 'is', 'me!']&gt;&gt;&gt;"
    } ,
  
    {
      "title"       : "Markdown) 텍스트에 색깔입히기",
      "category"    : "",
      "tags"        : "",
      "url"         : "./Markdown)-%ED%85%8D%EC%8A%A4%ED%8A%B8%EC%97%90-%EC%83%89%EA%B9%94%EC%9E%85%ED%9E%88%EA%B8%B0.html",
      "date"        : "2021-12-29 00:00:00 +0900",
      "description" : "",
      "content"     : "Red Text &lt;span style=\"color:red\"&gt;Red Text&lt;/span&gt;"
    } ,
  
    {
      "title"       : "Python 정규표현식",
      "category"    : "",
      "tags"        : "",
      "url"         : "./python-%EC%A0%95%EA%B7%9C%ED%91%9C%ED%98%84%EC%8B%9D.html",
      "date"        : "2021-12-29 00:00:00 +0900",
      "description" : "",
      "content"     : "정규 표현식 이란?정규 표현식은 특정 규칙을 가진 문자열의 집합을 표현하는 형식 언어로, 찾고자 하는 문자열 패턴을 정의하고 기존 문자열과 일치하는지를 비교하여 문자열을 검색하거나 다른 문자열로 치환하는 데 사용됩니다. 파이썬에서는 표준 라이브러리인 re 모듈을 import 해야 정규 표현식을 사용할 수 있습니다.메타문자|A|B라는 정규식이 있다면 A 또는 B라는 의미가 된다.&gt;&gt;&gt; p = re.compile('Crow|Servo')&gt;&gt;&gt; m = p.match('CrowHello')&gt;&gt;&gt; print(m)&lt;re.Match object; span=(0, 4), match='Crow'&gt;^^ 메타 문자는 문자열의 맨 처음과 일치함을 의미가 된다.re.MULTILINE을 사용할 경우에는 여러 줄의 문자열일 때 각 줄의 처음과 일치하게 된다&gt;&gt;&gt; print(re.search('^Life', 'Life is too short'))&lt;re.Match object; span=(0, 4), match='Life'&gt;&gt;&gt;&gt; print(re.search('^Life', 'My Life'))None$$는 문자열의 끝과 매치함을 의미한다.&gt;&gt;&gt; print(re.search('short$', 'Life is too short'))&lt;re.Match object; span=(12, 17), match='short'&gt;&gt;&gt;&gt; print(re.search('short$', 'Life is too short, you need python'))None패턴패턴이야말로 정규 표현식을 강력하게 해줍니다. 특수문자 혹은 메타 문자라 불리는 []. -. . ? * + {} / 등을 이용해 특수한 패턴을 만들 수 있습니다. [ ] : 문자 - : 범위 . : 하나의 문자 ? : 0회 또는 1회 반복 * : 0회 이상 반복 + : 1회 이상 반복 {m, n} : m ~ n \\A : 는 문자열의 처음과 매치됨을 의미한다. re.MULTILINE 옵션을 사용할 경우 ^은 각 줄의 문자열의 처음과 매치되지만 \\A는 줄과 상관없이 전체 문자열의 처음하고만 매치된다. \\b : 단어 경계, 보통 단어는 whitespace에 의해 구분된다. \\B : 비 단어 경계 \\d : 숫자, [0-9]와 동일 \\D : 비 숫자, [^0-9]와 동일 \\w : 알파벳 문자 + 숫자 + _, [a-zA-Z0-9_]와 동일 \\W : 비 알파벳 문자 + 비숫자, [^a-zA-Z0-9_]와 동일 \\s : 공백 문자, [ \\t\\n\\r\\f\\v]와 동일 \\S : 비 공백 문자, [^ \\t\\n\\r\\f\\v]와 동일 \\t : 가로 탭(tab) \\v : 세로 탭(vertical tab) \\f : 폼 피드 \\n : 라인 피드(개행문자) \\r : 캐리지 리턴(원시 문자열) \\Z : 문자열의 끝과 매치됨을 의미한다. re.MULTILINE 옵션을 사용할 경우 $ 메타 문자와는 달리 전체 문자열의 끝과 매치된다. 그루핑특정 문자열이 반복되는지 조사하는 정규식으로 그룹을 만들어 주는 메타 문자는 ( )입니다.&gt;&gt;&gt; p = re.compile('(ABC)+')&gt;&gt;&gt; m = p.search('ABCABCABC OK?')&gt;&gt;&gt; print(m)&lt;re.Match object; span=(0, 9), match='ABCABCABC'&gt;&gt;&gt;&gt; print(m.group())ABCABCABC&gt;&gt;&gt; p = re.compile(r\"\\w+\\s+\\d+[-]\\d+[-]\\d+\")&gt;&gt;&gt; m = p.search(\"park 010-1234-1234\")여기서 특정 부분의 문자열만 뽑아내기 위해서는 다음과 같이 작성한다.&gt;&gt;&gt; p = re.compile(r\"(\\w+)\\s+\\d+[-]\\d+[-]\\d+\")&gt;&gt;&gt; m = p.search(\"park 010-1234-1234\")&gt;&gt;&gt; print(m.group(1))park그루핑안에 그룹을 더 추가할 수있다.&gt;&gt;&gt; p = re.compile(r\"(\\w+)\\s+(\\d+[-]\\d+[-]\\d+)\")&gt;&gt;&gt; m = p.search(\"park 010-1234-1234\")&gt;&gt;&gt; print(m.group(2))010-1234-1234그룹의 인덱스는 다음과 같은 의미를 가진다. group(인덱스) 설명 group(0) 매치된 전체 문자열 group(1) 첫 번째 그룹에 해당되는 문자열 group(2) 두 번째 그룹에 해당되는 문자열 group(n) n 번째 그룹에 해당되는 문자열 국번 부분만 뽑아내고 싶을시는 다음과 같이 그루핑을 더 추가해준다.&gt;&gt;&gt; p = re.compile(r\"(\\w+)\\s+((\\d+)[-]\\d+[-]\\d+)\")&gt;&gt;&gt; m = p.search(\"park 010-1234-1234\")&gt;&gt;&gt; print(m.group(3))010혹은, 그룹에 이름을 붙여줄 수도 있다.(?P&lt;name&gt;\\w+)\\s+((\\d+)[-]\\d+[-]\\d+)ex)&gt;&gt;&gt; p = re.compile(r\"(?P&lt;name&gt;\\w+)\\s+((\\d+)[-]\\d+[-]\\d+)\")&gt;&gt;&gt; m = p.search(\"park 010-1234-1234\")&gt;&gt;&gt; print(m.group(\"name\"))park&gt;&gt;&gt; p = re.compile(r'(?P&lt;word&gt;\\b\\w+)\\s+(?P=word)')&gt;&gt;&gt; p.search('Paris in the the spring').group()'the the'메소드 설명compile()findall()search()match()findall()split()strip()문자열의 처음과 끝에 있는 공백 제거sub()group()출처 AIFFEL LMS https://wikidocs.net/4309 todo : 전방탐색부터 문제시 말해주세요"
    } ,
  
    {
      "title"       : "Aiffel) 2주차 아이펠 일기",
      "category"    : "",
      "tags"        : "",
      "url"         : "./AIFFEL)-2%EC%A3%BC%EC%B0%A8-%EC%95%84%EC%9D%B4%ED%8E%A0-%EC%9D%BC%EA%B8%B0.html",
      "date"        : "2021-12-29 00:00:00 +0900",
      "description" : "",
      "content"     : ""
    } ,
  
    {
      "title"       : "Python) 변수의 유효 범위",
      "category"    : "",
      "tags"        : "",
      "url"         : "./python)-%EB%B3%80%EC%88%98%EC%9D%98-%EC%9C%A0%ED%9A%A8-%EB%B2%94%EC%9C%84.html",
      "date"        : "2021-12-28 00:00:00 +0900",
      "description" : "",
      "content"     : "변수란?컴퓨터 프로그래밍에서 ‘변수’란, 새로운 값이나 정보를 담을 수 있는 컨테이너이다.예를 들어, 다음과 같은 파이썬 코드가 있다고 해보자.a = '사과'b = '바나나'c = 3print(a)print(b)print(c)실행 결과 사과 바나나 3a와 b, c를 출력했는데, 사과, 바나나, 3이 출력되었다. 해석하자면, a,b,c는 변수이고 사과, 바나나, 3이라는 정보와 값을 담는 컨테이너 역할을 하는 것이다.그렇다면 프로그래밍에서는 아무데나 변수를 선언하고 아무데서나 변수를 사용할 수 있을까?답은 아니다.이다.프로그래밍에서 변수는 영향을 끼칠 수 있는 유효범위가 한정되어 있다. 특정 위치에서 변수에 접근 할 수 있는지, 혹은 한 곳에서 정의된 변수가 어디까지 유효한지 정의된 범위를 변수의 범위, SCOPE라고 한다.이 변수의 범위에 따라, 지역변수 Local variable, 전역변수 global variable, 정적 변수(static variable) 등이 있다.전역변수 global variable전역 변수는 어느 지역에서나 접근할 수 있는 변수이다.global scope의 영향력을 끼친다.greeting = 'Hello'def say_hello(): print(greeting)# say_hi() 함수의 밖에 정의된 변수(greeting), 'Hello'를 읽어옵니다.say_hello()greeting = 'hi'# greeting 변수는 문자열 'hi'로 변경됨say_hello()실행 결과 Hello hi지역변수 local variable지역 변수란 _‘블록’_내에서 선언된 변수를 의미합니다.local scope의 영향력을 끼친다.greeting = 'Hello'def change_greeting(): greeting = 'hi' # 여기서 greeting 해당 함수 내에서만 문자열 'Hello'를 가리킵니다.change_greeting()# 함수를 호출해도 아무 일도 일어나지 않습니다.print(greeting)# 첫 줄, 즉 함수 바깥에서 정의된 문자열 '하루'가 출력됩니다.실행 결과 Hello즉, 위의 코드에서 greeting = 'Hello'는 함수 내부를 포함해 코드 어디에서든 참조할 수 있으므로, 전역 변수라고 할 수 있습니다. 반대로 greeting = 'hi'는 change_greeting() 내부에서 정의되어 밖에서 볼 수 없으므로, 지역변수입니다.함수에서 전역변수 참고하기함수내에서 전역변수를 참고하려면, 함수내에서 global 전역변수명을 따로 추가해 주어야 한다.다음 코드 참고greeting = 'Hello'def change_greeting(): global greeting greeting = 'hi' # 여기서 greeting 해당 함수 내에서만 문자열 'Hello'를 가리킵니다.change_greeting()# 함수를 호출해도 아무 일도 일어나지 않습니다.print(greeting)# 첫 줄, 즉 함수 바깥에서 정의된 문자열 '하루'가 출력됩니다.실행 결과 hi정적 변수는 다음에 ^^/용어설명 블록 마치 한 문단처럼 보이는, 코드의 한 부분을 뜻하며, 중괄호로 묶여 있는 경우가 많다. 보통 1개 이상의 명령어를 가지고 있으나, 주석으로 이루어진 블록이나, 아무 내용도 없는 빈 블록도 가능하다. 출처 AIFFEL LMS"
    } ,
  
    {
      "title"       : "Python) 매개변수에 디폴트값 설정하는 법",
      "category"    : "",
      "tags"        : "",
      "url"         : "./python)-%EB%A7%A4%EA%B0%9C%EB%B3%80%EC%88%98%EC%97%90-%EB%94%94%ED%8F%B4%ED%8A%B8%EA%B0%92-%EC%84%A4%EC%A0%95%ED%95%98%EB%8A%94-%EB%B2%95.html",
      "date"        : "2021-12-28 00:00:00 +0900",
      "description" : "",
      "content"     : "요약파이썬에서 매개변수에 디폴트 값을 쓰는 방법은 다음과 같다.def testDefault(var='디폴트값'): print('var 값은 '+var+' 입니다!') testDefault('var')testDefault()var 값은 var 입니다!var 값은 디폴트값 입니다!"
    } ,
  
    {
      "title"       : "Fibo",
      "category"    : "",
      "tags"        : "",
      "url"         : "./fibo.html",
      "date"        : "2021-12-28 00:00:00 +0900",
      "description" : "",
      "content"     : "\"\"\"def fibonacci(n): if n &lt;= 2: return 1 else: return fibonacci(n-2) + fibonacci(n-1)n = 1while n &lt;= 20: print(fibonacci(n)) n = n + 1print('끝!')\"\"\"# 직접 만들어보세요.fiboIndex = 0; # peopleNum 값이 몇 번째 인덱스인지 저장할 변수def fibonacci(input): if input&lt;=2: return 1 else: return fibonacci(input-2)+fibonacci(input-1) def FibonaChicken(peopleNum): index = 0; while(peopleNum != fibonacci(index)): index = index+1 #print(index) return index\"\"\"fiboIndex = FibonaChicken(13)fibonacci(fiboIndex-1)\"\"\"fibonacci(FibonaChicken(13)-1)"
    } ,
  
    {
      "title"       : "Aiffel) 12월 27일 아이펠의 시작",
      "category"    : "",
      "tags"        : "",
      "url"         : "./AIFFEL)-12%EC%9B%94-27%EC%9D%BC-%EC%95%84%EC%9D%B4%ED%8E%A0%EC%9D%98-%EC%8B%9C%EC%9E%91.html",
      "date"        : "2021-12-27 00:00:00 +0900",
      "description" : "",
      "content"     : "1주차 1일일과표 오전 차시 HRD넷(국비교육)과 관련한 OT 교육 출결 및 공결, 지각, 결석과 같은 출석처리 와 제적 요건과 수강 포기에 관한 설명 훈련수당 지급에 관한 설명 오후 차시 AGIT, Gather, LMS 교육에 관한 사항 설명 팀별로 모여서 친목도모를 위한 간단한 게임 진행 및 개인 소개 벨로그 만들어 지금 작성 중~! 드디어 AIFFEL에 입성했다!쏘카와 연계교육 한다는 강남캠퍼스.. 면접까지는 봤었는데 정말 아쉽게도 불합격했다.아쉬움을 뒤로 하고, 중복 지원했던 양재 캠퍼스는 다행히 합격!그리고 오늘, 2021년 12월 27일, 기다리고 기다리던 AIFFEL 첫 교육날이다!사실 오전 차시에는 조금 실망감이 좀 컸다. AIFFEL 홍보에서 봤던 내용과 실제 이번에 진행될 기수 상세사항이 좀 달랐기 때문이다. 개인 노트북 지급 -&gt; 미지급 본래는 Ubuntu OS의 노트북을 주고, 거기서 개발을 진행했다고 한다. 이번에는 Cloud를 이용하여 원격 코딩?이 가능해졌기 때문에 개인 노트북은 미지급이 되었다. 아직 사용해보진 못했으나 Google colab과 같은 형식인듯하고, 이미 개인 노트북이 있기 때문에 별로 신경쓰이진 않았다. 훈련수당 30만원 -&gt; 10만원 훈련수당 지원이 감축되었다고 한다. 2020년에는 1만 5천원 지원에서, 2021에는 5800원으로. 내가 어찌할 수 있는 부분이 아니고 정부에서 이렇게 진행하겠다는데 어쩌겠나 싶다. 특히 이번엔 훈련수당이 작다고 그냥 알바를 원하는 사람이 있다면, 퍼실님/담임분께 말씀드려서 알바를 하라고 하는데,, 그저 아쉽고 속상하다. 부산에서 이거 듣겠다고 서울에 집구한 나는,,,,,,,,,,,,,,,,,,,,,,예민보스라구, 아이펠에서 OT 처음 진행할 때, 소개영상을 보여주셨는데, 저번 기수에서 아이펠을 통해 좋은 회사에 합격하신 분을 인터뷰 한 줄 알았는데 아니었다. 게다가 2020년거 재탕.. OT영상이 중요하고 필수인 사항은 아니지만, 웬지 작년과 똑같은 영상을 틀어준 거란걸 알고나니 조금 사기가 꺾였다. 하지만 오후 차시에는 다시 기대감이 뿜뿜 올라왔다.LMS를 소개받고, 혼자 공부를 할 때, 어떻게 진행될 지를 보여주셨다. 클라우드 연결로 개인 커널과 주피터 노트북환경이 제공되었고, 첫 차시 LMS를 진행했는데, Mnist 학습 예제 코드를 돌려보았다. 속도가 꽤 빠른듯 해서 엄청 만족! ㅎㅎㅎAGIT와 Gather라는 커뮤니티 공간도 소개 받았는데 Gather이라는 커뮤니티가 엄청 귀엽다 ㅋㅋㅋㅋ 마치 온라인 교실에 있는듯해서, 오프라인 진행을 못하는 것에 대한 아쉬움이 조금 해소되었다.그리고 notion과 velog에 관한 것도 설명을 주셨는데, 이 부분이 제일 마음에 들었다. 포트폴리오를 강조해주셨기 때문이다. 대학교 졸업을 앞 둔 취준생으로써 가장 시급하고 부족하다고 느꼈던 점이 이 점이었다. 학부 AI 연구생으로 활동하면서 공부했던 CNN, RNN, ResNet, KNN 등 과 각종 기초가 되는 퍼셉트론, Classification, Clustering, Regression 등을 공부했던 것이 구두로 공부 했다! 말고는 증빙 자료가 없어서 매우 난감하다고 느꼈다. 그리고 학교에서 진행했던 프로젝트들도 정리도 필요하다고 느꼈다.AIFFEL 6개월 교육 과정 동안, 여기서 학습하는 모든 것 들을 기술 블로그에 남기고, 프로젝트를 진행하면서 생기는 시행착오와 오류 해결과정을 블로그에 남길 것이다. 또한, 내 포트폴리오 페이지도 작성 완료하는 것이 최소 목표이다.더 나아가서, CS공부하는 것과 알고리즘과 코테준비도 틈틈이 할 것이다."
    } ,
  
    {
      "title"       : "Who owns the copyright for an AI generated creative work?",
      "category"    : "opinion",
      "tags"        : "copyright, creativity, neural networks, machine learning, artificial intelligence",
      "url"         : "./AI-and-intellectual-property.html",
      "date"        : "2021-04-20 00:00:00 +0900",
      "description" : "As neural networks are used more and more in the creative process, text, images and even music are now created by AI, but who owns the copyright for those works?",
      "content"     : "Recently I was reading an article about a cool project that intends to have a neural network create songs of the late club of the 27 (artists that have tragically died at age 27 or near, and in the height of their respective careers), artists such as Amy Winehouse, Jimmy Hendrix, Curt Cobain and Jim Morrison.The project was created by Over the Bridge, an organization dedicated to increase awareness on mental health and substance abuse in the music industry, trying to denormalize and remove the glamour around such illnesses within the music community.They are using Google’s Magenta, which is a neural network that precisely was conceived to explore the role of machine learning within the creative process. Magenta has been used to create a brand new “Beatles” song or even there was a band that used it to write a full album in 2019.So, while reading the article, my immediate thought was: who owns the copyright of these new songs?Think about it, imagine one of this new songs becomes a massive hit with millions of youtube views and spotify streams, who can claim the royalties generated?At first it seems quite simple, Over the Bridge should be the ones reaping the benefits, since they are the ones who had the idea, gathered the data and then fed the neural network to get the “work of art”. But in a second thought, didn’t the original artists provide the basis for the work the neural network generated? shouldn’t their state get credit? what about Google whose tool was used, should they get credit too?Neural networks have been also used to create poetry, paintings and to write news articles, but how do they do it? A computer program developed for machine learning purposes is an algorithm that “learns” from data to make future decisions. When applied to art, music and literary works, machine learning algorithms are actually learning from some input data to generate a new piece of work, making independent decisions throughout the process to determine what the new work looks like. An important feature of this is that while programmers can set the parameters, the work is actually generated by the neural network itself, in a process akin to the thought processes of humans.Now, creative works qualify for copyright protection if they are original, with most definitions of originality requiring a human author. Most jurisdictions, including Spain and Germany, specifically state that only works created by a human can be protected by copyright. In the United States, for example, the Copyright Office has declared that it will “register an original work of authorship, provided that the work was created by a human being.”So as we currently stand, a human author is required to grant a copyright, which makes sense, there is no point of having a neural network be the beneficiary of royalties of a creative work (no bank would open an account for them anyways, lol).I think amendments have to be made to the law to ensure that the person who undertook all the arrangements necessary for the work to be created by the neural network gets the credit but also we need to modify copyright law to ensure the original authors of the body of work used as data input to produce the new piece get their corresponding share of credit. This will get messy if someone uses for example the #1 song of every month in a decade to create the decade song, then there would be as many as 120 different artists to credit.In a computer generated artistic work, both the person who undertook all the arrangements necessary for its creation as well as the original authors of the data input need to be credited.There will still be some ambiguity as to who undertook the arrangements necessary, only the one who gathered the data and pressed the button to let the network learn, or does the person who created the neural network’s model also get credit? Shall we go all the way and say that even the programmer of the neural network gets some credit as well?There are some countries, in particular the UK where some progress has been made to amend copyright laws to cater for computer generated works of art, but I believe this is one of those fields where technology will surpass our law making capacity and we will live under a grey area for a while, and maybe this is just what we need, by having these works ending up free for use by anyone in the world, perhaps a new model for remunerating creative work can be established, one that does not require commercial success to be necessary for artists to make a living, and thus they can become free to explore their art.Perhaps a new model for remunerating creative work can be established, one that does not require commercial success to be necessary for artists to make a living.The Next Rembrandt is a computer-generated 3-D–printed painting developed by a facial-recognition algorithm that scanned data from 346 known paintings by the Dutch painter in a process lasting 18 months. The portrait is based on 168,263 fragments from Rembrandt’s works."
    } ,
  
    {
      "title"       : "So, what is a neural network?",
      "category"    : "theory",
      "tags"        : "neural networks, machine learning, artificial intelligence",
      "url"         : "./back-to-basics.html",
      "date"        : "2021-04-02 00:00:00 +0900",
      "description" : "ELI5: what is a neural network.",
      "content"     : "The omnipresence of technology nowadays has made it commonplace to read news about AI, just a quick glance at today’s headlines, and I get: This Powerful AI Technique Led to Clashes at Google and Fierce Debate in Tech. How A.I.-powered companies dodged the worst damage from COVID AI technology detects ‘ticking time bomb’ arteries AI in Drug Discovery Starts to Live Up to the Hype Pentagon seeks commercial solutions to get its data ready for AITopics from business, manufacturing, supply chain, medicine and biotech and even defense are covered in those news headlines, definitively the advancements on the fields of artificial intelligence, in particular machine learning and deep neural networks have permeated into our daily lives and are here to stay. But, do the general population know what are we talking about when we say “an AI”? I assume most people correctly imagine a computer algorithm or perhaps the more adventurous minds think of a physical machine, an advanced computer entity or even a robot, getting smarter by itself with every use-case we throw at it. And most people will be right, when “an AI” is mentioned it is indeed an algorithm run by a computer, and there is where the boundary of their knowledge lies.They say that the best way to learn something is to try to explain it, so in a personal exercise I will try to do an ELI5 (Explain it Like I am 5) version of what is a neural network.Let’s start with a little history, humans have been tinkering with the idea of an intelligent machine for a while now, some even say that the idea of artificial intelligence was conceived by the ancient greeks (source), and several attempts at devising “intelligent” machines have been made through history, a notable one was ‘The Analytical Engine’ created by Charles Babbage in 1837:The Analytical Engine of Charles Babbage - 1837Then, in the middle of last century by trying to create a model of how our brain works, Neural Networks were born. Around that time, Frank Rosenblatt at Cornell trying to understand the simple decision system present in the eye of a common housefly, proposed the idea of a perceptron, a very simple system that processes certain inputs with basic math operations and produces an output.To illustrate, let’s say that the brain of the housefly is a perceptron, its inputs are whatever values are produced by the multiple cells in its eyes, when the eye cell detects “something” it’s output will be a 1, and if there is nothing a 0. Then the combination of all those inputs can be processed by the perceptron (the fly brain), and the output is a simple 0 or 1 value. If it is a 1 then the brain is telling the fly to flee and if it is a 0 it means it is safe to stay where it is.We can imagine then that if many of the eye cells of the fly produce 1s, it means that an object is quite near, and therefore the perceptron will calculate a 1, it is time to flee.The perceptron is just a math operation, one that multiplies certain input values with preset “parameters” (called weights) and adds up the resulting multiplications to generate a value.Then the magic spark was ignited, the parameters (weights) of the perceptron could be “learnt” by a process of minimizing the difference between known results of particular observations, and what the perceptron is actually calculating. It is this process of learning what we call training the neural network.This idea is so powerful that even today it is one of the fundamental building blocks of what we call AI.From this I will try to explain how this simple concept can have such diverse applications as natural language processing (think Alexa), image recognition like medical diagnosis from a CTR scan, autonomous vehicles, etc.A basic neural network is a combination of perceptrons in different arrangements, the perceptron therefore was downgraded from “fly brain” to “network neuron”.A neural network has different components, in its basic form it has: Input Hidden layers OutputInputThe inputs of a neural network are in their essence just numbers, therefore anything that can be converted to a number can become an input. Letters in a text, pixels in an image, frequencies in a sound wave, values from a sensor, etc. are all different things that when converted to a numerical value serve as inputs for the neural network. This is one of the reasons why applications of neural networks are so diverse.Inputs can be as many as one need for the task at hand, from maybe 9 inputs to teach a neural network how to play tic-tac-toe to thousands of pixels from a camera for an autonomous vehicle. Since the input of a perceptron needs to be a single value, if for example a color pixel is chosen as input, it most likely will be broken into three different values; its red, green and blue components, hence each pixel will become 3 different inputs for the neural network.Hidden layersA “layer” within a neural network is just a group of perceptrons that all perform the same exact mathematical operation to the inputs and produce an output. The catch is that each of them have different weights (parameters), therefore their output for a given input will be different amongst them. There are many types of layers, the most typical of them being a “dense” layer, which is another word to say that all the inputs are connected to all the neurons (individual perceptrons), and as said before, each of these connections have a weight associated with it, so that the operation that each neuron performs is a simple weighted sum of all the inputs.The hidden layer is then typically connected to another dense layer, and their connection means that each output of a neuron from the first layer is treated effectively as an input for the subsequent one, and it is thus connected to every neuron.A neural network can have from one to as many layers as one can think, and the number of layers depends solely on the experience we have gathered on the particular problem we would like to solve.Another critical parameter of a hidden layer is the number of neurons it has, and again, we need to rely on experience to determine how many neurons are needed for a given problem. I have seen networks that vary from a couple of neurons to the thousands. And of course each hidden layer can have as many neurons as we please, so the number of combinations is vast.To the number of layers, their type and how many neurons each have, is what we call the network topology (including the number of inputs and outputs).OutputAt the very end of the chain, another layer lies (which behaves just like a hidden layer), but has the peculiarity that it is the final layer, and therefore whatever it calculates will be the output values of the whole network. The number of outputs the network has is a function of the problem we would like to solve. It could be as simple as one output, with its value representing a probability of an action (like in the case of the flee reaction of the housefly), to many outputs, perhaps if our network is trying to distinguish images of animals, one would have an output for each animal species, and the output would represent how much confidence the network has that the particular image belongs to the corresponding species.As we said, the neural network is just a collection of individual neurons, doing basic math operations on certain inputs in series of layers that eventually generate an output. This mesh of neurons is then “trained” on certain output values from known cases of the inputs; once it has learned it can then process new inputs, values that it has never seen before with surprisingly accurate results.Many of the problems neural networks solve, could be certainly worked out by other algorithms, however, since neural networks are in their core very basic operations, once trained, they are extremely efficient, hence much quicker and economical to produce results.There are a few more details on how a simple neural network operate that I purposedly left out to make this explanation as simple as possible. Thinks like biases, the activation functions and the math behind learning, the backpropagation algorithm, I will leave to a more in depth article. I will also write (perhaps in a series) about the more complex topologies combining different types of layers and other building blocks, a part from the perceptron.Things like “Alexa”, are a bit more complex, but work on exactly the same principles. Let’s break down for example the case of asking “Alexa” to play a song in spotify. Alexa uses several different neural networks to acomplish this:1. Speech recognitionAs a basic input we have our speech: the command “Alexa, play Van Halen”. This might seem quite simple for us humans to process, but for a machine is an incredible difficult feat to be able to understand speech, things like each individual voice timbre, entonation, intention and many more nuances of human spoken language make it so that traditional algorithms have struggled a lot with this. In our simplified example let’s say that we use a neural network to transform our spoken speech into text characters a computer is much more familiarized to learn.2. Understanding what we mean (Natural Language Understanding)Once the previous network managed to succesfuly convert our spoken words into text, there comes the even more difficult task of making sense of what we said. Things that we humans take for granted such as context, intonation and non verbal communication, help give our words meaning in a very subtle, but powerful way, a machine will have to do with much less information to correctly understand what we mean. It has to correctly identify the intention of our sentence and the subject or entities of what we mean.The neural network has to identify that it received a command (by identifying its name), the command (“play music”), and our choice (“Van Halen”). And it does so by means of simple math operations as described before. Of course the network involved is quite complex and has different types of neurons and connection types, but the underlying principles remain.3. Replying to usOnce Alexa understood what we meant, it then proceeds to execute the action of the command it interpreted and it replies to us in turn using natural language. This is accomplished using a technique called speech synthesis, things like pitch, duration and intensity of the words and phonems are selected based on the “meaning” of what Alexa will respond to us: “Playing songs by Van Halen on Spotify” sounding quite naturally. And all is accomplished with neural networks executing many simple math operations.Although it seems quite complex, the process for AI to understand us can be boiled down to simple math operationsOf course Amazon’s Alexa neural networks have undergone quite a lot of training to get to the level where they are, the beauty is that once trained, to perform their magic they just need a few mathematical operations.As said before, I will continue to write about the basics of neural networks, the next article in the series will dive a bit deeper into the math behind a basic neural network."
    } ,
  
    {
      "title"       : "Starting the adventure",
      "category"    : "",
      "tags"        : "general blogging, thoughts, life",
      "url"         : "./starting-the-adventure.html",
      "date"        : "2021-03-24 00:00:00 +0900",
      "description" : "Midlife career change: a disaster or an opportunity?",
      "content"     : "In the midst of a global pandemic caused by the SARS-COV2 coronavirus; I decided to start blogging. I wanted to blog since a long time, I have always enjoyed writing, but many unknowns and having “no time” for it prevented me from taking it up. Things like: “I don’t really know who my target audience is”, “what would my topic or topics be?”, “I don’t think I am a world-class expert in anything”, and many more kept stopping me from setting up my own blog. Now seemed like a good time as any so with those and tons of other questions in my mind I decided it was time to start.Funnily, this is not my first post. The birth of the blog came very natural as a way to “document” my newly established pursuit for getting myself into Machine Learning. This new adventure of mine comprises several things, and if I want to succeed I need to be serious about them all: I want to start coding again! I used to code a long time ago, starting when I was 8 years old in a Tandy Color Computer hooked up to my parent’s TV. Machine Learning is a vast, wide subject, I want to learn the generals, but also to select a few areas to focus on. Setting up a blog to document my journey and share it: Establish a learning and blogging routine. If I don’t do this, I am sure this endeavour will die off soon.As for the focus areas I will start with: Neural Networks fundamentals: history, basic architecture and math behind them Deep Neural Networks Reinforcement Learning Current state of the art: what is at the cutting edge now in terms of Deep Neural Networks and Reinforcement Learning?I selected the above areas to focus on based on my personal interests, I have been fascinated by the developments in reinforcement learning for a long time, in particular Deep Mind’s awesome Go, Chess and Starcraft playing agents. Therefore, I started reading a lot about it and even started a personal project for coding a tic-tac-toe learning agent.With my limited knowledge I have drafted the following learning path: Youtube: Three Blue One Brown’s videos on Neural Networks, Calculus and Linear Algebra. I cannot recommend them enough, they are of sufficient depth and use animation superbly to facilitate the understanding of the subjects. Coursera: Andrew Ng’s Machine Learning course Book: Deep Learning with Python by Francois Chollet Book: Reinforcement Learning: An Introduction, by Richard S. Sutton and Andrew G. BartoAs for practical work I decided to start by coding my first models from scratch (without using libraries such as Tensorflow), to be able to deeply understand the math and logic behind the models, so far it has proven to be priceless.For my next project I think I will start to do the basic hand-written digits recognition, which is the Machine Learning Hello World, for this I think I will start to use Tensorflow already.I will continue to write about my learning road, what I find interesting and relevant, and to document all my practical exercises, as well as news and the state of the art in the world of AI.So far, all I have learned has been so engaging that I am seriously thinking of a career change. I have 17 years of international experience in multinational corporations across various functions, such as Information Services, Sales, Customer Care and New Products Introduction, and sincerely, I am finding more joy in artificial intelligence than anything else I have worked on before. Let’s see where the winds take us.Thanks for reading!P.S. For the geeks like me, here is a snippet on the technical side of the blog.Static Website GeneratorI researched a lot on this, when I started I didn’t even know I needed a static website generator. I was just sure of one thing, I wanted my blog site to look modern, be easy to update and not to have anything extra or additional content or functionality I did not need.There is a myriad of website generators nowadays, after a lengthy search the ones I ended up considering are: wordpress wix squarespace ghost webflow netlify hugo gatsby jekyllI started with the web interfaced generators with included hosting in their offerings:wordpress is the old standard, it is the one CMS I knew from before, and I thought I needed a fully fledged CMS, so I blindly ran towards it. Turns out, it has grown a lot since I remembered, it is now a fully fledged platform for complex websites and ecommerce development, even so I decided to give it a try, I picked a template and created a site. Even with the most simplistic and basic template I could find, there is a lot going on in the site. Setting it up was not as difficult or cumbersome as others claim, it took me about one hour to have it up and running, it looks good, but a bit crowded for my personal taste, and I found out it serves ads in your site for the readers, that is a big no for me.I have tried wix and squarespace before, they are fantastic for quick and easy website generation, but their free offering has ads, so again, a big no for me.I discovered ghost as the platform used by one of the bloggers I follow (Sebastian Ruder), turns out is a fantastic evolution over wordpress. It runs on the latest technologies, its interface is quite modern, and it is focused on one thing only: publishing. They have a paid hosting service, but the software is open sourced, therefore free to use in any hosting.I also tested webflow and even created a mockup there, the learning curve was quite smooth, and its CMS seems quite robust, but a bit too much for the functionalities I required.Next were the generators that don’t have a web interface, but can be easily set up:The first I tried was netlify, I also set up a test site in it. Netlify provides free hosting, and to keep your source files it uses GitHub (a repository keeps the source files where it publishes from). It has its own CMS, Netlify CMS, and you have a choice of site generators: Hugo, Gatsby, MiddleMan, Preact CLI, Next.js, Elevently and Nuxt.js, and once you choose there are some templates for each. I did not find the variety of templates enticing enough, and the set up process was much more cumbersome than with wordpress (at least for my knowledge level). I choose Hugo for my test site.I also tested gatsby with it’s own Gatsby Cloud hosting service, here is my test site. They also use GitHub as a base to host the source files to build the website, so you create a repository, and it is connected to it. I found the free template offerings quite limited for what I was looking for.Finally it came the turn for jekyll, although an older, and slower generator (compared to Hugo and Gatsby), it was created by one of the founders of GitHub, so it’s integration with GitHub Pages is quite natural and painless, so much so, that to use them together you don’t even have to install Jekyll in your machine! You have two choices: keep it all online, by having one repository in Github keep all the source files, modify or add them online, and having Jekyll build and publish your site to the special gh-pages repository everytime you change or add a new file to the source repository. Have a synchronized local copy of the source files for the website, this way you can edit your blog and customize it in your choice of IDE (Integrated Development Environment). Then, when you update any file on your computer, you just “push” the changes to GitHub, and GitHub Pages automatically uses Jekyll to build and publish your site.I chose the second option, specially because I can manipulate files, like images, in my laptop, and everytime I sync my local repository with GitHub, they are updated and published automatically. Quite convenient.After testing with several templates to get the feel for it, I decided to keep Jekyll for my blog for several reasons: the convenience of not having to install anything extra on my computer to build my blog, the integration with GitHub Pages, the ease of use, the future proofing via integration with modern technologies such as react or vue and the vast online community that has produced tons of templates and useful information for issue resolution, customization and added functionality.I picked up a template, just forked the repository and started modifying the files to customize it, it was fast and easy, I even took it upon myself to add some functionality to the template (it served as a coding little project) like: SEO meta tags Dark mode (configurable in _config.yml file) automatic sitemap.xml automatic archive page with infinite scrolling capability new page of posts filtered by a single tag (without needing autopages from paginator V2), also with infinite scrolling click to tweet functionality (just add a &lt;tweet&gt; &lt;/tweet&gt; tag in your markdown. custom and responsive 404 page responsive and automatic Table of Contents (optional per post) read time per post automatically calculated responsive post tags and social share icons (sticky or inline) included linkedin, reddit and bandcamp icons copy link to clipboard sharing option (and icon) view on github link button (optional per post) MathJax support (optional per post) tag cloud in the home page ‘back to top’ button comments ‘courtain’ to mask the disqus interface until the user clicks on it (configurable in _config.yml) CSS variables to make it easy to customize all colors and fonts added several pygments themes for code syntax highlight configurable from the _config.yml file. See the highlighter directory for reference on the options. responsive footer menu and footer logo (if setup in the config file) smoother menu animationsAs a summary, Hugo and Gatsby might be much faster than Jekyll to build the sites, but their complexity I think makes them useful for a big site with plenty of posts. For a small site like mine, Jekyll provides sufficient functionality and power without the hassle.You can use the modified template yourself by forking my repository. Let me know in the comments or feel free to contact me if you are interested in a detailed walkthrough on how to set it all up.HostingSince I decided on Jekyll to generate my site, the choice for hosting was quite obvious, Github Pages is very nicely integrated with it, it is free, and it has no ads! Plus the domain name isn’t too terrible (the-mvm.github.io).Interplanetary File SystemTo contribute to and test IPFS I also set up a mirror in IPFS by using fleek.co. I must confess that it was more troublesome than I imagined, it was definetively not plug and play because of the paths used to fetch resources. The nature of IPFS makes short absolute paths for website resources (like images, css and javascript files) inoperative; the easiest fix for this is to use relative paths, however the same relative path that works for the root directory (i.e. /index.html) does not work for links inside directories (i.e. /tags/), and since the site is static, while generating it, one must make the distinction between the different directory levels for the page to be rendered correctly.At first I tried a simple (but brute force solution):# determine the level of the current file{% assign lvl = page.url | append:'X' | split:'/' | size %}# create the relative base (i.e. \"../\"){% capture relativebase %}{% for i in (3..lvl) %}../{% endfor %}{% endcapture %}{% if relativebase == '' %} {% assign relativebase = './' %}{% endif %}...# Eliminate unecesary double backslashes{% capture post_url %}{{ relativebase }}{{ post.url }}{% endcapture %}{% assign post_url = post_url | replace: \"//\", \"/\" %}This jekyll/liquid code was executed in every page (or include) that needed to reference a resource hosted in the same server.But this fix did not work for the search function, because it relies on a search.json file (also generated programmatically to be served as a static file), therefore when generating this file one either use the relative path for the root directory or for a nested directory, thus the search results will only link correctly the corresponding pages if the page where the user searched for something is in the corresponding scope.So the final solution was to make the whole site flat, meaning to live in a single directory. All pages and posts will live under the root directory, and by doing so, I can control how to address the relative paths for resources."
    } ,
  
    {
      "title"       : "Deep Q Learning for Tic Tac Toe",
      "category"    : "",
      "tags"        : "machine learning, artificial intelligence, reinforcement learning, coding, python",
      "url"         : "./deep-q-learning-tic-tac-toe.html",
      "date"        : "2021-03-19 06:14:20 +0900",
      "description" : "Inspired by Deep Mind's astonishing feats of having their Alpha Go, Alpha Zero and Alpha Star programs learn (and be amazing at it) Go, Chess, Atari games and lately Starcraft; I set myself to the task of programming a neural network that will learn by itself how to play the ancient game of tic tac toe. How hard could it be?",
      "content"     : "BackgroundAfter many years of a corporate career (17) diverging from computer science, I have now decided to learn Machine Learning and in the process return to coding (something I have always loved!).To fully grasp the essence of ML I decided to start by coding a ML library myself, so I can fully understand the inner workings, linear algebra and calculus involved in Stochastic Gradient Descent. And on top learn Python (I used to code in C++ 20 years ago).I built a general purpose basic ML library that creates a Neural Network (only DENSE layers), saves and loads the weights into a file, does forward propagation and training (optimization of weights and biases) using SGD. I tested the ML library with the XOR problem to make sure it worked fine. You can read the blog post for it here.For the next challenge I am interested in reinforcement learning greatly inspired by Deep Mind’s astonishing feats of having their Alpha Go, Alpha Zero and Alpha Star programs learn (and be amazing at it) Go, Chess, Atari games and lately Starcraft; I set myself to the task of programming a neural network that will learn by itself how to play the ancient game of tic tac toe (or noughts and crosses).How hard could it be?Of course the first thing to do was to program the game itself, so I chose Python because I am learning it, so it gives me a good practice opportunity, and PyGame for the interface.Coding the game was quite straightforward, albeit for the hiccups of being my first PyGame and almost my first Python program ever.I created the game quite openly, in such a way that it can be played by two humans, by a human vs. an algorithmic AI, and a human vs. the neural network. And of course the neural network against a choice of 3 AI engines: random, minimax or hardcoded (an exercise I wanted to do since a long time).While training, the visuals of the game can be disabled to make training much faster.Now, for the fun part, training the network, I followed Deep Mind’s own DQN recommendations:The network will be an approximation for the Q value function or Bellman equation, meaning that the network will be trained to predict the \"value\" of each move available in a given game state.A replay experience memory was implemented. This meant that the neural network will not be trained after each move. Each move will be recorded in a special \"memory\" alongside with the state of the board and the reward it received for taking such an action (move).After the memory is sizable enough, batches of random experiences sampled from the replay memory are used for every training roundA secondary neural network (identical to the main one) is used to calculate part of the Q value function (Bellman equation), in particular the future Q values. And then it is updated with the main network's weights every n games. This is done so that we are not chasing a moving target.Designing the neural networkThe Neural Network chosen takes 9 inputs (the current state of the game) and outputs 9 Q values for each of the 9 squares in the board of the game (possible actions). Obviously some squares are illegal moves, hence while training there was a negative reward given to illegal moves hoping that the model would learn not to play illegal moves in a given position.I started out with two hidden layers of 36 neurons each, all fully connected and activated via ReLu. The output layer was initially activated using sigmoid to ensure that we get a nice value between 0 and 1 that represents the QValue of a given state action pair.The many models…Model 1 - the first tryAt first the model was trained by playing vs. a “perfect” AI, meaning a hard coded algorithm that never looses and that will win if it is given the chance. After several thousand training rounds, I noticed that the Neural Network was not learning much; so I switched to training vs. a completely random player, so that it will also learn how to win. After training vs. the random player, the Neural Network seems to have made progress and is steadily diminishing the loss function over time.However, the model was still generating many illegal moves, so I decided to modify the reinforcement learning algorithm to punish more the illegal moves. The change consisted in populating with zeros all the corresponding illegal moves for a given position at the target values to train the network. This seemed to work very well for diminishing the illegal moves:Nevertheless, the model was still performing quite poorly winning only around 50% of games vs. a completely random player (I expected it to win above 90% of the time). This was after only training 100,000 games, so I decided to keep training and see the results:Wins: 65.46% Losses: 30.32% Ties: 4.23%Note that when training restarts, the loss and illegal moves are still high in the beginning of the training round, and this is caused by the epsilon greedy strategy that prefers exploration (a completely random move) over exploitation, this preference diminishes over time.After another round of 100,000 games, I can see that the loss function actually started to diminish, and the win rate ended up at 65%, so with little hope I decided to carry on and do another round of 100,000 games (about 2 hours in an i7 MacBook Pro):Wins: 46.40% Losses: 41.33% Ties: 12.27%As you can see in the chart, the calculated loss not even plateaued, but it seemed to increase a bit over time, which tells me the model is not learning anymore. This was confirmed by the win rate decreasing with respect of the previous round to a meek 46.4% that looks no better than a random player.Model 2 - Linear activation for the outputAfter not getting the results I wanted, I decided to change the output activation function to linear, since the output is supposed to be a Q value, and not a probability of an action.Wins: 47.60% Losses: 39% Ties: 13.4%Initially I tested with only 1000 games to see if the new activation function was working, the loss function appears to be decreasing, however it reached a plateau around a value of 1, hence still not learning as expected. I came across a technique by Brad Kenstler, Carl Thome and Jeremy Jordan called Cyclical Learning Rate, which appears to solve some cases of stagnating loss functions in this type of networks. So I gave it a go using their Triangle 1 model.With the cycling learning rate in place, still no luck after a quick 1,000 games training round; so I decided to implement on top a decaying learning rate as per the following formula:The resulting learning rate combining the cycles and decay per epoch is:Learning Rate = 0.1, Decay = 0.0001, Cycle = 2048 epochs, max Learning Rate factor = 10xtrue_epoch = epoch - c.BATCH_SIZElearning_rate = self.learning_rate*(1/(1+c.DECAY_RATE*true_epoch))if c.CLR_ON: learning_rate = self.cyclic_learning_rate(learning_rate,true_epoch)@staticmethoddef cyclic_learning_rate(learning_rate, epoch): max_lr = learning_rate*c.MAX_LR_FACTOR cycle = np.floor(1+(epoch/(2*c.LR_STEP_SIZE))) x = np.abs((epoch/c.LR_STEP_SIZE)-(2*cycle)+1) return learning_rate+(max_lr-learning_rate)*np.maximum(0,(1-x))c.DECAY_RATE = learning rate decay ratec.MAX_LR_FACTOR = multiplier that determines the max learning ratec.LR_STEP_SIZE = the number of epochs each cycle lastsWith these many changes, I decided to restart with a fresh set of random weights and biases and try training more (much more) games.1,000,000 episodes, 7.5 million epochs with batches of 64 moves eachWins: 52.66% Losses: 36.02% Ties: 11.32%After 24 hours!, my computer was able to run 1,000,000 episodes (games played), which represented 7.5 million training epochs of batches of 64 plays (480 million plays learned), the learning rate did decreased (a bit), but is clearly still in a plateau; interestingly, the lower boundary of the loss function plot seems to continue to decrease as the upper bound and the moving average remains constant. This led me to believe that I might have hit a local minimum.Model 3 - new network topologyAfter all the failures I figured I had to rethink the topology of the network and play around with combinations of different networks and learning rates.100,000 episodes, 635,000 epochs with batches of 64 moves eachWins: 76.83% Losses: 17.35% Ties: 5.82%I increased to 200 neurons each hidden layer. In spite of this great improvement the loss function was still in a plateau at around 0.1 (Mean Squared Error). Which, although it is greatly reduced from what we had, still was giving out only 77% win rate vs. a random player, the network was playing tic tac toe as a toddler!*I can still beat the network most of the time! (I am playing with the red X)*100,000 more episodes, 620,000 epochs with batches of 64 moves eachWins: 82.25% Losses: 13.28% Ties: 4.46%Finally we crossed the 80% mark! This is quite an achievement, it seems that the change in network topology is working, although it also looks like the loss function is stagnating at around 0.15.After more training rounds and some experimenting with the learning rate and other parameters, I couldn’t improve past the 82.25% win rate.These have been the results so far:It is quite interesting to learn how the many parameters (hyper-parameters as most authors call them) of a neural network model affect its training performance, I have played with: the learning rate the network topology and activation functions the cycling and decaying learning rate parameters the batch size the target update cycle (when the target network is updated with the weights from the policy network) the rewards policy the epsilon greedy strategy whether to train vs. a random player or an “intelligent” AI.And so far the most effective change has been the network topology, but being so close but not quite there yet to my goal of 90% win rate vs. a random player, I will still try to optimize further.Network topology seems to have the biggest impact on a neural network's learning ability.Model 4 - implementing momentumI reached out to the reddit community and a kind soul pointed out that maybe what I need is to apply momentum to the optimization algorithm. So I did some research and ended up deciding to implement various optimization methods to experiment with: Stochastic Gradient Descent with Momentum RMSProp: Root Mean Square Plain Momentum NAG: Nezterov’s Accelerated Momentum Adam: Adaptive Moment Estimation and keep my old vanilla Gradient Descent (vGD) ☺Click here for a detailed explanation and code of all the implemented optimization algorithms.So far, I have not been able to get better results with Model 4, I have tried all the momentum optimization algorithms with little to no success.Model 5 - implementing one-hot encoding and changing topology (again)I came across an interesting project in Github that deals exactly with Deep Q Learning, and I noticed that he used “one-hot” encoding for the input as opposed to directly entering the values of the player into the 9 input slots. So I decided to give it a try and at the same time change my topology to match his:So, ‘one hot’ encoding is basically changing the input of a single square in the tic tac toe board to three numbers, so that each state is represented with different inputs, thus the network can clearly differentiate the three of them. As the original author puts it, the way I was encoding, having 0 for empty, 1 for X and 2 for O, the network couldn’t easily tell that, for instance, O and X both meant occupied states, because one is two times as far from 0 as the other. With the new encoding, the empty state will be 3 inputs: (1,0,0), the X will be (0,1,0) and the O (0,0,1) as in the diagram.Still, no luck even with Model 5, so I am starting to think that there could be a bug in my code.To test this hypothesis, I decided to implement the same model using Tensorflow / Keras.Model 6 - Tensorflow / Kerasself.PolicyNetwork = Sequential()for layer in hidden_layers: self.PolicyNetwork.add(Dense( units=layer, activation='relu', input_dim=inputs, kernel_initializer='random_uniform', bias_initializer='zeros'))self.PolicyNetwork.add(Dense( outputs, kernel_initializer='random_uniform', bias_initializer='zeros'))opt = Adam(learning_rate=c.LEARNING_RATE, beta_1=c.GAMMA_OPT, beta_2=c.BETA, epsilon=c.EPSILON, amsgrad=False)self.PolicyNetwork.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])As you can see I am reusing all of my old code, and just replacing my Neural Net library with Tensorflow/Keras, keeping even my hyper-parameter constants.The training function changed to:reduce_lr_on_plateau = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=25)history = self.PolicyNetwork.fit(np.asarray(states_to_train), np.asarray(targets_to_train), epochs=c.EPOCHS, batch_size=c.BATCH_SIZE, verbose=1, callbacks=[reduce_lr_on_plateau], shuffle=True)With Tensorflow implemented, the first thing I noticed, was that I had an error in the calculation of the loss, although this only affected reporting and didn’t change a thing on the training of the network, so the results kept being the same, the loss function was still stagnating! My code was not the issue.Model 7 - changing the training scheduleNext I tried to change the way the network was training as per u/elBarto015 advised me on reddit.The way I was training initially was: Games begin being simulated and the outcome recorded in the replay memory Once a sufficient ammount of experiences are recorded (at least equal to the batch size) the Network will train with a random sample of experiences from the replay memory. The ammount of experiences to sample is the batch size. The games continue to be played between the random player and the network. Every move from either player generates a new training round, again with a random sample from the replay memory. This continues until the number of games set up conclude.The first change was to train only after every game concludes with the same ammount of data (a batch). This was still not giving any good results.The second change was more drastic, it introduced the concept of epochs for every training round, it basically sampled the replay memory for epochs * batch size experiences, for instance if epochs selected were 10, and batch size was 81, then 810 experiences were sampled out of the replay memory. With this sample the network was then trained for 10 epochs randomly using the batch size.This meant that I was training now effectively 10 (or the number of epochs selected) times more per game, but in batches of the same size and randomly shuffling the experiences each epoch.After still playing around with some hyperparameters I managed to get similar performance as I got before, reaching 83.15% win rate vs. the random player, so I decided to keep training in rounds of 2,000 games each to evaluate performance. With almost every round I could see improvement:As of today, my best result so far is 87.5%, I will leave it rest for a while and keep investigating to find a reason for not being able to reach at least 90%. I read about self play, and it looks like a viable option to test and a fun coding challenge. However, before embarking in yet another big change I want to ensure I have been thorough with the model and have tested every option correctly.I feel the end is near… should I continue to update this post as new events unfold or shall I make it a multi post thread?"
    } ,
  
    {
      "title"       : "Neural Network Optimization Methods and Algorithms",
      "category"    : "",
      "tags"        : "coding, machine learning, optimization, deep Neural networks",
      "url"         : "./neural-network-optimization-methods.html",
      "date"        : "2021-03-13 04:32:20 +0900",
      "description" : "Some neural network optimization algorithms mostly to implement momentum when doing back propagation.",
      "content"     : "For the seemingly small project I undertook of creating a machine learning neural network that could learn by itself to play tic-tac-toe, I bumped into the necesity of implementing at least one momentum algorithm for the optimization of the network during backpropagation.And since my original post for the TicTacToe project is quite large already, I decided to post separately these optimization methods and how did I implement them in my code.AdamsourceAdaptive Moment Estimation (Adam) is an optimization method that computes adaptive learning rates for each weight and bias. In addition to storing an exponentially decaying average of past squared gradients \\(v_t\\) and an exponentially decaying average of past gradients \\(m_t\\), similar to momentum. Whereas momentum can be seen as a ball running down a slope, Adam behaves like a heavy ball with friction, which thus prefers flat minima in the error surface. We compute the decaying averages of past and past squared gradients \\(m_t\\) and \\(v_t\\) respectively as follows:\\(\\begin{align}\\begin{split}m_t &amp;= \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\v_t &amp;= \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\\end{split}\\end{align}\\)\\(m_t\\) and \\(v_t\\) are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively, hence the name of the method. As \\(m_t\\) and \\(v_t\\) are initialized as vectors of 0's, the authors of Adam observe that they are biased towards zero, especially during the initial time steps, and especially when the decay rates are small (i.e. \\(\\beta_1\\) and \\(\\beta_2\\) are close to 1).They counteract these biases by computing bias-corrected first and second moment estimates:\\(\\begin{align}\\begin{split}\\hat{m}_t &amp;= \\dfrac{m_t}{1 - \\beta^t_1} \\\\\\hat{v}_t &amp;= \\dfrac{v_t}{1 - \\beta^t_2} \\end{split}\\end{align}\\)We then use these to update the weights and biases which yields the Adam update rule:\\(\\theta_{t+1} = \\theta_{t} - \\dfrac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t\\).The authors propose defaults of 0.9 for \\(\\beta_1\\), 0.999 for \\(\\beta_2\\), and \\(10^{-8}\\) for \\(\\epsilon\\).view on github# decaying averages of past gradientsself.v[\"dW\" + str(i)] = ((c.BETA1 * self.v[\"dW\" + str(i)]) + ((1 - c.BETA1) * np.array(self.gradients[i]) ))self.v[\"db\" + str(i)] = ((c.BETA1 * self.v[\"db\" + str(i)]) + ((1 - c.BETA1) * np.array(self.bias_gradients[i]) ))# decaying averages of past squared gradientsself.s[\"dW\" + str(i)] = ((c.BETA2 * self.s[\"dW\"+str(i)]) + ((1 - c.BETA2) * (np.square(np.array(self.gradients[i]))) ))self.s[\"db\" + str(i)] = ((c.BETA2 * self.s[\"db\" + str(i)]) + ((1 - c.BETA2) * (np.square(np.array( self.bias_gradients[i]))) ))if c.ADAM_BIAS_Correction: # bias-corrected first and second moment estimates self.v[\"dW\" + str(i)] = self.v[\"dW\" + str(i)] / (1 - (c.BETA1 ** true_epoch)) self.v[\"db\" + str(i)] = self.v[\"db\" + str(i)] / (1 - (c.BETA1 ** true_epoch)) self.s[\"dW\" + str(i)] = self.s[\"dW\" + str(i)] / (1 - (c.BETA2 ** true_epoch)) self.s[\"db\" + str(i)] = self.s[\"db\" + str(i)] / (1 - (c.BETA2 ** true_epoch))# apply to weights and biasesweight_col -= ((eta * (self.v[\"dW\" + str(i)] / (np.sqrt(self.s[\"dW\" + str(i)]) + c.EPSILON))))self.bias[i] -= ((eta * (self.v[\"db\" + str(i)] / (np.sqrt(self.s[\"db\" + str(i)]) + c.EPSILON))))SGD MomentumsourceVanilla SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another, which are common around local optima. In these scenarios, SGD oscillates across the slopes of the ravine while only making hesitant progress along the bottom towards the local optimum.Momentum is a method that helps accelerate SGD in the relevant direction and dampens oscillations. It does this by adding a fraction \\(\\gamma\\) of the update vector of the past time step to the current update vector:\\(\\begin{align}\\begin{split}v_t &amp;= \\beta_1 v_{t-1} + \\eta \\nabla_\\theta J( \\theta) \\\\\\theta &amp;= \\theta - v_t\\end{split}\\end{align}\\)The momentum term \\(\\beta_1\\) is usually set to 0.9 or a similar value.Essentially, when using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill, becoming faster and faster on the way (until it reaches its terminal velocity if there is air resistance, i.e. \\(\\beta_1 &lt; 1\\)). The same thing happens to our weight and biases updates: The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain faster convergence and reduced oscillation.view on githubself.v[\"dW\"+str(i)] = ((c.BETA1*self.v[\"dW\" + str(i)]) +(eta*np.array(self.gradients[i]) ))self.v[\"db\"+str(i)] = ((c.BETA1*self.v[\"db\" + str(i)]) +(eta*np.array(self.bias_gradients[i]) ))weight_col -= self.v[\"dW\" + str(i)]self.bias[i] -= self.v[\"db\" + str(i)]Nesterov accelerated gradient (NAG)sourceHowever, a ball that rolls down a hill, blindly following the slope, is highly unsatisfactory. We'd like to have a smarter ball, a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again.Nesterov accelerated gradient (NAG) is a way to give our momentum term this kind of prescience. We know that we will use our momentum term \\(\\beta_1 v_{t-1}\\) to move the weights and biases \\(\\theta\\). Computing \\( \\theta - \\beta_1 v_{t-1} \\) thus gives us an approximation of the next position of the weights and biases (the gradient is missing for the full update), a rough idea where our weights and biases are going to be. We can now effectively look ahead by calculating the gradient not w.r.t. to our current weights and biases \\(\\theta\\) but w.r.t. the approximate future position of our weights and biases:\\(\\begin{align}\\begin{split}v_t &amp;= \\beta_1 v_{t-1} + \\eta \\nabla_\\theta J( \\theta - \\beta_1 v_{t-1} ) \\\\\\theta &amp;= \\theta - v_t\\end{split}\\end{align}\\)Again, we set the momentum term \\(\\beta_1\\) to a value of around 0.9. While Momentum first computes the current gradient and then takes a big jump in the direction of the updated accumulated gradient, NAG first makes a big jump in the direction of the previous accumulated gradient, measures the gradient and then makes a correction, which results in the complete NAG update. This anticipatory update prevents us from going too fast and results in increased responsiveness, which has significantly increased the performance of Neural Networks on a number of tasks.Now that we are able to adapt our updates to the slope of our error function and speed up SGD in turn, we would also like to adapt our updates to each individual weight and bias to perform larger or smaller updates depending on their importance.view on githubv_prev = {\"dW\" + str(i): self.v[\"dW\" + str(i)], \"db\" + str(i): self.v[\"db\" + str(i)]}self.v[\"dW\" + str(i)] = (c.NAG_COEFF * self.v[\"dW\" + str(i)] - eta * np.array(self.gradients[i]))self.v[\"db\" + str(i)] = (c.NAG_COEFF * self.v[\"db\" + str(i)] - eta * np.array(self.bias_gradients[i]))weight_col += ((-1 * c.BETA1 * v_prev[\"dW\" + str(i)]) + (1 + c.BETA1) * self.v[\"dW\" + str(i)])self.bias[i] += ((-1 * c.BETA1 * v_prev[\"db\" + str(i)]) + (1 + c.BETA1) * self.v[\"db\" + str(i)])RMSpropsourceRMSprop is an unpublished, adaptive learning rate method proposed by Geoff Hinton in Lecture 6e of his Coursera Class.RMSprop was developed stemming from the need to resolve other method's radically diminishing learning rates.\\(\\begin{align}\\begin{split}E[\\theta^2]_t &amp;= \\beta_1 E[\\theta^2]_{t-1} + (1-\\beta_1) \\theta^2_t \\\\\\theta_{t+1} &amp;= \\theta_{t} - \\dfrac{\\eta}{\\sqrt{E[\\theta^2]_t + \\epsilon}} \\theta_{t}\\end{split}\\end{align}\\)RMSprop divides the learning rate by an exponentially decaying average of squared gradients. Hinton suggests \\(\\beta_1\\) to be set to 0.9, while a good default value for the learning rate \\(\\eta\\) is 0.001.view on githubself.s[\"dW\" + str(i)] = ((c.BETA1 * self.s[\"dW\" + str(i)]) + ((1-c.BETA1) * (np.square(np.array(self.gradients[i]))) ))self.s[\"db\" + str(i)] = ((c.BETA1 * self.s[\"db\" + str(i)]) + ((1-c.BETA1) * (np.square(np.array(self.bias_gradients[i]))) ))weight_col -= (eta * (np.array(self.gradients[i]) / (np.sqrt(self.s[\"dW\"+str(i)]+c.EPSILON))) )self.bias[i] -= (eta * (np.array(self.bias_gradients[i]) / (np.sqrt(self.s[\"db\"+str(i)]+c.EPSILON))) )Complete codeAll in all the code ended up like this:view on github@staticmethoddef cyclic_learning_rate(learning_rate, epoch): max_lr = learning_rate * c.MAX_LR_FACTOR cycle = np.floor(1 + (epoch / (2 * c.LR_STEP_SIZE)) ) x = np.abs((epoch / c.LR_STEP_SIZE) - (2 * cycle) + 1) return learning_rate + (max_lr - learning_rate) * np.maximum(0, (1 - x))def apply_gradients(self, epoch): true_epoch = epoch - c.BATCH_SIZE eta = self.learning_rate * (1 / (1 + c.DECAY_RATE * true_epoch)) if c.CLR_ON: eta = self.cyclic_learning_rate(eta, true_epoch) for i, weight_col in enumerate(self.weights): if c.OPTIMIZATION == 'vanilla': weight_col -= eta * np.array(self.gradients[i]) / c.BATCH_SIZE self.bias[i] -= eta * np.array(self.bias_gradients[i]) / c.BATCH_SIZE elif c.OPTIMIZATION == 'SGD_momentum': self.v[\"dW\"+str(i)] = ((c.BETA1 *self.v[\"dW\" + str(i)]) +(eta *np.array(self.gradients[i]) )) self.v[\"db\"+str(i)] = ((c.BETA1 *self.v[\"db\" + str(i)]) +(eta *np.array(self.bias_gradients[i]) )) weight_col -= self.v[\"dW\" + str(i)] self.bias[i] -= self.v[\"db\" + str(i)] elif c.OPTIMIZATION == 'NAG': v_prev = {\"dW\" + str(i): self.v[\"dW\" + str(i)], \"db\" + str(i): self.v[\"db\" + str(i)]} self.v[\"dW\" + str(i)] = (c.NAG_COEFF * self.v[\"dW\" + str(i)] - eta * np.array(self.gradients[i])) self.v[\"db\" + str(i)] = (c.NAG_COEFF * self.v[\"db\" + str(i)] - eta * np.array(self.bias_gradients[i])) weight_col += ((-1 * c.BETA1 * v_prev[\"dW\" + str(i)]) + (1 + c.BETA1) * self.v[\"dW\" + str(i)]) self.bias[i] += ((-1 * c.BETA1 * v_prev[\"db\" + str(i)]) + (1 + c.BETA1) * self.v[\"db\" + str(i)]) elif c.OPTIMIZATION == 'RMSProp': self.s[\"dW\" + str(i)] = ((c.BETA1 *self.s[\"dW\" + str(i)]) +((1-c.BETA1) *(np.square(np.array(self.gradients[i]))) )) self.s[\"db\" + str(i)] = ((c.BETA1 *self.s[\"db\" + str(i)]) +((1-c.BETA1) *(np.square(np.array(self.bias_gradients[i]))) )) weight_col -= (eta *(np.array(self.gradients[i]) /(np.sqrt(self.s[\"dW\"+str(i)]+c.EPSILON))) ) self.bias[i] -= (eta *(np.array(self.bias_gradients[i]) /(np.sqrt(self.s[\"db\"+str(i)]+c.EPSILON))) ) if c.OPTIMIZATION == \"ADAM\": # decaying averages of past gradients self.v[\"dW\" + str(i)] = (( c.BETA1 * self.v[\"dW\" + str(i)]) + ((1 - c.BETA1) * np.array(self.gradients[i]) )) self.v[\"db\" + str(i)] = (( c.BETA1 * self.v[\"db\" + str(i)]) + ((1 - c.BETA1) * np.array(self.bias_gradients[i]) )) # decaying averages of past squared gradients self.s[\"dW\" + str(i)] = ((c.BETA2 * self.s[\"dW\"+str(i)]) + ((1 - c.BETA2) * (np.square( np.array( self.gradients[i]))) )) self.s[\"db\" + str(i)] = ((c.BETA2 * self.s[\"db\" + str(i)]) + ((1 - c.BETA2) * (np.square( np.array( self.bias_gradients[i]))) )) if c.ADAM_BIAS_Correction: # bias-corrected first and second moment estimates self.v[\"dW\" + str(i)] = self.v[\"dW\" + str(i)] / (1 - (c.BETA1 ** true_epoch)) self.v[\"db\" + str(i)] = self.v[\"db\" + str(i)] / (1 - (c.BETA1 ** true_epoch)) self.s[\"dW\" + str(i)] = self.s[\"dW\" + str(i)] / (1 - (c.BETA2 ** true_epoch)) self.s[\"db\" + str(i)] = self.s[\"db\" + str(i)] / (1 - (c.BETA2 ** true_epoch)) # apply to weights and biases weight_col -= ((eta * (self.v[\"dW\" + str(i)] / (np.sqrt(self.s[\"dW\" + str(i)]) + c.EPSILON)))) self.bias[i] -= ((eta * (self.v[\"db\" + str(i)] / (np.sqrt(self.s[\"db\" + str(i)]) + c.EPSILON)))) self.gradient_zeros()"
    } ,
  
    {
      "title"       : "Machine Learning Library in Python from scratch",
      "category"    : "",
      "tags"        : "machine learning, coding, neural networks, python",
      "url"         : "./ML-Library-from-scratch.html",
      "date"        : "2021-03-01 03:32:20 +0900",
      "description" : "Single neuron perceptron that classifies elements learning quite quickly.",
      "content"     : "It must sound crazy that in this day and age, when we have such a myriad of amazing machine learning libraries and toolkits all open sourced, all quite well documented and easy to use, I decided to create my own ML library from scratch.Let me try to explain; I am in the process of immersing myself into the world of Machine Learning, and to do so, I want to deeply understand the basic concepts and its foundations, and I think that there is no better way to do so than by creating myself all the code for a basic neural network library from scratch. This way I can gain in depth understanding of the math that underpins the ML algorithms.Another benefit of doing this is that since I am also learning Python, the experiment brings along good exercise for me.To call it a Machine Learning Library is perhaps a bit of a stretch, since I just intended to create a multi-neuron, multi-layered perceptron.The library started very narrowly, with just the following functionality: create a neural network based on the following parameters: number of inputs size and number of hidden layers number of outputs learning rate forward propagate or predict the output values when given some inputs learn through back propagation using gradient descentI restricted the model to be sequential, and the layers to be only dense / fully connected, this means that every neuron is connected to every neuron of the following layer. Also, as a restriction, the only activation function I implemented was sigmoid:With my neural network coded, I tested it with a very basic problem, the famous XOR problem.XOR is a logical operation that cannot be solved by a single perceptron because of its linearity restriction:As you can see, when plotted in an X,Y plane, the logical operators AND and OR have a line that can clearly separate the points that are false from the ones that are true, hence a perceptron can easily learn to classify them; however, for XOR there is no single straight line that can do so, therefore a multilayer perceptron is needed for the task.For the test I created a neural network with my library:import Neural_Network as nninputs = 3hidden_layers = [2, 1]outputs = 1learning_rate = 0.03NN = nn.NeuralNetwork(inputs, hidden_layers, outputs, learning_rate)The three inputs I decided to use (after a lot of trial and error) are the X and Y coordinate of a point (between X = 0, X = 1, Y = 0 and Y = 1) and as the third input the multiplication of both X and Y. Apparently it gives the network more information, and it ends up converging much more quickly with this third input.Then there is a single hidden layer with 2 neurons and one output value, that will represent False if the value is closer to 0 or True if the value is closer to 1.Then I created the learning data, which is quite trivial for this problem, since we know very easily how to compute XOR.training_data = []for n in range(learning_rounds): x = rnd.random() y = rnd.random() training_data.append([x, y, x * y, 0 if (x &lt; 0.5 and y &lt; 0.5) or (x &gt;= 0.5 and y &gt;= 0.5) else 1])And off we go into training:for data in training_data: NN.train(data[:3].reshape(inputs), data[3:].reshape(outputs))The ML library can only train on batches of 1 (another self-imposed coding restriction), therefore only one “observation” at a time, this is why the train function accepts two parameters, one is the inputs packed in an array, and the other one is the outputs, packed as well in an array.To see the neural net in action I decided to plot the predicted results in both a 3d X,Y,Z surface plot (z being the network’s predicted value), and a scatter plot with the color of the points representing the predicted value.This was plotted in MatPlotLib, so we needed to do some housekeeping first:fig = plt.figure()fig.canvas.set_window_title('Learning XOR Algorithm')fig.set_size_inches(11, 6)axs1 = fig.add_subplot(1, 2, 1, projection='3d')axs2 = fig.add_subplot(1, 2, 2)Then we need to prepare the data to be plotted by generating X and Y values distributed between 0 and 1, and having the network calculate the Z value:x = np.linspace(0, 1, num_surface_points)y = np.linspace(0, 1, num_surface_points)x, y = np.meshgrid(x, y)z = np.array(NN.forward_propagation([x, y, x * y])).reshape(num_surface_points, num_surface_points)As you can see, the z values array is reshaped as a 2d array of shape (x,y), since this is the way Matplotlib interprets it as a surface:axs1.plot_surface(x, y, z, rstride=1, cstride=1, cmap='viridis', vmin=0, vmax=1, antialiased=True)The end result looks something like this:Then we reshape the z array as a one dimensional array to use it to color the scatter plot:z = z.reshape(num_surface_points ** 2)scatter = axs2.scatter(x, y, marker='o', s=40, c=z.astype(float), cmap='viridis', vmin=0, vmax=1)To actually see the progress while learning, I created a Matplotlib animation, and it is quite interesting to see as it learns. So my baby ML library is completed for now, but still I would like to enhance it in several ways: include multiple activation functions (ReLu, linear, Tanh, etc.) allow for multiple optimizers (Adam, RMSProp, SGD Momentum, etc.) have batch and epoch training schedules functionality save and load trained model to fileI will get to it soon…"
    } ,
  
    {
      "title"       : "Conway&#39;s Game of Life",
      "category"    : "",
      "tags"        : "coding, python",
      "url"         : "./conways-game-of-life.html",
      "date"        : "2021-02-11 04:32:20 +0900",
      "description" : "Taking on the challenge of picking up coding again through interesting small projects, this time it is the turn of Conway's Game of Life.",
      "content"     : "I&nbsp;am lately trying to take on coding again. It had always been a part of my life since my early years when I&nbsp;learned to program a Tandy Color Computer at the age of 8, the good old days.Tandy Color Computer TRS80 IIIHaving already programed in Java, C# and of course BASIC, I&nbsp;thought it would be a great idea to learn Python since I&nbsp;have great interest in data science and machine learning, and those two topics seem to have an avid community within Python coders.For one of my starter quick programming tasks, I&nbsp;decided to code Conway's Game of Life, a very simple cellular automata that basically plays itself.The game consists of a grid of n size, and within each block of the grid a cell could either be dead or alive according to these rules:If a cell has less than 2 neighbors, meaning contiguous alive cells, the cell will die of lonelinessIf a cell has more than 3 neighbors, it will die of overpopulationIf an empty block has exactly 3 contiguous alive neighbors, a new cell will be born in that spotIf an alive cell has 2 or 3 alive neighbors, it continues to liveConway’s rules for the Game of LifeTo make it more of a challenge I&nbsp;also decided to implement an \"sparse\" method of recording the game board, this means that instead of the typical 2d array representing the whole board, I&nbsp;will only record the cells which are alive. Saving a lot of memory space and processing time, while adding some spice to the challenge.The trickiest part was figuring out how to calculate which empty blocks had exactly 3 alive neighbors so that a new cell will spring to life there, this is trivial in the case of recording the whole grid, because we just iterate all over the board and find the alive neighbors of ALL&nbsp;the blocks in the grid, but in the case of only keeping the alive cells proved quite a challenge.In the end the algorithm ended up as follows:Iterate through all the alive cells and get all of their neighborsdef get_neighbors(self, cell): neighbors = [] for x in range(-1, 2, 1): for y in range(-1, 2, 1): if not (x == 0 and y == 0): if (0 &amp;lt;= (cell[0] + x) &amp;lt;= self.size_x) and (0 &amp;lt;= (cell[1] + y) &amp;lt;= self.size_y): neighbors.append((cell[0] + x, cell[1] + y)) return neighborsMark all the neighboring blocks as having +1 neighbor each time a particular cell is encountered. This way, for each neighboring alive cell the counter of the particular block will increase, and in the end it will contain the total number of live cells which are contiguous to it.def next_state(self): alive_neighbors = {} for cell in self.alive_cells: if cell not in alive_neighbors: alive_neighbors[cell] = 0 neighbors = self.get_neighbors(cell) for neighbor in neighbors: if neighbor not in alive_neighbors: alive_neighbors[neighbor] = 1 else: alive_neighbors[neighbor] += 1The trick was using a dictionary to keep the record of the blocks that have alive neighbors and the cells who are alive in the current state but have zero alive neighbors (thus will die).With the dictionary it became easy just to add cells and increase their neighbor counter each time it was encountered as a neighbor of an alive cell.Having the dictionary now filled with all the cells that have alive neighbors and how many they have, it was just a matter of applying the rules of the game:for cell in alive_neighbors: if alive_neighbors[cell] &amp;lt; 2 or alive_neighbors[cell] &gt; 3: self.alive_cells.discard(cell) elif alive_neighbors[cell] == 3: self.alive_cells.add(cell)Notice that since I am keeping an array of the coordinates of only the cells who are alive, I could apply just 3 rules, die of loneliness, die of overpopulation and become alive from reproduction (exactly 3 alive neighbors) because the ones who have 2 or 3 neighbors and are already alive, can remain alive in the next iteration.I&nbsp;found it very interesting to implement the Game of Life like this, it was quite a refreshing challenge and I am beginning to feel my coding skills ramping up again."
    } ,
  
    {
      "title"       : "Single Neuron Perceptron",
      "category"    : "",
      "tags"        : "machine learning, coding, neural networks",
      "url"         : "./single-neuron-perceptron.html",
      "date"        : "2021-01-26 04:32:20 +0900",
      "description" : "Single neuron perceptron that classifies elements learning quite quickly.",
      "content"     : "As an entry point to learning python and getting into Machine Learning, I decided to code from scratch the Hello World! of the field, a single neuron perceptron.What is a perceptron?A perceptron is the basic building block of a neural network, it can be compared to a neuron, And its conception is what detonated the vast field of Artificial Intelligence nowadays.Back in the late 1950’s, a young Frank Rosenblatt devised a very simple algorithm as a foundation to construct a machine that could learn to perform different tasks.In its essence, a perceptron is nothing more than a collection of values and rules for passing information through them, but in its simplicity lies its power.Imagine you have a ‘neuron’ and to ‘activate’ it, you pass through several input signals, each signal connects to the neuron through a synapse, once the signal is aggregated in the perceptron, it is then passed on to one or as many outputs as defined. A perceptron is but a neuron and its collection of synapses to get a signal into it and to modify a signal to pass on.In more mathematical terms, a perceptron is an array of values (let’s call them weights), and the rules to apply such values to an input signal.For instance a perceptron could get 3 different inputs as in the image, lets pretend that the inputs it receives as signal are: $x_1 = 1, \\; x_2 = 2\\; and \\; x_3 = 3$, if it’s weights are $w_1 = 0.5,\\; w_2 = 1\\; and \\; w_3 = -1$ respectively, then what the perceptron will do when the signal is received is to multiply each input value by its corresponding weight, then add them up.\\(\\begin{align}\\begin{split}\\left(x_1 * w_1\\right) + \\left(x_2 * w_2\\right) + \\left(x_3 * w_3\\right)\\end{split}\\end{align}\\)\\(\\begin{align}\\begin{split}\\left(0.5 * 1\\right) + \\left(1 * 2\\right) + \\left(-1 * 3\\right) = 0.5 + 2 - 3 = -0.5\\end{split}\\end{align}\\)Typically when this value is obtained, we need to apply an “activation” function to smooth the output, but let’s say that our activation function is linear, meaning that we keep the value as it is, then that’s it, that is the output of the perceptron, -0.5.In a practical application, the output means something, perhaps we want our perceptron to classify a set of data and if the perceptron outputs a negative number, then we know the data is of type A, and if it is a positive number then it is of type B.Once we understand this, the magic starts to happen through a process called backpropagation, where we “educate” our tiny one neuron brain to have it learn how to do its job.The magic starts to happen through a process called backpropagation, where we \"educate\" our tiny one neuron brain to have it learn how to do its job.For this we need a set of data that it is already classified, we call this a training set. This data has inputs and their corresponding correct output. So we can tell the little brain when it misses in its prediction, and by doing so, we also adjust the weights a bit in the direction where we know the perceptron committed the mistake hoping that after many iterations like this the weights will be so that most of the predictions will be correct.After the model trains successfully we can have it classify data it has never seen before, and we have a fairly high confidence that it will do so correctly.The math behind this magical property of the perceptron is called gradient descent, and is just a bit of differential calculus that helps us convert the error the brain is having into tiny nudges of value of the weights towards their optimum. This video series by 3 blue 1 brown explains it wonderfuly.My program creates a single neuron neural network tuned to guess if a point is above or below a randomly generated line and generates a visualization based on graphs to see how the neural network is learning through time.The neuron has 3 inputs and weights to calculate its output:input 1 is the X coordinate of the point,Input 2 is the y coordinate of the point,Input 3 is the bias and it is always 1Input 3 or the bias is required for lines that do not cross the origin (0,0)The Perceptron starts with weights all set to zero and learns by using 1,000 random points per each iteration.The output of the perceptron is calculated with the following activation function: if x * weight_x + y weight_y + weight_bias is positive then 1 else 0The error for each point is calculated as the expected outcome of the perceptron minus the real outcome therefore there are only 3 possible error values: Expected Calculated Error 1 -1 1 1 1 0 -1 -1 0 -1 1 -1 With every point that is learned if the error is not 0 the weights are adjusted according to:New_weight = Old_weight + error * input * learning_ratefor example: New_weight_x = Old_weight_x + error * x * learning rateA very useful parameter in all of neural networks is teh learning rate, which is basically a measure on how tiny our nudge to the weights is going to be.In this particular case, I coded the learning_rate to decrease with every iteration as follows:learning_rate = 0.01 / (iteration + 1)this is important to ensure that once the weights are nearing the optimal values the adjustment in each iteration is subsequently more subtle.In the end, the perceptron always converges into a solution and finds with great precision the line we are looking for.Perceptrons are quite a revelation in that they can resolve equations by learning, however they are very limited. By their nature they can only resolve linear equations, so their problem space is quite narrow.Nowadays the neural networks consist of combinations of many perceptrons, in many layers, and other types of “neurons”, like convolution, recurrent, etc. increasing significantly the types of problems they solve."
    } 
  
]
