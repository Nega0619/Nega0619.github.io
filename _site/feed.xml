<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-03-21T12:21:25+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Hwi’s footsteps</title><subtitle>Artificial Intelligence and Computer Science Study Notes.</subtitle><author><name>Shin Hwi Jeong</name></author><entry><title type="html">Gd 03. 텍스트의 분포로 벡터화 하기</title><link href="http://localhost:4000/GD-03.-%ED%85%8D%EC%8A%A4%ED%8A%B8%EC%9D%98-%EB%B6%84%ED%8F%AC%EB%A1%9C-%EB%B2%A1%ED%84%B0%ED%99%94-%ED%95%98%EA%B8%B0.html" rel="alternate" type="text/html" title="Gd 03. 텍스트의 분포로 벡터화 하기" /><published>2022-03-21T00:00:00+09:00</published><updated>2022-03-21T00:00:00+09:00</updated><id>http://localhost:4000/GD%2003.%20%ED%85%8D%EC%8A%A4%ED%8A%B8%EC%9D%98%20%EB%B6%84%ED%8F%AC%EB%A1%9C%20%EB%B2%A1%ED%84%B0%ED%99%94%20%ED%95%98%EA%B8%B0</id><content type="html" xml:base="http://localhost:4000/GD-03.-%ED%85%8D%EC%8A%A4%ED%8A%B8%EC%9D%98-%EB%B6%84%ED%8F%AC%EB%A1%9C-%EB%B2%A1%ED%84%B0%ED%99%94-%ED%95%98%EA%B8%B0.html"><![CDATA[<p>오늘은 아이펠 Going Deeper 노드 2번을 공부하고 공부한 내용 정리해보았습니다.</p>

<hr />

<h6 id="출처">출처</h6>

<ul>
  <li>AIFFEL LMS</li>
  <li>https://wikidocs.net/22592</li>
</ul>

<hr />

<h6 id="학습데이터">학습데이터</h6>

<blockquote>
  <p>$ wget https://github.com/jungyeul/korean-parallel-corpora/raw/master/korean-english-news-v1/korean-english-park.train.tar.gz</p>

  <p>$ mkdir -p ~/aiffel/sp_tokenizer/data</p>

  <p>$ mv korean-english-park.train.tar.gz ~/aiffel/sp_tokenizer/data</p>

  <p>$ cd ~/aiffel/sp_tokenizer/data</p>

  <p>$ tar -xzvf korean-english-park.train.tar.gz</p>
</blockquote>

<hr />

<p>오늘 배운 내용은 <strong>문장을 조각 내는 방법</strong>입니다.</p>

<ul>
  <li>중복 제거한 코드 분석
    <ul>
      <li>주요 요점!
        <ol>
          <li>min = 999, max = 0으로 표시되어있다.</li>
          <li>set(raw_data)를 함으로써 중복은 없어진 것이다!</li>
        </ol>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name>Shin Hwi Jeong</name></author><summary type="html"><![CDATA[오늘은 아이펠 Going Deeper 노드 2번을 공부하고 공부한 내용 정리해보았습니다.]]></summary></entry><entry><title type="html">Gd 02. 멋진 단어사전 만들기</title><link href="http://localhost:4000/GD-02.-%EB%A9%8B%EC%A7%84-%EB%8B%A8%EC%96%B4%EC%82%AC%EC%A0%84-%EB%A7%8C%EB%93%A4%EA%B8%B0.html" rel="alternate" type="text/html" title="Gd 02. 멋진 단어사전 만들기" /><published>2022-03-17T00:00:00+09:00</published><updated>2022-03-17T00:00:00+09:00</updated><id>http://localhost:4000/GD%2002.%20%EB%A9%8B%EC%A7%84%20%EB%8B%A8%EC%96%B4%EC%82%AC%EC%A0%84%20%EB%A7%8C%EB%93%A4%EA%B8%B0</id><content type="html" xml:base="http://localhost:4000/GD-02.-%EB%A9%8B%EC%A7%84-%EB%8B%A8%EC%96%B4%EC%82%AC%EC%A0%84-%EB%A7%8C%EB%93%A4%EA%B8%B0.html"><![CDATA[<p>오늘은 아이펠 Going Deeper 노드 2번을 공부하고 공부한 내용 정리해보았습니다.</p>

<hr />

<h6 id="출처">출처</h6>

<ul>
  <li>AIFFEL LMS</li>
  <li>https://wikidocs.net/22592</li>
</ul>

<hr />

<h6 id="학습데이터">학습데이터</h6>

<blockquote>
  <p>$ wget https://github.com/jungyeul/korean-parallel-corpora/raw/master/korean-english-news-v1/korean-english-park.train.tar.gz</p>

  <p>$ mkdir -p ~/aiffel/sp_tokenizer/data</p>

  <p>$ mv korean-english-park.train.tar.gz ~/aiffel/sp_tokenizer/data</p>

  <p>$ cd ~/aiffel/sp_tokenizer/data</p>

  <p>$ tar -xzvf korean-english-park.train.tar.gz</p>
</blockquote>

<hr />

<p>오늘 배운 내용은 <strong>문장을 조각 내는 방법</strong>입니다.</p>

<ul>
  <li>중복 제거한 코드 분석
    <ul>
      <li>주요 요점!
        <ol>
          <li>min = 999, max = 0으로 표시되어있다.</li>
          <li>set(raw_data)를 함으로써 중복은 없어진 것이다!</li>
        </ol>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name>Shin Hwi Jeong</name></author><summary type="html"><![CDATA[오늘은 아이펠 Going Deeper 노드 2번을 공부하고 공부한 내용 정리해보았습니다.]]></summary></entry><entry><title type="html">Gd 01. 텍스트 데이터 다루기</title><link href="http://localhost:4000/GD-01.-%ED%85%8D%EC%8A%A4%ED%8A%B8-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%8B%A4%EB%A3%A8%EA%B8%B0.html" rel="alternate" type="text/html" title="Gd 01. 텍스트 데이터 다루기" /><published>2022-03-16T00:00:00+09:00</published><updated>2022-03-16T00:00:00+09:00</updated><id>http://localhost:4000/GD%2001.%20%ED%85%8D%EC%8A%A4%ED%8A%B8%20%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EB%8B%A4%EB%A3%A8%EA%B8%B0</id><content type="html" xml:base="http://localhost:4000/GD-01.-%ED%85%8D%EC%8A%A4%ED%8A%B8-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%8B%A4%EB%A3%A8%EA%B8%B0.html"><![CDATA[<p>오늘은 아이펠 Going Deeper 노드 1번을 공부하고 공부한 내용을 포스팅해보았습니다.</p>

<hr />

<h6 id="출처">출처</h6>

<ul>
  <li>AIFFEL LMS</li>
  <li><a href="https://wikidocs.net/22592">딥러닝을 이용한 자연어 처리 입문</a></li>
  <li><a href="https://brunch.co.kr/@learning/7">한국어를 위한 어휘 임베딩의 개발</a></li>
</ul>

<hr />

<h1 id="1-자연언어와-인공언어">1. 자연언어와 인공언어</h1>

<p>자연언어란, 몇만년간 인류가 나름대로 발전시키고 통신 수단으로 사용하는 언어. 현재 약 200가지가 넘는 언어가 존재하며 이 중 40가지 정도가 글을 가지고 있다.</p>

<p><strong>문맥 의존 언어 이다.</strong></p>

<p>인공언어란, 컴퓨터 프로그래밍을 위하여 특별히 개발된 포트란, 파스칼 C와 같은 언어</p>

<p><strong>문맥자유 언어이다.</strong></p>

<p>자연어를 처리하는 것이 어려운 이유는 문맥 자유 언어가 아니기 때문입니다. 컴퓨터가 언어를 해석할 때, 문맥 자유 언어는 문법에 맞게 작성했다면, 글의 흐름을 파악할 필요가 없습니다. 문법에 맞게 파싱하기만 하면 되기 때문에 프로그래머가 작성한 대로 프로그램이 동작하게 됩니다.</p>

<h1 id="2-자연언어에서-발견할-수-있는-노이즈들">2. 자연언어에서 발견할 수 있는 노이즈들</h1>

<ul>
  <li>
    <p>불완전한 문장으로 구성된 대화</p>

    <p>카톡과 같은 메신저를 사용할 때, 한 문장을 한번에 보내지 않고 여러번에 나눠 전송하는 형태입니다.</p>
  </li>
  <li>
    <p>문장의 길이가 너무 길거나 짧은 경우</p>

    <p>대체로 <code class="language-plaintext highlighter-rouge">ㅋㅋㅋ</code> 혹은 <code class="language-plaintext highlighter-rouge">ㅎㅎ</code>, <code class="language-plaintext highlighter-rouge">ㅠㅠ</code>, <code class="language-plaintext highlighter-rouge">헐ㅋ</code>와 같은 리액션에 해당하는 경우가 많음.</p>
  </li>
  <li>
    <p>채팅 데이터에서 문장 시간 간격이 너무 긴 경우</p>

    <ul>
      <li>
        <p>서로의 말이 얽히며, 서로 각자의 할말만 하는 경우가 있음</p>

        <blockquote>
          <p>A: 베트맨2</p>

          <p>B: 보러가자</p>

          <p>A: 어제 봤는데 꿀잼</p>

          <p>B: 오늘 저녀.. 아니 ㅠ</p>
        </blockquote>
      </li>
      <li>
        <p>말의 텀이 매우 길 때</p>

        <blockquote>
          <p>A: 혹시 돈좀 빌려 줄 수 있어?</p>

          <p>…. (한참 뒤)</p>

          <p>B: 아 미안 지금 봄. 지금도 필요해?</p>
        </blockquote>
      </li>
    </ul>
  </li>
  <li>
    <p>바람직하지 않은 문장의 사용</p>

    <p>욕설이나 비속어, 오타 비율이 높은 경우</p>
  </li>
</ul>

<p>위와 같은 언어는 UNFORMAL한 언어 데이터에서 자주 보이므로, FORMAL한 데이터에서도 자주 보이는 더 단순하고 근본적인 노이즈에 대해서도 알아보았습니다.</p>

<ul>
  <li>
    <p>문장부호</p>

    <p><code class="language-plaintext highlighter-rouge">Hi</code> 와 <code class="language-plaintext highlighter-rouge">Hi,</code>를 컴퓨터는 다르게 이해할 수 있습니다. 이같은 경우 문장부호 앞 뒤에 공백을 넣어 분리해줍니다.</p>

    <p>하지만, www.naver.com 이나 101,456 과 같은 데이터의 경우 공백을 넣으면 오히려 이상하게 처리를 해주는 꼴입니다.</p>

    <p>위 같은 경우는 불가피한 손실로 취급하고 넘어가게 됩니다. 모든 규칙에 대해서 변환을 정의해줄 수는 없기 때문입니다. 게다가 보통 수행하고자 하는 목적에 큰 영향을 주지 못하기 때문에 오류가 있음에도 그냥 넘어가게 됩니다.</p>
  </li>
  <li>
    <p>대소문자</p>

    <p>First와 first를 컴퓨터는 다르게 인식할 수 있으므로 lower()혹은 upper()메소드를 이용해 변경해줍니다.</p>
  </li>
  <li>
    <p>특수문자</p>

    <p>영어에서는 나이를 ten-year-old와 같이 표현하는 경우가 있습니다. 이 같은 경우 특정 특수문자를 공백으로 변경하거나 없애는 처리를 추가해주며 보통 정규표현식으 사용하여 처리합니다.</p>
  </li>
</ul>

<h1 id="3-단어의-희소-표현과-분산-표현">3. 단어의 희소 표현과 분산 표현</h1>

<h2 id="희소-표현">희소 표현</h2>

<p>해당 단어의 속성이 어느 그룹에 속하느냐를 벡터로 표현한 것.</p>

<p>원-핫 인코딩을 통해서 얻은 원-핫 벡터는 표현하고자 하는 단어의 인덱스의 값만 1이고, 나머지 인덱스에는 전부 0으로 표현되는 벡터 표현 방법이었습니다. 이와 같이 벡터 또는 행렬의 값이 대부분이 0으로 표현되는 방법이 희소표현입니다.</p>

<p>ex ) 0번째 인덱스를 여자 = 1, 남자 -1로 표현한다면</p>

<blockquote>
  <p>남자 = [-1]</p>

  <p>여자 = [1]</p>

  <p>사과 = [0]</p>

  <p>소년 = [-1]</p>

  <p>바나나 = [0]</p>
</blockquote>

<p>이런 속성들이 여러개가 된다면 결국 0000001000000000011001000000과 같은 표현이 될것입니다. 메모리 낭비가 심합니다.</p>

<h2 id="분산표현">분산표현</h2>

<p>컴퓨터가 추상적으로 단어의 유사도를 계산하여 각각에 표현한 것.</p>

<p>분산 표현(distributed representation) 방법은 기본적으로 분포 가설(distributional hypothesis)이라는 가정 하에 만들어진 표현 방법</p>

<p>100개의 단어를 256차원의 속성으로 표현하고 싶다면 Embedding 레이어는 다음과 같이 정의됩니다.</p>

<p><code class="language-plaintext highlighter-rouge">embedding_layer = tf.keras.layers.Embedding(input_dim=100, output_dim=256)</code></p>

<h1 id="4-토큰화">4. 토큰화</h1>

<p>단어에 대해 희소표현과 분산표현을 해주는 것은 단어 사전을 만들어 주기 위함입니다. 하지만 단어사전이 존재한다고 해도 컴퓨터는 엉뚱하게 해석하고 표현합니다. 그 이유는 컴퓨터가 문장을 단어단위로 정확히 끊어내지 못하기 때문입니다.</p>

<p>예를 들어,<code class="language-plaintext highlighter-rouge">나</code>와 <code class="language-plaintext highlighter-rouge">나는</code> 이란 단어가 있으면 <code class="language-plaintext highlighter-rouge">나</code>와 <code class="language-plaintext highlighter-rouge">나</code>,``-는<code class="language-plaintext highlighter-rouge">으로 단어를 끊어 해석해야 하는데  컴퓨터는 </code>나<code class="language-plaintext highlighter-rouge">와 </code>나는`을 전혀 연관성 없는 단어로 해석할 수 있습니다.</p>

<p>그래서 문장을 어떤 기준으로 쪼개주어야 하는데 쪼개진 단어들을 토큰이라고 합니다.</p>

<h2 id="토큰화-기법-1-공백-기반-토큰화">토큰화 기법 1. 공백 기반 토큰화</h2>

<p>공백을 베이스로하여 문장을 나누어줍니다.  python에선 split함수를 많이 씀.</p>

<h2 id="토큰화-기법-2-형태소-기반-토큰화">토큰화 기법 2. 형태소 기반 토큰화</h2>

<p>대표적인 한국어 형태소 분석기 KoLNLPy를 사용해서 토큰화 해주기도 합니다.</p>

<p><a href="https://konlpy-ko.readthedocs.io/ko/v0.4.3/">KoNLPy: 파이썬 한국어 NLP - KoNLPy 0.4.3 documentation</a></p>

<p>다른 분석기를 사용하려면 다음 사이트를 참고하면 좋습니다</p>

<p><a href="https://iostream.tistory.com/144">한국어 형태소 분석기 성능 비교</a></p>

<h2 id="토큰화-기법-3-사전에-없는-단어의-문제">토큰화 기법 3. 사전에 없는 단어의 문제</h2>

<p>토큰화 기법 1, 2에서 설명한 공백 기반, 형태소 기반은 모두 각각의 단어가 가지는 의미 단위로 토큰화 합니다. 이 기법의 경우는 데이터에  포함되는 모든 단어를 처리할 수 는 없기 때문에, 상위 N개의 단어만 사용하고 나머지는 특수한 토큰 <code class="language-plaintext highlighter-rouge">&lt;unk&gt;</code>토큰으로 치환합니다. 사진은 예시입니다.</p>

<p><img src="../assets/img/posts/image-20220316173133220.png" alt="image-20220316173133220" /></p>

<p>이런 문장을 영문으로 번역할 땐 어려움이 있을 수 밖에 없습니다. 이 같은 문제를 해결하고자 하는 시도들이 있었고 그것이 Wordpiece Model입니다.</p>

<h1 id="5-토큰화-다른-방법">5. 토큰화 다른 방법</h1>

<h2 id="5-1-byte-pair-encodingbpe">5-1. Byte Pair Encoding(BPE)</h2>

<p>Subword(접두사/ 접미사 ex) Pre- )의 집합으로 보는 방법이 WPM(WordPiece Model)입니다. 이 모델을 알기전에 알아야 하는 것이  <strong>Byte Pair Encoding(BPE)</strong> 이므로, 이에 대해 간단히 설명하겠습니다.</p>

<p>데이터에서 가장 많이 등장하는 바이트 쌍(Byte Pair)을 새로운 단어로 치환하여 압축하는 작업을 반복하는 방식으로 동작합니다.</p>

<p>예시 )</p>

<p><img src="../assets/img/posts/image-20220316173104200.png" alt="image-20220316173104200" /></p>

<p>이 방식을 사용하면, <code class="language-plaintext highlighter-rouge">lowest</code>라는 처음 등장해도, <code class="language-plaintext highlighter-rouge">low</code>와 <code class="language-plaintext highlighter-rouge">est</code>라는 단어 묶음이 이미 있다면, <code class="language-plaintext highlighter-rouge">lowest</code>를 <code class="language-plaintext highlighter-rouge">low</code>와 <code class="language-plaintext highlighter-rouge">est</code>의 조합으로, 즉 아는 단어로 처리할 수 있습니다.</p>

<p>즉 그렇기 대문에 아무리 큰 데이터도 원하는 크기로 OOV 문제없이 사전을 정의할 수 있습니다.</p>

<blockquote>
  <p>OOV : Out of Vocabulary</p>
</blockquote>

<p>Emdedding 레이어는 단어수 * Embedding 차원의 수 = weight 이기 때문에 단어의 개수 줄어드는 문제 = 메모리 절약</p>

<p>많은 데이터는 정확도로 이어지기 때문에 굉장히 큰 의미가 있습니다.</p>

<h2 id="5-2-wordpiece-modelwpm">5-2. Wordpiece Model(WPM)</h2>

<p>BPE의 단점은 나누는 그룹이 <code class="language-plaintext highlighter-rouge">i / a / o / n / b</code> 단위라면 이 단위들을 합치기는 어렵습니다. 이러한 단점을 극복하려 구글에서 만든 것이 WPM입니다. WPM은 BPE보다 두가지가 다릅니다.</p>

<ol>
  <li>공백 복원을 위하여 단어 시작부분에 <code class="language-plaintext highlighter-rouge">_</code>를 추가합니다.</li>
  <li>빈도수 기반이 아니라 가능도<code class="language-plaintext highlighter-rouge">likelihood</code>를 증가시키는 방향으로 문자쌍을 합칩니다.</li>
</ol>

<p>예를 들어 아까 예시는 <code class="language-plaintext highlighter-rouge">_i, _a, o, _n, b</code> 와 같이 토큰화가 진행됩니다.</p>

<p>이 경우, 모든 토큰을 합친 후, 언더바를 _를 공백으로 치환으로 마무리되어 간편합니다.</p>

<h2 id="5-3-soynlp">5-3. soynlp</h2>

<p>한국어 자연어 처리를 위한 라이브러리로 한국어를 위한 토크나이저로 사용됩니다. 토크나이저 이외에도 단어 추출, 품사 판별, 전처리 기능도 제공합니다.</p>

<p>soynlp는 문장에서 처음 단어를 받아들일 때 단어의 경계를 비지도 학습을 통해 결정합니다. 비지도 학습을 통한 방법이라, 미등록 단어도 토큰화가 가능합니다. 여기서 사용된 비지도 학습은 통계적 방법이라 soynlp를 통계 기반 토크나이저로 분류하기도 합니다. 즉, <code class="language-plaintext highlighter-rouge">젤다의 전설</code>이 한 단어임을 인지하기 위하여 젤, 젤다, 젤다의, 젤다의 전, 젤다의 전설 각각 다음 글자의 확률을 계산하는 방식입니다.</p>

<h1 id="6-토큰에게-의미를-부여하기">6. 토큰에게 의미를 부여하기</h1>

<p>토큰화된 단어조각들끼리 유사도 연산을 연산을 할 수 있게 의미를 부여하는 알고리즘을 소개합니다.</p>

<h2 id="6-1-word2vec">6-1. Word2Vec</h2>

<p><code class="language-plaintext highlighter-rouge">단어로 벡터를 만든다</code>는 뜻.</p>

<p>단어를 희소표현이 아닌 분산표현으로 나타낸 방법. 저차원에 단어의 이미를 여러 차원에다가 분산하여 표현합니다. 이러한 표현 방법을 사용하면, 단어 벡터간 유의미한 유사도를 계산할 수있는데 이를 위한 대표적인 학습 방법이 Word2Vec입니다.</p>

<p>word2Vec에는 두가지 학습 방식이 있습니다.</p>

<ul>
  <li>
    <p>CBOW Continuous Bag of words</p>

    <p>주변에 있는 단어들을 입력으로 중간에 있는 단어들을 예측하는 방법입니다.</p>
  </li>
  <li>
    <p>Skip gram</p>

    <p>중간에 있는 단어들을 입력으로 주변 단어들을 예측하는 방법입니다.</p>
  </li>
</ul>

<h3 id="cbow">CBOW</h3>

<p>주변에 있는 단어들을 입력으로 중간에 있는 단어들을 예측하는 방법입니다.</p>

<p>예문 : “The fat cat sat on the mat” 이 있다고 하면, [‘The’, ‘fat’, ‘cat’, ‘on’, ‘the’, ‘mat’]를 입력으로 받고, ‘sat’이라는 중간 단어를 예측하는 것입니다.</p>

<p>여기서 리스트에 존재하는 단어를 주변단어 context word라고 하고, sat을 중심단어 center word라고 합니다.</p>

<p><img src="https://wikidocs.net/images/page/22660/%EB%8B%A8%EC%96%B4.PNG" alt="img" /></p>

<p>중심 단어를 예측하기 위해 앞, 뒤로 몇 개의 단어를 볼 지 결정하는 것 = window</p>

<p>위 예시는 window = 2일 경우, 중심단어에 따라 앞 뒤로 주변단어를 받는 예시입니다.</p>

<p>슬라이딩 윈도우 sliding window = 윈도우 크기가 결정됐을 경우, 윈도우를 옆으로 움직여서 주변 단어와 중심단어의 선택을 변경해가며 학습을 위한 데이터셋을 만드는 방법</p>

<p><img src="https://wikidocs.net/images/page/22660/word2vec_renew_1.PNG" alt="img" /></p>

<p>CBOW의 인공 신경망을 간단히 도식화한 그림입니다.</p>

<p>위 그림에서 알수있는것.</p>

<ol>
  <li>Word2Vec은 은닉층이 다수인 딥러닝 모델이 아니라, 은닉층이 1개인 얕은 신경망 shallow neural network라는 점.</li>
  <li>일반적인 은닉층과 달리 활성화 함수 존재 X</li>
  <li>Word2Vec의 은닉층은 룩업 테이블이라는 연산을 담당하는 층으로 투사층 projection layer이라 부르기도 합니다.</li>
</ol>

<h3 id="skip-gram">Skip gram</h3>

<p>중간에 있는 단어들을 입력으로 주변 단어들을 예측하는 방법입니다.</p>

<p>윈도우 크기가 2일때 구성되는 데이터 셋 :</p>

<p><img src="https://wikidocs.net/images/page/22660/skipgram_dataset.PNG" alt="img" /></p>

<p>인공 신경망을 도식화해보면 아래와 같습니다.</p>

<p><img src="https://wikidocs.net/images/page/22660/word2vec_renew_6.PNG" alt="img" /></p>

<p>중심 단어에서 주변 단어를 예측하므로, 투사층에서 벡터들의 평균을 구하는 과정은 없습니다.</p>

<p>일반적으로 Skip-gram이 CBOW보다 성능이 좋다고 알려져있습니다.</p>

<h2 id="6-2-fasttext">6-2. FastText</h2>

<h2 id="elmo---the-1st-contextualized-word-embedding">ELMo - the 1st Contextualized Word Embedding</h2>]]></content><author><name>Shin Hwi Jeong</name></author><summary type="html"><![CDATA[오늘은 아이펠 Going Deeper 노드 1번을 공부하고 공부한 내용을 포스팅해보았습니다.]]></summary></entry><entry><title type="html">운영체제 ) ch3. processes 실습</title><link href="http://localhost:4000/%EC%9A%B4%EC%98%81%EC%B2%B4%EC%A0%9C-)-ch3.-Processes-%EC%8B%A4%EC%8A%B5.html" rel="alternate" type="text/html" title="운영체제 ) ch3. processes 실습" /><published>2022-03-16T00:00:00+09:00</published><updated>2022-03-16T00:00:00+09:00</updated><id>http://localhost:4000/%EC%9A%B4%EC%98%81%EC%B2%B4%EC%A0%9C%20)%20ch3.%20Processes%20%EC%8B%A4%EC%8A%B5</id><content type="html" xml:base="http://localhost:4000/%EC%9A%B4%EC%98%81%EC%B2%B4%EC%A0%9C-)-ch3.-Processes-%EC%8B%A4%EC%8A%B5.html"><![CDATA[<p>실습 1.</p>

<p><img src="../assets/img/posts/image-20220316091214611.png" alt="image-20220316091214611" /></p>

<p><img src="../assets/img/posts/image-20220316091455080.png" alt="image-20220316091455080" /></p>

<p>gcc -&gt; a.out 으로 실행파일만듬.</p>

<p>프로세서는 어디에 올라가는가? 모든 램에 올라가나?</p>

<p>큰 파일이면,, 그 전체가?</p>

<p>프로세스 실행상태</p>

<p>PCB란?</p>

<p>PCB는 어디에샐겨?</p>

<p>프로그램 카운터란?</p>

<p>프로세스 당 PCB가 하나인건가?</p>

<p>at the same time, simultaneously,concurrently&lt;-&gt; parallel</p>

<p>wait queue -&gt; ready Queue로감</p>

<p><img src="../assets/img/posts/image-20220316095719889.png" alt="image-20220316095719889" /></p>]]></content><author><name>Shin Hwi Jeong</name></author><summary type="html"><![CDATA[실습 1.]]></summary></entry><entry><title type="html">[ex_16] 다음에 볼 영화 예측하기</title><link href="http://localhost:4000/ex_16-%EB%8B%A4%EC%9D%8C%EC%97%90-%EB%B3%BC-%EC%98%81%ED%99%94-%EC%98%88%EC%B8%A1%ED%95%98%EA%B8%B0.html" rel="alternate" type="text/html" title="[ex_16] 다음에 볼 영화 예측하기" /><published>2022-03-07T00:00:00+09:00</published><updated>2022-03-07T00:00:00+09:00</updated><id>http://localhost:4000/%5Bex_16%5D%20%EB%8B%A4%EC%9D%8C%EC%97%90%20%EB%B3%BC%20%EC%98%81%ED%99%94%20%EC%98%88%EC%B8%A1%ED%95%98%EA%B8%B0</id><content type="html" xml:base="http://localhost:4000/ex_16-%EB%8B%A4%EC%9D%8C%EC%97%90-%EB%B3%BC-%EC%98%81%ED%99%94-%EC%98%88%EC%B8%A1%ED%95%98%EA%B8%B0.html"><![CDATA[<p>오늘은 아이펠 Exploration 노드 14번을 공부하고 공부한 내용을 포스팅해보았습니다.</p>

<hr />

<h6 id="출처">출처</h6>

<ul>
  <li>
    <p>AIFFEL LMS</p>

    <p>문제시 연락 부탁드립니다. :)</p>
  </li>
</ul>

<hr />]]></content><author><name>Shin Hwi Jeong</name></author><summary type="html"><![CDATA[오늘은 아이펠 Exploration 노드 14번을 공부하고 공부한 내용을 포스팅해보았습니다.]]></summary></entry><entry><title type="html">Session Parallel mini Batches</title><link href="http://localhost:4000/Session-Parallel-Mini-Batches.html" rel="alternate" type="text/html" title="Session Parallel mini Batches" /><published>2022-03-07T00:00:00+09:00</published><updated>2022-03-07T00:00:00+09:00</updated><id>http://localhost:4000/Session-Parallel%20Mini-Batches</id><content type="html" xml:base="http://localhost:4000/Session-Parallel-Mini-Batches.html"><![CDATA[<p>오늘의 포스팅 주제는 Session-Parallel Mini-Batches입니다.</p>

<p>아이펠 Exploration 노드 16번 추천시스템을 공부하다 알게 된 이론인데, 이해가 잘 되지 않아 더 조사하게 되었습니다.</p>

<hr />

<h6 id="관련논문">관련논문</h6>

<ul>
  <li><a href="https://arxiv.org/pdf/1511.06939.pdf">SESSION-BASED RECOMMENDATIONS WITH RECURRENT NEURAL NETWORKS</a></li>
  <li><a href="https://iopscience.iop.org/article/10.1088/1742-6596/1314/1/012194/pdf">IMPROVED SESSION-BASED RECOMMENDATIONS USING RECURRENT NEURAL NETWORKS FOR MUSIC DISCOVERY</a></li>
</ul>

<hr />

<h1 id="session-parallel-mini-batches란">Session-Parallel Mini-Batches란?</h1>

<p><strong>Session-Based Recommendatation</strong>에서 보통 쓰입니다.</p>

<h2 id="session-based-recommendation이란">Session-Based Recommendation이란?</h2>

<p>이 추천 시스템은 사용자의 개인정보나 아이템들의 정보를 분석하여 추천하는 것이 아니라, 세션 데이터를 기반으로 사용자가 다음에 클릭 혹은 구매할 아이템을 예측하는 추천입니다.</p>

<ul>
  <li>Session이란?
    <ul>
      <li>서버쪽에 저장되는 데이터로, 서비스를 이용하며 발생하는 중요한 정보들을 담은 데이터</li>
      <li>같은 SessionID를 존재는 같은 유저라고 인지하며 보통 다음과 같은 정보를 가지고 있습니다.</li>
      <li><img src="../assets/img/posts/image-20220307115736517.png" alt="image-20220307115736517" /> 사진 출처 : Aiffel LMS</li>
    </ul>
  </li>
</ul>

<h2 id="session-parallel-mini-batches의-동작">Session-Parallel Mini-Batches의 동작</h2>

<p>다음과 같은 session 데이터가 있을때,</p>

<p><img src="../assets/img/posts/image-20220307145442657.png" alt="image-20220307145442657" /></p>

<p>Session parallel Mini batches는 다음과 같이 처리된다.</p>

<p><img src="../assets/img/posts/image-20220307145529789.png" alt="image-20220307145529789" /></p>

<ul>
  <li>설명</li>
</ul>

<p><img src="../assets/img/posts/image-20220307145421363.png" alt="image-20220307145421363" /></p>]]></content><author><name>Shin Hwi Jeong</name></author><summary type="html"><![CDATA[오늘의 포스팅 주제는 Session-Parallel Mini-Batches입니다.]]></summary></entry><entry><title type="html">Epoch, batch size, iteration의 의미</title><link href="http://localhost:4000/Epoch,-batch-size,-iteration%EC%9D%98-%EC%9D%98%EB%AF%B8.html" rel="alternate" type="text/html" title="Epoch, batch size, iteration의 의미" /><published>2022-03-07T00:00:00+09:00</published><updated>2022-03-07T00:00:00+09:00</updated><id>http://localhost:4000/Epoch,%20batch%20size,%20iteration%EC%9D%98%20%EC%9D%98%EB%AF%B8</id><content type="html" xml:base="http://localhost:4000/Epoch,-batch-size,-iteration%EC%9D%98-%EC%9D%98%EB%AF%B8.html"><![CDATA[<h1 id="epoch">Epoch</h1>

<p>전체 데이터 셋에 대하여 foward pass/ backward pass 학습을  진행</p>

<p>전체 데이터가 순전파와 역전파를 통해 신경망을 한 번 통과했습니다.</p>

<h1 id="batch-size">Batch Size</h1>

<p>batch size는 Single batch이 구성하고 있는 데이터 샘플의 크기를 의미합니다. Single batch는 mini-batch라고 보통 표현됩니다.</p>

<p>전체 데이터셋을 batch size만큼의 수로 여러개 나눕니다.</p>

<p>전체 데이터 셋을 나누는 이유는, 트레이닝 데이터를 통째로 신경망에 넣으면 메모리 초과가 날 수 도 있기 때문입니다.</p>

<p><strong>batch size만큼 feed foward 학습과 backprop 학습이 둘 다 진행됩니다.</strong></p>

<h1 id="iteration">Iteration</h1>

<p>전체 데이터는 batch size수 만큼 데이터가 나뉘어져 있고, ㅑiteration수만큼 batch를 학습하면 한 epoch를 학습하게 되는 것입니다.</p>

<p>즉, 1-epoch를 마치는데 필요한 mini batch의 갯수</p>

<h1 id="총정리">총정리</h1>

<p>5000개의 데이터가 존재하고 epoch = 100, batch_size = 250이라고 가정합니다.</p>

<p>전체 데이터는 batch_size 수인 250만큼 나뉘고 1 Epoch당 250개 씩 데이터를 받아 20 iteration 학습합니다.</p>

<p>그리고 전체 데이터는 100번 학습됩니다.</p>]]></content><author><name>Shin Hwi Jeong</name></author><summary type="html"><![CDATA[Epoch]]></summary></entry><entry><title type="html">Batch normalization</title><link href="http://localhost:4000/Batch-Normalization.html" rel="alternate" type="text/html" title="Batch normalization" /><published>2022-03-07T00:00:00+09:00</published><updated>2022-03-07T00:00:00+09:00</updated><id>http://localhost:4000/Batch%20Normalization</id><content type="html" xml:base="http://localhost:4000/Batch-Normalization.html"><![CDATA[<p>https://sjpyo.tistory.com/59</p>

<p>https://velog.io/@gibonki77/Batch-Normalization-backpropagation-%EC%9C%A0%EB%8F%84%ED%95%98%EA%B8%B0</p>]]></content><author><name>Shin Hwi Jeong</name></author><summary type="html"><![CDATA[https://sjpyo.tistory.com/59]]></summary></entry><entry><title type="html">강화학습 ) finite markov decision processes</title><link href="http://localhost:4000/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-)-Finite-Markov-Decision-Processes.html" rel="alternate" type="text/html" title="강화학습 ) finite markov decision processes" /><published>2022-02-28T00:00:00+09:00</published><updated>2022-02-28T00:00:00+09:00</updated><id>http://localhost:4000/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5%20)%20Finite%20Markov%20Decision%20Processes</id><content type="html" xml:base="http://localhost:4000/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-)-Finite-Markov-Decision-Processes.html"><![CDATA[<p>오늘은 강화학습 분야에서 Finite Markov Decision Processes에 대하여 공부한 것을 정리해 보았습니다.</p>

<hr />

<h6 id="참고사이트">참고사이트</h6>

<ul>
  <li>https://towardsdatascience.com/introduction-to-reinforcement-learning-rl-part-3-finite-markov-decision-processes-51e1f8d3ddb7</li>
</ul>

<hr />

<h6 id="핵심개념">핵심개념</h6>

<ul>
  <li>RL Problem이 MDP 프레임워크에 어떻게 적용이 되는가</li>
  <li>Markov Property란 무엇인가</li>
  <li>전환 확률 transition probabilities란 무엇인가</li>
  <li>미래 보상 할인이란 무엇인가 Discounting future rewards</li>
  <li>일시적인 작업과 연속작업</li>
  <li>벨만 최적 방정식 Bellman optimality Equations을 사용하여 최적의 정책 optimal Policy Function및 가치함수 optimal Value Function 풀기</li>
</ul>

<hr />

<h1 id="강화학습-정리">강화학습 정리</h1>

<ul>
  <li>강화학습에서 learner, decision-maker는 Agent입니다.</li>
  <li>Agent 외부의 모든 것을 포함하여 상호작용 하는 것은 Environment입니다.</li>
  <li>매 time step마다 environment는 agent에게 state를 보냅니다.</li>
  <li>state를 기초로 하여 agent는 Action을 선택합니다.</li>
  <li>다음 time step에서 Agent는 reward를 받게 되고, 다음 단계의 state를 받게 됩니다.</li>
  <li>매 time step마다 Agent는 현재 State에 따라 가능한 Action들을 선택할 확률에 대해 맵핑 알고리즘을 따르게 됩니다.
    <ul>
      <li>At each time step, the agent follows a mapping from its current state to probabilities of selecting each possible action in that state.</li>
    </ul>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>위에서 설명한 맵핑 알고리즘을 Agent의 Policy라고 얘기하며 πt(A</td>
          <td>S)라고 표기합니다. time step = t에서 State가 S일 때 Action A를 선택할 확률입니다.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Agent의 목표는 장기적으로 reward를 최대화 하는 것이 목표이며 reward를 최대화 하는 방향으로 학습하게 됩니다.</li>
</ul>

<h1 id="보상">보상</h1>

<ul>
  <li>time step t 이후,  agent가 reward를 받게될 보상을 <code class="language-plaintext highlighter-rouge">R(t+1), R(t+2), . . .</code>라고 한다면, Agent는 기대보상값  <strong>E</strong>(<strong>Gt)</strong>를 최대화 하는 방법을 학습하게 됩니다.
    <ul>
      <li>Gt = R(t+1) +R(t+2) +R(t+3) + · · · +R(T)</li>
      <li>T는 마지막 time step</li>
    </ul>
  </li>
  <li>
    <p>기대 보상값을 최대화 할 수 있는 경우는 실제 최종 단계인 time = T가 존재할 경우에만 유의미합니다.</p>
  </li>
  <li>바둑을 두는 강화학습 모델이 있다고 했을 때, 바둑 돌을 한수, 두수 두는 것을 episode라고 하고, 각각의 episode는 terminal 상태라는 특수한 상태에서 끝납니다. (바둑으로 치면 상대가 바둑을 한 수 둔 것으로 이해했음.)</li>
  <li>
    <p>에이전트가 terminal 상태에 도달하면 게임이 일부 재시작 상태로 세팅이 됩니다. (reset to some starting state)</p>
  </li>
  <li>이러한 에피소드를 가진 task를 episodic tasks라고 합니다.
    <ul>
      <li>(사실 여기서 episode가 바둑 한판 한판을 의미하는지, 한수 씩 두는 그 단계를 episode라고 두는지 모르겟음. 처음엔 바둑게임 한판 한판으로 해석했는데  reset to start state가 아니라 reset to some starting state라고 해서 한수 / 두수 씩 두는 단계를 episode라고 해석했습니다.)</li>
    </ul>
  </li>
  <li>이와 반대로, environment와 agnet가 제한 없이 계속 상호작용하는 경우가 있는데 이 경우는 continuing tasks 라고 합니다.</li>
  <li>이경우에 에이전트가 각 단계에서 +1점을 받고 최종 Time step = T  = ∞ 이므로 G(t)는 발산하게 됩니다.</li>
  <li>그래서 이경우 discounting을 도입하게 됩니다.
    <ul>
      <li>아마 여기서 finite가 나온듯.</li>
    </ul>
  </li>
</ul>

<p><img src="https://miro.medium.com/max/700/1*9n95udYDF8TueFRpHzg80w.png" alt="img" /></p>

<p>여기서 γ는 매개변수이며 0 ≤ γ ≤ 1이며 할인율 이라고 합니다.</p>

<p>γ는 기본적으로 미래에 k 시간 단계에서 받은 보상이 즉시 받았을 경우의 가치의 γ^k−1배의 가치가 있다고 말합니다.</p>

<p>γ &lt; 1이면 무한 합이 유한한 값을 갖기 때문에 문제가 해결됩니다(보상 시퀀스가 제한되는 한).</p>

<p>이제 에이전트의 목표는 받는 할인 보상의 합이 최대가 되도록 행동을 선택하는 것입니다.</p>

<p><img src="https://miro.medium.com/max/1400/1*9MNcQRw7Wd376OnfM0eOMA.png" alt="img" /></p>

<p>우리의 수익(Gt)이 다음 단계의 보상에 할인 요소를 곱한 다음 단계의 기대 수익과 같게 하는 것입니다.
이것은 우리가 나중에 보게 될 벨만 방정식의 핵심 아이디어라고 합니다.</p>

<h1 id="markov-property">Markov Property</h1>

<p>과거의 모든 관련 정보를 가질 수 있는 현재 상태 신호를 <strong>Markov property</strong> 를 갖는다고 얘기합니다.</p>

<p>TicTacToe 위치(모든 상태(즉, 보드의 모든 조각 구성))는 중요한 모든 것을 요약하기 때문에 <strong>Markov 상태 로 사용됩니다.</strong></p>

<p>-&gt; 예전 상태 필요없이 현재 상태만 알면 내가 어디둬야할지 계획이 똑띠 선다~~</p>

<p>즉, 미래를 예측하는 데 필요한 모든 정보는 우리가 가지고 있는 상태 표현에 포함되어 있습니다.</p>

<p>환경에 Markov property가 있는 경우 다음 방정식을 사용할 수 있습니다.</p>

<p><img src="https://miro.medium.com/max/1400/1*Ft9QcOqZ1TGlkY3Lw5CPEA.png" alt="img" /></p>

<p>위의 방정식을 <strong>동적 함수 p라고 합니다.</strong></p>

<p><img src="../assets/img/posts/image-20220306174740825.png" alt="image-20220306174740825" /></p>

<ul>
  <li>이를 만족한다하는데 = 1은 왜나오는지 모르겠음 ;</li>
</ul>

<p>현재 상태(S)와 행동(a)이 주어지면 다음 상태(S’)와 예상되는 다음 보상(r) 을 예측할 수 있습니다 .</p>

<p>이는 기본적으로 우리가 상태에서 상태로 이동하고 다른 조치를 취할 때 환경이 어떻게 변하는지 알려줍니다.</p>

<p>Markov 속성은 결정과 값이 현재 상태만의 함수로 가정되기 때문에 RL에서 중요합니다.</p>

<h1 id="markov-decdision-processes">Markov Decdision Processes</h1>

<p>Markov 속성을 만족하는 RL 문제를 <strong>Markov Decision Process</strong> 또는 <strong>MDP</strong> 라고 합니다.</p>

<ul>
  <li>state s_t가 이전 모든 state s_1s_2…s_{t-1}까지의 모든 상태를 포함하는 것을 말한다.</li>
</ul>

<p>유한한 수의 상태와 행동만 있는 경우 이를 <strong>유한 마르코프 결정 프로세스</strong> ( <strong>유한 MDP</strong> )라고 합니다.</p>

<p><img src="../assets/img/posts/image-20220308093859394.png" alt="image-20220308093859394" /></p>

<p>위 함수는 p: S Χ R Χ S Χ A -&gt; [0,1]를 만족하는 네가지 인수의 일반적인 결정론적 함수이기 때문에, 동적 함수에서 유용할 수 있는 몇 가지 다른 함수를 파생할 수도 있습니다.</p>

<blockquote>
  <p>4변수 역학 함수 p로부터, 상태 전이 확률과 같이 환경에 대해 알고 싶은 다른 어떤 것도 계산할 수 있다..</p>
</blockquote>

<p><img src="../assets/img/posts/image-20220308094041735.png" alt="image-20220308094041735" />4변수 역학 함수로부터 3변수 역학함수로 변형할 수있습니다. 아래는 상태전이 확률값을 계산하는 식입니다.</p>

<p><img src="https://miro.medium.com/max/1400/1*QWa9G8-UfTyt8P9Q84BrzQ.png" alt="img" /></p>

<p><img src="../assets/img/posts/image-20220308094205450.png" alt="image-20220308094205450" /> state와 Action 쌍에서 Expected rewards functions을 구해낼 수 도있습니다.</p>

<p><img src="https://miro.medium.com/max/1400/1*4cZK2h7RJJvH8ZWB-FlB2g.png" alt="img" /></p>

<p><img src="../assets/img/posts/image-20220308094506650.png" alt="image-20220308094506650" />state - Action- Next state 세 변수로 expected reward를 계산할 수 있습니다.</p>

<p><img src="https://miro.medium.com/max/1400/1*W2Egywx1TA_NX7M9mbBBEA.png" alt="img" /></p>

<h1 id="transition-diagram">Transition diagram</h1>

<p>finite MDP dynamic을 확인하는 유용한 방법.</p>

<p>에이전트에게 각 상태에서 가능한 조치가 무엇인지 알려주는 그래프</p>

<p>각 행동을 취할 확률과 각 행동에 대한 보상을 표기하기도 합니다.</p>

<p>이 그래프는 테이블로도 표현할 수 있습니다.</p>

<h1 id="value-functions">Value Functions</h1>

<p>가치 함수는 agent에게 주어진 state가 얼마나 좋은지를 평가할 수 있습니다.</p>

<table>
  <tbody>
    <tr>
      <td>πt(A</td>
      <td>S)는 timestep = t에서 State S일 때, Action A를 취할 확률을 의미하고 이를 Policy라고 합니다.</td>
    </tr>
  </tbody>
</table>

<h2 id="--policy---value-functions">- Policy - value functions</h2>

<p>정책 π에서 상태 S의 값은 V <strong>π</strong> (s)로 표시되며, S에서 시작하여 마지막 time step까지 그 이후에 π를 따를 때의 기대 수익입니다.<img src="https://miro.medium.com/max/700/1*VIdvGE6uWhTVkQVToNISqw.png" alt="img" /></p>

<p>정책 π에 대한 함수 V π를 state-value 함수라고 합니다. Gt에 대한 기댓값입니다.</p>

<h2 id="action-value-functions">Action-Value functions</h2>

<p>정책 π에 따라 상태 “S”에서 행동 “A”를 취하는 값을 q <strong>π</strong> (S, A)로 정의할 수 있습니다. 이것을 정책 π에 대한 행동 가치 함수라고 부릅니다.</p>

<p>상태 S에서 시작하여 조치 A를 취하고 이후에 정책 π를 따르는 기대 수익입니다.</p>

<p>https://towardsdatascience.com/introduction-to-reinforcement-learning-rl-part-3-finite-markov-decision-processes-51e1f8d3ddb7</p>

<h1 id="bellman-equations">Bellman Equations</h1>

<p><img src="https://miro.medium.com/max/700/0*5egXnk4bBL1ynI6q" alt="img" /></p>

<table>
  <tbody>
    <tr>
      <td>π(a</td>
      <td>s)는 정책 π에 따라 상태 “s”에서 행동 “a”를 취할 확률</td>
    </tr>
  </tbody>
</table>

<p>위의 방정식은 <strong>Vπ에 대한 Bellman 방정식입니다</strong> 정책 π에 따라 상태 S의 값과 다음 상태 S’의 값 사이의 관계를 나타냅니다.</p>

<p><img src="https://miro.medium.com/max/684/1*F4iO6B63gDQ8CLb94Ut_TQ.png" alt="img" /></p>

<p>업데이트 또는 백업 작업이 작동하는 방식을 보여주기 때문에 이러한 다이어그램을 backup diagrams이라고 합니다.</p>

<p><img src="https://miro.medium.com/max/698/1*W0vsxmosaPj7kYzG6svNpA.png" alt="img" /></p>

<p>상태 값에 대한 벨만 방정식</p>

<p><img src="https://miro.medium.com/max/700/1*4BtFvlvr7YjW24GrjIUQvg.png" alt="img" /></p>

<p>Gt+1(4.4, 3.0, 0.7, 1.9)이라는 특정 값</p>

<p>각각에 들어갈 확률은 0.25</p>

<p>할인 계수 γ는 0.9</p>

<p>0 + 0.9* [(0.25 * 4.4) + (0.25<em>1.9) + (0.25</em>0.7) + (0.25*3.0)] = 2.25 — &gt; 2.3</p>

<h1 id="optimal-value-functions">Optimal Value Functions</h1>

<p>V*에 대한 Bellman 최적 방정식</p>

<p>보상을 최대화하기 위해 각 상태에 대해 최상의 보상을 제공하는 작업을 의미하는 “max(a)”를 사용</p>

<p><img src="https://miro.medium.com/max/700/1*K3HPdeqPEjpa9Blf_boSmg.png" alt="img" /></p>

<p>최적의 정책 <strong>π*</strong> 은 최선의 조치가 무엇인지 알려줍니다. 모든 state에서 우리는 단순히 가장 높은 값의 지시를 따릅니다.</p>

<hr />

<h6 id="알아두면-좋을-정보">알아두면 좋을 정보</h6>

<ul>
  <li>MDP란 S, A, P, R등이 정의 되어있고, S(state)가 Markov property를 만족하는 것을 말한다. 여기서 중요한 게 state가 Markov property를 만족한다는 건데, Markov property란 현재의 state s_t가 이전 모든 state s_1s_2…s_{t-1}까지의 모든 상태를 포함하는 것을 말한다.</li>
  <li>일단 강화학습에서 MDP를 가정하는 이유는 여러 가지가 있겠지만, state가 Markov property를 만족하지 않을 경우 다음 상태를 알기 위해 처음부터 현재까지의 모든 정보를 기록해서 사용해야 한다. 이건 단순히 생각해봐도 비효율적이고 나중에 가면 계산이 불가능해 질 것이다.</li>
  <li>
    <p>S0, A0, R1, S1, A1, R2, S2, A2, R3,…</p>
  </li>
  <li>이 경우, 무작위 변수 Rt와 St는 선행 상태와 작용에만 의존하는 잘 정의된 이산 확률 분포를 갖는다.</li>
</ul>

<hr />

<h6 id="모르겠는거">모르겠는거</h6>

<ul>
  <li>The dot over the equals sign in the equation reminds us that it is a definition (in this case of the function p) rather than a fact that follows from previous definitions.</li>
  <li>small p는 probability 라던데 Pr{은 뭘까여</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>The ‘</td>
          <td>’ in the middle of it comes from the notation for conditional probability, but here it just reminds us that p specifies a probability distribution for each choice of s and a, that is, that <img src="../assets/img/posts/image-20220306155928560.png" alt="image-20220306155928560" /></td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p>여기서 왜 p값이 1일까!</p>
  </li>
  <li>
    <p>This is best viewed a restriction not on the decision process, but on the state.</p>
  </li>
  <li>
    <p>In particular, the boundary between agent and environment is typically not the same as the physical boundary of a robot’s or animal’s body. Usually, the boundary is drawn closer to the agent than that. For example, the motors and mechanical linkages of a robot and its sensing hardware should usually be considered parts of the environment rather than parts of the agent. Similarly, if we apply the MDP framework to a person or animal, the muscles, skeleton, and sensory organs should be considered part of the environment. Rewards, too, presumably are computed inside the physical bodies of natural and artificial learning systems, but are considered external to the agent.</p>

    <ul>
      <li>특히, agent와 환경 사이의 경계는 일반적으로 로봇이나 동물의 몸의 물리적 경계와 동일하지 않다. 보통, 그 경계는 그것보다 에이전트에 더 가깝게 그려집니다. 예를 들어 로봇과 감지 하드웨어의 모터와 기계적 연결은 일반적으로 에이전트의 일부가 아닌 환경의 일부로 간주해야 한다. 마찬가지로, MDP 프레임워크를 사람이나 동물에 적용한다면, 근육, 골격, 감각 기관은 환경의 일부로 간주되어야 한다. 보상도 아마도 자연 및 인공 학습 시스템의 물리적 본체 내부에서 계산되지만 agent의 외부로 간주된다.</li>
      <li>the agent than that
        <ul>
          <li>that?!</li>
          <li>기계적 연결이 환경에 가깝다는건 그냥 직관적으로 이해했는데, 앞에 글에선 boundary가 Agent에 가까워야한다는데 왜 environment라고 하냐규~</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>But we always consider the reward computation to be external to the agent because it defines the task facing the agent and thus must be beyond its ability to change arbitrarily</p>

    <ul>
      <li>external의 뜻? Agent가 할수있는게 아니라고? Agent외부에서 일어나는일이라고?</li>
      <li>근데 왜 agent가 facing한 task야</li>
      <li>change arbitraily가 뭐람 -_-</li>
    </ul>
  </li>
  <li></li>
</ul>

<hr />

<p>The MDP framework is abstract and flexible and can be applied to many different problems in many different ways. For example, the time steps need not refer to fixed intervals of real time; they can refer to arbitrary successive stages of decision making and acting. The actions can be low-level controls, such as the voltages applied to the motors of a robot arm, or high-level decisions, such as whether or not to have lunch or to go to graduate school. Similarly, the states can take a wide variety of forms. They can be completely determined by low-level sensations, such as direct sensor readings, or
they can be more high-level and abstract, such as symbolic descriptions of objects in a room. Some of what makes up a state could be based on memory of past sensations or even be entirely mental or subjective. For example, an agent could be in the state of not being sure where an object is, or of having just been surprised in some clearly defined sense. Similarly, some actions might be totally mental or computational. For example, some actions might control what an agent chooses to think about, or where it focuses its attention. In general, actions can be any decisions we want to learn how to make, and the states can be anything we can know that might be useful in making them.</p>

<p>In particular, the boundary between agent and environment is typically not the same as the physical boundary of a robot’s or animal’s body. Usually, the boundary is drawn closer to the agent than that. For example, the motors and mechanical linkages of a robot and its sensing hardware should usually be considered parts of the environment rather than parts of the agent. Similarly, if we apply the MDP framework to a person or animal, the muscles, skeleton, and sensory organs should be considered part of the environment. Rewards, too, presumably are computed inside the physical bodies of natural and artificial learning systems, but are considered external to the agent.
The general rule we follow is that anything that cannot be changed arbitrarily by the agent is considered to be outside of it and thus part of its environment.  We do not assume that everything in the environment is unknown to the agent. For example, the agent often knows quite a bit about how its rewards are computed as a function of its actions and the states in which they are taken. But we always consider the reward computation to be external to the agent because it defines the task facing the agent and thus must be beyond its ability to change arbitrarily. In fact, in some cases the agent may know everything about how its environment works and still face a dicult reinforcement learning task, just as we may know exactly how a puzzle like Rubik’s cube works, but still be unable to solve it. The agent–environment boundary represents the limit of the agent’s absolute control, not of its knowledge.</p>

<p>민주당 프레임워크는 추상적이고 유연하며 많은 다양한 방법으로 다양한 문제에 적용될 수 있다. 예를 들어, 시간 단계는 고정된 실시간 간격을 참조할 필요가 없으며 의사 결정과 행동의 임의의 연속 단계를 참조할 수 있다. 그 동작은 로봇 팔의 모터에 인가되는 전압과 같은 낮은 수준의 제어일 수도 있고, 점심을 먹을지 대학원에 갈지 말지 같은 높은 수준의 결정일 수도 있다. 마찬가지로, 주들도 매우 다양한 형태를 취할 수 있다. 이들은 직접 센서 판독값과 같은 낮은 수준의 감각에 의해 완전히 결정될 수 있다.
그것들은 룸에 있는 물체에 대한 상징적 설명과 같이 좀 더 고급적이고 추상적일 수 있다. 국가를 구성하는 것 중 일부는 과거의 감각에 대한 기억에 기초하거나 심지어 완전히 정신적이거나 주관적일 수도 있다. 예를 들어, 에이전트는 물체가 어디에 있는지 확신하지 못하는 상태이거나 명확하게 정의된 의미에서 놀랐을 수 있다. 마찬가지로, 일부 행동은 완전히 정신적이거나 계산적일 수 있다. 예를 들어, 일부 작업은 에이전트가 무엇을 생각하도록 선택했는지 또는 에이전트가 어디에 주의를 기울이는지를 제어할 수 있습니다. 일반적으로, 행동은 우리가 어떻게 만드는지 배우고 싶은 어떤 결정도 될 수 있고, 상태는 우리가 알 수 있는 그것들을 만드는 데 유용할 수 있는 모든 것이 될 수 있습니다.</p>

<p>특히, 작용제와 환경 사이의 경계는 일반적으로 로봇이나 동물의 몸의 물리적 경계와 동일하지 않다. 보통, 그 경계는 그것보다 에이전트에 더 가깝게 그려집니다. 예를 들어 로봇과 감지 하드웨어의 모터와 기계적 연결은 일반적으로 에이전트의 일부가 아닌 환경의 일부로 간주해야 한다. 마찬가지로, MDP 프레임워크를 사람이나 동물에 적용한다면, 근육, 골격, 감각 기관은 환경의 일부로 간주되어야 한다. 보상도 아마도 자연 및 인공 학습 시스템의 물리적 본체 내부에서 계산되지만 대리인의 외부로 간주된다.
우리가 따르는 일반적인 규칙은 에이전트가 임의로 변경할 수 없는 모든 것은 에이전트 외부에 있는 것으로 간주하여 환경의 일부로 간주하는 것입니다. 우리는 환경에 있는 모든 것을 에이전트가 알 수 없다고 가정하지 않습니다. 예를 들어, 에이전트는 종종 보상이 조치의 함수로 계산되는 방법과 조치를 취하는 상태에 대해 상당히 많이 알고 있다. 그러나 보상 계산은 에이전트가 직면한 작업을 정의하고 따라서 임의로 변경할 수 있는 능력을 벗어나야 하기 때문에 항상 에이전트의 외부로 간주한다. 실제로, 루빅 큐브와 같은 퍼즐이 어떻게 작동하는지 정확히 알면서도 여전히 풀지 못하는 것처럼 에이전트가 환경의 작동 방식에 대해 모든 것을 알고 있을 수 있지만 여전히 어려운 강화 학습 과제에 직면할 수도 있습니다. 에이전트-환경 경계는 에이전트의 지식이 아닌 절대적 통제의 한계를 나타냅니다.</p>]]></content><author><name>Shin Hwi Jeong</name></author><summary type="html"><![CDATA[오늘은 강화학습 분야에서 Finite Markov Decision Processes에 대하여 공부한 것을 정리해 보았습니다.]]></summary></entry><entry><title type="html">Python ) 리스트</title><link href="http://localhost:4000/Python-)-%EB%A6%AC%EC%8A%A4%ED%8A%B8.html" rel="alternate" type="text/html" title="Python ) 리스트" /><published>2022-02-27T00:00:00+09:00</published><updated>2022-02-27T00:00:00+09:00</updated><id>http://localhost:4000/Python%20)%20%EB%A6%AC%EC%8A%A4%ED%8A%B8</id><content type="html" xml:base="http://localhost:4000/Python-)-%EB%A6%AC%EC%8A%A4%ED%8A%B8.html"><![CDATA[<p>오늘은 파이썬 리스트 자료형의 개념과 사용법에 대해서 공부를 해보았습니다.</p>

<hr />

<h6 id="출처">출처</h6>

<ul>
  <li><a href="https://dojang.io/mod/page/view.php?id=2200">파이썬 코딩도장 - 리스트</a></li>
</ul>

<hr />

<h1 id="1-리스트-만들기">1. 리스트 만들기</h1>

<ul>
  <li>리스트 = [값, 값, 값]</li>
  <li>리스트 = list( iterable object )
    <ul>
      <li>리스트 = list(range(4,40,5))</li>
    </ul>
  </li>
</ul>

<h2 id="빈-리스트-만들기">빈 리스트 만들기</h2>

<ul>
  <li>리스트 = []</li>
  <li>리스트 = list()</li>
</ul>

<h2 id="range를-사용하여-리스트-만들기">range를 사용하여 리스트 만들기</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span>
</code></pre></div></div>

<p>실행결과</p>

<blockquote>
  <p>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</p>
</blockquote>

<ul>
  <li>리스트 = list(range(시작, 끝, 증가폭))</li>
</ul>

<h2 id="문자열을-이용해-리스트-만들기">문자열을 이용해 리스트 만들기</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="s">"hello"</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span>
</code></pre></div></div>

<p>실행결과</p>

<blockquote>
  <p>[‘H’, ‘e’, ‘l’, ‘l’, ‘o’]</p>
</blockquote>

<h1 id="2-리스트로-변수-만들기">2. 리스트로 변수 만들기</h1>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt;&gt;&gt; a, b, c = [1, 2, 3]
&gt;&gt;&gt; print(a, b, c)
</code></pre></div></div>

<p>실행결과</p>

<blockquote>
  <p>1 2 3</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">x</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
</code></pre></div></div>

<p>실행결과</p>

<blockquote>
  <p>1 2 3</p>
</blockquote>

<h1 id="3-특정-값이-있는지-확인하기">3. 특정 값이 있는지 확인하기</h1>

<h2 id="값-in-시퀀스-자료형">값 in 시퀀스 자료형</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">60</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">90</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mi">30</span> <span class="ow">in</span> <span class="n">a</span>
<span class="bp">True</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mi">100</span> <span class="ow">in</span> <span class="n">a</span>
<span class="bp">False</span>
</code></pre></div></div>

<h1 id="4-객체끼리-연결하기">4. 객체끼리 연결하기</h1>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt;&gt;&gt; a = [10, 20, 30]
&gt;&gt;&gt; b = [1, 2, 3]
&gt;&gt;&gt; a + b
</code></pre></div></div>

<p>실행결과</p>

<blockquote>
  <p>[10, 20, 30, 1, 2, 3]</p>
</blockquote>

<h1 id="5-시퀀스-객체-반복하기">5. 시퀀스 객체 반복하기</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span>
</code></pre></div></div>

<p>실행결과</p>

<blockquote>
  <p>[0, 10, 20, 30, 0, 10, 20, 30, 0, 10, 20, 30]</p>
</blockquote>

<h1 id="6-특정-값-가져오기">6. 특정 값 가져오기</h1>

<ul>
  <li>
    <p>대괄호 사용하기</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">38</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">53</span><span class="p">,</span> <span class="mi">62</span><span class="p">,</span> <span class="mi">19</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div>    </div>

    <p>실행결과</p>

    <blockquote>
      <p>21</p>
    </blockquote>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">__getitem__</code> 사용하기</p>

    <p>시퀀스 객체에서 <code class="language-plaintext highlighter-rouge">[]</code>(대괄호)를 사용하면 실제로는 <code class="language-plaintext highlighter-rouge">__getitem__</code> 메서드를 호출하여 요소를 가져옵니다. 따라서 메서드를 직접 호출하여 요소를 가져올 수도 있습니다.</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">38</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">53</span><span class="p">,</span> <span class="mi">62</span><span class="p">,</span> <span class="mi">19</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span><span class="p">.</span><span class="n">__getitem__</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>    </div>

    <p>실행결과</p>

    <blockquote>
      <p>21</p>
    </blockquote>
  </li>
</ul>

<h1 id="7-특정-값-삭제하기">7. 특정 값 삭제하기</h1>

<ul>
  <li>del a[5]</li>
</ul>]]></content><author><name>Shin Hwi Jeong</name></author><summary type="html"><![CDATA[오늘은 파이썬 리스트 자료형의 개념과 사용법에 대해서 공부를 해보았습니다.]]></summary></entry></feed>