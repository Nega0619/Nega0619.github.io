현재 저는 강화학습 스터디를 참여해 강화학습에 대한 강의를 보고있습니다.

오늘의 포스팅은 그 강의에 대한 회고록 및 요약 정리에 관한 것입니다.





-----------

###### 강의정보

- 링크

  https://www.youtube.com/watch?v=TCCjZe0y4Qc

- 강의자 

  Hado van Hasselt, Senior Staff Reasearch Scientist, Deepmind

- 강화학습 스터디에서 진행하는 강의 같이보기 도전중!

-------------------





# 1. what is reinforcement learning? 



Industrial revolution과 Digital Revolution으로 이야기를 시작고, 본격적인 AI에 관한 이야기가 진행되는데 이 말이 너무 멋있어서 메모했다.

> Next thing that you can think of which is already ongoing.

지금 현재, AI 기술이 얼마나 빠르게 발전하고 있는지에 대해 알 수 있는 문장이다.



## Artificial Intelligence란?

현재는 데이터를 학습하여 그저 계산을 하는 단계는 넘어섰고, 스스로 학습하여 솔루션을 찾을 수 있는 기계의 단계로 넘어가고 있다. 즉 자율적으로 결정을 하고, 학습을 하는 것이다.



우리가 정의 내리는 인공지능은 다음에 따라 결정될 것이다 : To be able to learn to make dicisions to achieve goals. 



## Reinforcement learning이란?

사람이 환경과 상호작용을 하며 학습을 한다는 아이디어를 시작으로 개발된 이론

강화학습의 특징

- 수동적이기보단 적극적이며, (2강에서 자세히)

- 독립적이지 않고 연속적이다. 향후 상호작용은 이전 상호작용에 의해 달라질 수 있다.

- 목표 지향적이다.

- we can learn **without examples** of optimal behavior

  모든 하위 행동을 하나하나 지정해주지 않아도, 명확한 목적을 통해 하위 행동이 자동적으로 행동이 정해진다.(이 부분이 잘 이해가 안갑니다. 약 10:00분쯤)

- 보상을 최적화 한다.(학습한다.)

  즉, 목표 : Agent is going to try to optimize sum of rewards, through repeated interaction

  그렇기 때문에 강화학습은, environment와 agent의 상호작용(observation / action)에서 상호작용 하나하나에 관심을 갖기보다, 그 상호작용의 결과물, 즉 보상이 최대가 되도록 하는데에 관심을 갖는다.



### The reward hypothesis 

16:14



## Examples RL problems

실제로 강화학습을 이용해 해결한 문제들



### Fly a helicopter

보상 : air time(비행시간), inverse distance, ..



### Manage an investment porfolio

보상 : gains, gains minus risk, ..



### Control a power station

보상 : efficienct, ...



### Make a robot walk

보상 : distance, speed, ...



### play video or board games

보상 : win, maximise score, ...



### 강화학습을 꼭 이용했어야 하는 distinct한 이유들



1. find solutions
2. it can adapt online, deal with unforeseen circumstances

강화학습은, 해결책을 찾아야 하거나, 전혀 모르는 상황에 대해서도 적응할 수 있어, 위 두개의 문제에 대한 알고리즘을 제공해 줄 수 있다.

하지만, 강화학습 can adapt online에서, 일반화를 하는것이 아닌, online 환경에서 효과적으로 계속 배울수 있다는 의미.

-> online의 데이터를 이용하여 학습을 하고 추상화 한다는게 아니라, 온라인 환경에서 학습을 할 수 있다는 의미인가요? (22"00)



# 2. About Reinforcement Learning



## Agent and Environment - Interaction Loop

![image-20220111190532616](../../../../Typora Images/image-20220111190532616.png)

- 매 t단계마다 관찰과 보상을 받음.
  - Agent
    - 관찰받음 O_t 혹은 보상 R_t
    - 행동 A 실행 A_t
  - Environment
    - 행동 A 수령
    - O_t+1 방출 혹은 보상 R_t+1



### Reward

- reward R_t 는 스칼라값
- Agent의 할일은 보상을 최대화 하는 것.



![image-20220111191110182](../../../../Typora Images/image-20220111191110182.png)

G는 Goal이라고 생각하는게 보통이지만, 이건 return 반환이라고 생각. 즉, return은 accumulative reward or the sum of rewards into the future의 약어





### Values

state s에 대한 cumulative reward를 의미

![image-20220111191513362](../../../../Typora Images/image-20220111191513362.png)

- agent가 취하는 행동에 따라 다름

- 우리가 원하는 것은, agent가 value가 커지는 액션을 취하도록 만드는 것.

  so, we want to pick actions such that this value becomes large.

- reward와 value는 해당 state에 대한 action의 유용함(타당함?)을 의미한다.

- reward와 value는 재귀적으로 정의될 수 있다.

  32:30 return값이 R_t-1+G_t-1이 아니라, t+1인 이유가 이해가 잘안됩니다.

  ![image-20220111192113239](../../../../Typora Images/image-20220111192113239.png)



## Maximising value by taking actions



- 강화학습의 목표는, value를 최대화하는 액션들을 고르는것.
  - 액션들은 장기간의 결과를 가질수있음.
  - 보상이 지연되어 발생할 수 있음
  - 즉시보상을 안받는 것이, 장기간의 보상을 받는게 더 좋을 수있음.



### Action values



![image-20220111193007555](../../../../Typora Images/image-20220111193007555.png)

q = 어떤 state일때의 action값. 즉 historical 한 의미 가짐.

q는 상태 및 행동의 가치 기능을 나타냄



# 3. Core concepts



## Environment



### reward signal



## Agent

- Agent component
  - agent state
  - policy
  - value function
  - model

- agent의 내부

  ![image-20220111193643781](../../../../Typora Images/image-20220111193643781.png)



### Agent state 

36분쯤 여길 좀 중점적으로 같이봣으면 좋겟어여ㅠ

- 예측
  - 정책을 정의하고 선택해야함.
  - agent 내부 사진에 정책에서 작업으로 action으로 향하는 화살표 추가 가능 



![image-20220111200442278](../../../../Typora Images/image-20220111200442278.png)

# 어떻게 어디로 나눠야할지 모르겟음 ;



### Environment state

Environment's internal state

It is usually invisible to the agent

​	even if it visible it may contains lots of irrelevant information

- additional memory가 필요함



### Fully Observable Environments state

흔한 경우는 아니지만, agent가 모든 environments state를 다 아는경우가 있다. 이때는 observation = environment state	



#### 4. Markov Decision process

이 이론은 강화학습의 기반이 된 수학적 프레임워크 이론이다.

![image-20220111195217002](../../../../Typora Images/image-20220111195217002.png)

보상확률과 이후의 연속적인 state 에 대한것.

마르코브 의사결정은 state가 우리가 알아야하는 history를 전부 알고있다. 즉 그 state값만 알아도 의사결정을 내리는데 아무 무리가없다.

![image-20220111195607379](../../../../Typora Images/image-20220111195607379.png)



### Partially Observable Environments

These observations are not Markovian

- ex

  ![image-20220111200107127](../../../../Typora Images/image-20220111200107127.png)



아래 그림 내용 설명할때 잘 이해가 안가요. 45:40

![image-20220111200255965](../../../../Typora Images/image-20220111200255965.png)



### Policy

 에이전트의 행동을 정의

단지, 에이전트에서 작업으로의 맵핑입니다.

![image-20220111202046194](../../../../Typora Images/image-20220111202046194.png)

pi : 상태가 주어진 행동의 확률



### value function & value Estimates



#### value function

![image-20220111202206439](../../../../Typora Images/image-20220111202206439.png)

value 함수는 return에 대한것을 예측할 수있습니다.

value func는 정책에 의존하기 때문에 앞전에 나온 정의를 명시적으로 수정하였습니다.

![image-20220111203848332](../../../../Typora Images/image-20220111203848332.png)위의 식에 새로운 요소

discount factor는 즉시보상과 장기보상의 중요성을 상쇄시킨다?

- value func는 상태의 만족도를 평가하는 데 사용할 수 있습니다.
- value func는 action들을 선택하는데 사용할 수 있습니다.



- return 

  ![image-20220111204821578](../../../../Typora Images/image-20220111204821578.png)

- value :

  ![image-20220111204846660](../../../../Typora Images/image-20220111204846660.png)

  ~pi는 (pi가 결정론적인 상태일지라도) 정책 pi에 의해 선택된다는 것을 의미한다. (벨만 방정식, bellman equation)



### model

모델은 다음 상태를 예측하는 모델 p를 가질 수있습니다.

![image-20220111205854381](../../../../Typora Images/image-20220111205854381.png)

a state, an action, 그리고 다음상태의 state를 input으로 관찰 한후, 다음 상태를 예상하는 확률에 대한 근사치  

