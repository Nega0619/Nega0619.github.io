I"è
<p>ì˜¤ëŠ˜ì€ ê°•í™”í•™ìŠµ ë¶„ì•¼ì—ì„œ Finite Markov Decision Processesì— ëŒ€í•˜ì—¬ ê³µë¶€í•œ ê²ƒì„ ì •ë¦¬í•´ ë³´ì•˜ìŠµë‹ˆë‹¤.</p>

<hr />

<h6 id="ì°¸ê³ ì‚¬ì´íŠ¸">ì°¸ê³ ì‚¬ì´íŠ¸</h6>

<ul>
  <li>https://towardsdatascience.com/introduction-to-reinforcement-learning-rl-part-3-finite-markov-decision-processes-51e1f8d3ddb7</li>
</ul>

<hr />

<h6 id="í•µì‹¬ê°œë…">í•µì‹¬ê°œë…</h6>

<ul>
  <li>RL Problemì´ MDP í”„ë ˆì„ì›Œí¬ì— ì–´ë–»ê²Œ ì ìš©ì´ ë˜ëŠ”ê°€</li>
  <li>Markov Propertyë€ ë¬´ì—‡ì¸ê°€</li>
  <li>ì „í™˜ í™•ë¥  transition probabilitiesë€ ë¬´ì—‡ì¸ê°€</li>
  <li>ë¯¸ë˜ ë³´ìƒ í• ì¸ì´ë€ ë¬´ì—‡ì¸ê°€ Discounting future rewards</li>
  <li>ì¼ì‹œì ì¸ ì‘ì—…ê³¼ ì—°ì†ì‘ì—…</li>
  <li>ë²¨ë§Œ ìµœì  ë°©ì •ì‹ Bellman optimality Equationsì„ ì‚¬ìš©í•˜ì—¬ ìµœì ì˜ ì •ì±… optimal Policy Functionë° ê°€ì¹˜í•¨ìˆ˜ optimal Value Function í’€ê¸°</li>
</ul>

<hr />

<h1 id="ê°•í™”í•™ìŠµ-ì •ë¦¬">ê°•í™”í•™ìŠµ ì •ë¦¬</h1>

<ul>
  <li>ê°•í™”í•™ìŠµì—ì„œ learner, decision-makerëŠ” Agentì…ë‹ˆë‹¤.</li>
  <li>Agent ì™¸ë¶€ì˜ ëª¨ë“  ê²ƒì„ í¬í•¨í•˜ì—¬ ìƒí˜¸ì‘ìš© í•˜ëŠ” ê²ƒì€ Environmentì…ë‹ˆë‹¤.</li>
  <li>ë§¤ time stepë§ˆë‹¤ environmentëŠ” agentì—ê²Œ stateë¥¼ ë³´ëƒ…ë‹ˆë‹¤.</li>
  <li>stateë¥¼ ê¸°ì´ˆë¡œ í•˜ì—¬ agentëŠ” Actionì„ ì„ íƒí•©ë‹ˆë‹¤.</li>
  <li>ë‹¤ìŒ time stepì—ì„œ AgentëŠ” rewardë¥¼ ë°›ê²Œ ë˜ê³ , ë‹¤ìŒ ë‹¨ê³„ì˜ stateë¥¼ ë°›ê²Œ ë©ë‹ˆë‹¤.</li>
  <li>ë§¤ time stepë§ˆë‹¤ AgentëŠ” í˜„ì¬ Stateì— ë”°ë¼ ê°€ëŠ¥í•œ Actionë“¤ì„ ì„ íƒí•  í™•ë¥ ì— ëŒ€í•´ ë§µí•‘ ì•Œê³ ë¦¬ì¦˜ì„ ë”°ë¥´ê²Œ ë©ë‹ˆë‹¤.
    <ul>
      <li>At each time step, the agent follows a mapping from its current state to probabilities of selecting each possible action in that state.</li>
    </ul>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>ìœ„ì—ì„œ ì„¤ëª…í•œ ë§µí•‘ ì•Œê³ ë¦¬ì¦˜ì„ Agentì˜ Policyë¼ê³  ì–˜ê¸°í•˜ë©° Ï€t(A</td>
          <td>S)ë¼ê³  í‘œê¸°í•©ë‹ˆë‹¤. time step = tì—ì„œ Stateê°€ Sì¼ ë•Œ Action Aë¥¼ ì„ íƒí•  í™•ë¥ ì…ë‹ˆë‹¤.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Agentì˜ ëª©í‘œëŠ” ì¥ê¸°ì ìœ¼ë¡œ rewardë¥¼ ìµœëŒ€í™” í•˜ëŠ” ê²ƒì´ ëª©í‘œì´ë©° rewardë¥¼ ìµœëŒ€í™” í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤.</li>
</ul>

<h1 id="ë³´ìƒ">ë³´ìƒ</h1>

<ul>
  <li>time step t ì´í›„,  agentê°€ rewardë¥¼ ë°›ê²Œë  ë³´ìƒì„ <code class="language-plaintext highlighter-rouge">R(t+1), R(t+2), . . .</code>ë¼ê³  í•œë‹¤ë©´, AgentëŠ” ê¸°ëŒ€ë³´ìƒê°’  <strong>E</strong>(<strong>Gt)</strong>ë¥¼ ìµœëŒ€í™” í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤.
    <ul>
      <li>Gt = R(t+1) +R(t+2) +R(t+3) + Â· Â· Â· +R(T)</li>
      <li>TëŠ” ë§ˆì§€ë§‰ time step</li>
    </ul>
  </li>
  <li>ê¸°ëŒ€ ë³´ìƒê°’ì„ ìµœëŒ€í™” í•  ìˆ˜ ìˆëŠ” ê²½ìš°ëŠ” ì‹¤ì œ ìµœë™ ë‹¨ê³„ì¸ Tê°€ ì¡´ì¬í•  ê²½ìš°ì—ë§Œ ìœ ì˜ë¯¸í•©ë‹ˆë‹¤.</li>
</ul>

:ET